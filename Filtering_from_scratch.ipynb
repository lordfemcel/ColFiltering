{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM31AaYr0uok7995QK++vzH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordfemcel/ColFiltering/blob/main/Filtering_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4Ki9KHGbAf-G"
      },
      "outputs": [],
      "source": [
        "#!pip install torch pandas numpy scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Download MovieLens 100k data\n",
        "!wget -nc https://files.grouplens.org/datasets/movielens/ml-100k/u.data\n",
        "\n",
        "# Load ratings\n",
        "ml_ratings = pd.read_csv('u.data', sep='\\t', names=['user', 'item', 'rating', 'timestamp'])\n",
        "ml_ratings['user'] -= 1\n",
        "ml_ratings['item'] -= 1\n",
        "ml_ratings['rating'] = (ml_ratings['rating'] > 0).astype(int)  # Binarize ratings\n",
        "\n",
        "num_users = ml_ratings['user'].nunique()\n",
        "num_items = ml_ratings['item'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETPpvmOfAoHE",
        "outputId": "3445b13f-6d9f-4925-8233-1a213fd04d50"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘u.data’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class UserItemRatingDataset(Dataset):\n",
        "    def __init__(self, user_item_pairs, ratings, num_items, num_ng=4, is_training=True):\n",
        "        self.user_item_pairs = user_item_pairs\n",
        "        self.ratings = ratings\n",
        "        self.num_items = num_items\n",
        "        self.num_ng = num_ng\n",
        "        self.is_training = is_training\n",
        "        if is_training:\n",
        "            self._ng_sample()\n",
        "        else:\n",
        "            self.data = [(u, i, r) for (u, i), r in zip(self.user_item_pairs, self.ratings)]\n",
        "    def _ng_sample(self):\n",
        "        self.data = []\n",
        "        user_item_set = set(self.user_item_pairs)\n",
        "        for idx, ((u, i), r) in enumerate(zip(self.user_item_pairs, self.ratings)):\n",
        "            self.data.append((u, i, r))\n",
        "            for _ in range(self.num_ng):\n",
        "                j = np.random.randint(self.num_items)\n",
        "                while (u, j) in user_item_set:\n",
        "                    j = np.random.randint(self.num_items)\n",
        "                self.data.append((u, j, 0))\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        u, i, r = self.data[idx]\n",
        "        return torch.LongTensor([u]), torch.LongTensor([i]), torch.FloatTensor([r])\n",
        "\n",
        "class SampleGenerator:\n",
        "    def __init__(self, ratings):\n",
        "        # Split train/test (leave one out for test)\n",
        "        self.ratings = ratings\n",
        "        self.train_pairs, self.train_ratings, self.test_pairs, self.test_ratings = self._split()\n",
        "    def _split(self):\n",
        "        train_pairs, train_ratings = [], []\n",
        "        test_pairs, test_ratings = [], []\n",
        "        ratings_group = self.ratings.groupby('user')\n",
        "        for user, group in ratings_group:\n",
        "            group = group.sort_values('timestamp')\n",
        "            test = group.iloc[-1]\n",
        "            train = group.iloc[:-1]\n",
        "            for _, row in train.iterrows():\n",
        "                train_pairs.append((int(row['user']), int(row['item'])))\n",
        "                train_ratings.append(int(row['rating']))\n",
        "            test_pairs.append((int(test['user']), int(test['item'])))\n",
        "            test_ratings.append(int(test['rating']))\n",
        "        return train_pairs, train_ratings, test_pairs, test_ratings"
      ],
      "metadata": {
        "id": "LsyjJRFOAoOr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gmf.py\n",
        "# gmf.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GMF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim):\n",
        "        super(GMF, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.output = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self._init_weights_()\n",
        "    def _init_weights_(self):\n",
        "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
        "        nn.init.xavier_uniform_(self.output.weight)\n",
        "        if self.output.bias is not None:\n",
        "            nn.init.zeros_(self.output.bias)\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeds = self.user_embedding(user_indices)\n",
        "        item_embeds = self.item_embedding(item_indices)\n",
        "        elementwise_product = user_embeds * item_embeds\n",
        "        logits = self.output(elementwise_product)\n",
        "        rating = self.sigmoid(logits)\n",
        "        return rating.view(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3WnCBqCAoRs",
        "outputId": "243ea654-b4d2-4545-8f66-34a3aa266d30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gmf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "# engine.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class Engine(object):\n",
        "    def __init__(self, config):\n",
        "        self.model = None\n",
        "        self.device = torch.device(\"cuda\" if config['use_cuda'] else \"cpu\")\n",
        "        self.criterion = nn.BCELoss()\n",
        "        if config['optimizer'] == 'adam':\n",
        "            self.optimizer = lambda model: optim.Adam(model.parameters(), lr=config['adam_lr'], weight_decay=config['l2_regularization'])\n",
        "        else:\n",
        "            self.optimizer = lambda model: optim.SGD(model.parameters(), lr=config['adam_lr'], weight_decay=config['l2_regularization'])\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_an_epoch(self, train_loader, epoch):\n",
        "        self.model.train()\n",
        "        optimizer = self.optimizer(self.model)\n",
        "        total_loss = 0\n",
        "        for user, item, label in train_loader:\n",
        "            user = user.view(-1).to(self.device)\n",
        "            item = item.view(-1).to(self.device)\n",
        "            label = label.view(-1).to(self.device)\n",
        "            self.model.zero_grad()\n",
        "            prediction = self.model(user, item)\n",
        "            loss = self.criterion(prediction, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch} Average Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    def evaluate(self, evaluate_data, epoch, k=10):\n",
        "        self.model.eval()\n",
        "        test_users, test_items, neg_users, neg_items = evaluate_data\n",
        "        hits, ndcgs = [], []\n",
        "        num_users = len(test_users)\n",
        "        with torch.no_grad():\n",
        "            for idx in range(num_users):\n",
        "                u = test_users[idx].item()\n",
        "                gtItem = test_items[idx].item()\n",
        "                # 99 negative + 1 positive\n",
        "                item_candidates = [gtItem]\n",
        "                # 99 negatives for this user\n",
        "                item_candidates += [neg_items[i].item() for i in range(idx*99, (idx+1)*99)]\n",
        "                users = torch.LongTensor([u]*100).to(self.device)\n",
        "                items = torch.LongTensor(item_candidates).to(self.device)\n",
        "                predictions = self.model(users, items)\n",
        "                _, indices = torch.topk(predictions, k)\n",
        "                recommends = torch.take(torch.tensor(item_candidates), indices.cpu())\n",
        "                hr = int(gtItem in recommends)\n",
        "                if hr:\n",
        "                    rank = (recommends == gtItem).nonzero(as_tuple=True)[0].item() + 1\n",
        "                    ndcg = np.log(2) / np.log(rank + 1)\n",
        "                else:\n",
        "                    ndcg = 0\n",
        "                hits.append(hr)\n",
        "                ndcgs.append(ndcg)\n",
        "        hr_avg = np.mean(hits)\n",
        "        ndcg_avg = np.mean(ndcgs)\n",
        "        print(f\"Epoch {epoch} HR@{k}: {hr_avg:.4f}, NDCG@{k}: {ndcg_avg:.4f}\")\n",
        "        return hr_avg, ndcg_avg\n",
        "\n",
        "    def save(self, alias, epoch, hr, ndcg):\n",
        "        path = f\"{alias}_epoch{epoch}_hr{hr:.4f}_ndcg{ndcg:.4f}.pth\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9IlJDsKAoUb",
        "outputId": "204dd9db-ba40-4c25-93eb-cdd8afbea41f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate SampleGenerator and datasets\n",
        "sample_generator = SampleGenerator(ratings=ml_ratings)\n",
        "\n",
        "train_dataset = UserItemRatingDataset(\n",
        "    user_item_pairs=sample_generator.train_pairs,\n",
        "    ratings=sample_generator.train_ratings,\n",
        "    num_items=num_items,\n",
        "    num_ng=4,\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# Prepare test set and 99 negatives per user for evaluation\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "test_users = []\n",
        "test_items = []\n",
        "neg_users = []\n",
        "neg_items = []\n",
        "\n",
        "user_item_set = set(zip(ml_ratings['user'], ml_ratings['item']))\n",
        "for idx, (u, i) in enumerate(sample_generator.test_pairs):\n",
        "    test_users.append(u)\n",
        "    test_items.append(i)\n",
        "    negatives = set()\n",
        "    while len(negatives) < 99:\n",
        "        neg = np.random.randint(num_items)\n",
        "        if (u, neg) not in user_item_set:\n",
        "            negatives.add(neg)\n",
        "    for ni in negatives:\n",
        "        neg_users.append(u)\n",
        "        neg_items.append(ni)\n",
        "\n",
        "test_users = torch.LongTensor(test_users)\n",
        "test_items = torch.LongTensor(test_items)\n",
        "neg_users = torch.LongTensor(neg_users)\n",
        "neg_items = torch.LongTensor(neg_items)\n",
        "evaluate_data = (test_users, test_items, neg_users, neg_items)"
      ],
      "metadata": {
        "id": "E7dwmoHuAoW9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug one batch before full training\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (user, item, label) in enumerate(train_loader):\n",
        "        user = user.view(-1).to(engine.device)\n",
        "        item = item.view(-1).to(engine.device)\n",
        "        label = label.view(-1).float().to(engine.device)\n",
        "        output = model(user, item)\n",
        "        print(\"Sample labels:\", label[:10].cpu().numpy())\n",
        "        print(\"Sample predictions:\", output[:10].cpu().numpy())\n",
        "        print(\"Min/max predictions:\", output.min().item(), output.max().item())\n",
        "        loss_fn = torch.nn.BCELoss()\n",
        "        loss = loss_fn(output, label)\n",
        "        print(f\"Loss for this batch: {loss.item():.4f}\")\n",
        "       # break  # Remove this break to check more batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6yOSZ6kFeE7",
        "outputId": "7ad20c40-ac6f-4360-a9c5-0e48527c0bb9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " 0.00859732 0.00669624 0.01711457 0.0079324 ]\n",
            "Min/max predictions: 0.0001176319201476872 0.9347106218338013\n",
            "Loss for this batch: 0.2534\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.24601911 0.5723344  0.10891068 0.43581295 0.06853947 0.30182672\n",
            " 0.02493706 0.01006257 0.13452585 0.06168091]\n",
            "Min/max predictions: 6.841542926849797e-05 0.9447848796844482\n",
            "Loss for this batch: 0.3297\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.809363   0.00912402 0.19349693 0.38928625 0.00439139 0.02228536\n",
            " 0.0507263  0.00848372 0.21610595 0.45622563]\n",
            "Min/max predictions: 0.00015929891378618777 0.8667452931404114\n",
            "Loss for this batch: 0.3697\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.00200144 0.09426749 0.4299387  0.00521309 0.12945653 0.28927553\n",
            " 0.36506462 0.602963   0.70545506 0.36817527]\n",
            "Min/max predictions: 0.0001102673340938054 0.8940492868423462\n",
            "Loss for this batch: 0.3376\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [3.3728682e-02 2.8339904e-02 1.8403658e-01 1.6540647e-01 1.6086405e-02\n",
            " 2.1209130e-01 1.4996295e-01 1.6059183e-02 9.1096961e-01 4.0096373e-04]\n",
            "Min/max predictions: 0.0002968883200082928 0.910969614982605\n",
            "Loss for this batch: 0.3700\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.47710016 0.23309961 0.06670257 0.01929862 0.12410758 0.21112904\n",
            " 0.00298667 0.34929663 0.00921092 0.10132343]\n",
            "Min/max predictions: 6.66849737172015e-05 0.9334744215011597\n",
            "Loss for this batch: 0.3224\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [5.7469684e-01 2.0985711e-01 4.8904210e-02 1.9267142e-02 7.3968671e-02\n",
            " 1.9340314e-02 4.7636973e-03 2.1860191e-04 4.8940599e-02 3.3124167e-04]\n",
            "Min/max predictions: 4.559023727779277e-05 0.9247483611106873\n",
            "Loss for this batch: 0.3400\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04535027 0.34359998 0.28539544 0.14078464 0.0239519  0.07696382\n",
            " 0.11015492 0.00268838 0.2285641  0.40085983]\n",
            "Min/max predictions: 1.746549241943285e-05 0.9107434153556824\n",
            "Loss for this batch: 0.2893\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0354088  0.00329955 0.26130852 0.49367544 0.5401072  0.00719569\n",
            " 0.16345386 0.02843987 0.1496404  0.05224175]\n",
            "Min/max predictions: 6.603696965612471e-05 0.9309086799621582\n",
            "Loss for this batch: 0.3288\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [2.9741234e-01 2.6401475e-01 5.5435777e-02 2.8898721e-03 1.2505960e-02\n",
            " 7.8065274e-04 1.2737036e-01 8.7760073e-01 3.2952058e-03 3.4230977e-01]\n",
            "Min/max predictions: 0.00022088542755227536 0.8806737065315247\n",
            "Loss for this batch: 0.2616\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04173822 0.06818417 0.1913573  0.19252859 0.20861204 0.61433184\n",
            " 0.00374497 0.0776307  0.01089858 0.0040366 ]\n",
            "Min/max predictions: 4.9343903810949996e-05 0.8632594347000122\n",
            "Loss for this batch: 0.3181\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00977954 0.16027994 0.06029338 0.23776807 0.06801017 0.08385105\n",
            " 0.00064975 0.12300136 0.06962477 0.5382209 ]\n",
            "Min/max predictions: 0.00044366123620420694 0.9776667952537537\n",
            "Loss for this batch: 0.3317\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [8.9577781e-03 2.0453449e-01 5.8490735e-01 3.8046267e-02 3.9222553e-01\n",
            " 1.7232144e-01 3.5889295e-04 3.3856586e-01 3.0710191e-01 3.2778504e-01]\n",
            "Min/max predictions: 0.00033419253304600716 0.8930604457855225\n",
            "Loss for this batch: 0.4024\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
            "Sample predictions: [5.9593812e-02 2.4085528e-01 4.5184806e-01 2.0443812e-02 9.5887288e-02\n",
            " 8.0187333e-01 6.2924082e-04 5.2728260e-01 2.6902609e-02 5.8680415e-01]\n",
            "Min/max predictions: 0.0001571226748637855 0.9172818660736084\n",
            "Loss for this batch: 0.3268\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.61460435 0.2618123  0.37472478 0.43477735 0.57591087 0.28055808\n",
            " 0.28680766 0.09961596 0.46157888 0.00715886]\n",
            "Min/max predictions: 0.0001995463971979916 0.8320465683937073\n",
            "Loss for this batch: 0.3595\n",
            "Sample labels: [1. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.8189943  0.2150398  0.6130362  0.26191652 0.01461906 0.8027469\n",
            " 0.18173404 0.15623322 0.04220459 0.19845559]\n",
            "Min/max predictions: 1.1488964446471073e-05 0.982358992099762\n",
            "Loss for this batch: 0.2943\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00290004 0.06262975 0.01105366 0.00338809 0.29577857 0.27757904\n",
            " 0.02116151 0.6190446  0.4990381  0.01074726]\n",
            "Min/max predictions: 0.00036825757706537843 0.8543807864189148\n",
            "Loss for this batch: 0.3600\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [1.32393748e-01 3.39263290e-01 4.71131236e-04 6.03596449e-01\n",
            " 3.23503405e-01 1.01520225e-01 7.65399456e-01 5.47914863e-01\n",
            " 3.30789417e-01 1.04373218e-02]\n",
            "Min/max predictions: 9.630690328776836e-05 0.9746754765510559\n",
            "Loss for this batch: 0.3702\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.29490447 0.20357564 0.26051122 0.10504887 0.00066335 0.13764864\n",
            " 0.03343583 0.28688768 0.12704308 0.05099607]\n",
            "Min/max predictions: 0.0005087960744276643 0.8730752468109131\n",
            "Loss for this batch: 0.3284\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.4484106  0.0625708  0.53747237 0.514063   0.02335696 0.09714606\n",
            " 0.02292151 0.24129769 0.00215685 0.29868925]\n",
            "Min/max predictions: 6.400369784387294e-06 0.9380947947502136\n",
            "Loss for this batch: 0.3491\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.8246821  0.13513385 0.03354107 0.02370914 0.21012773 0.25004607\n",
            " 0.00261139 0.03442731 0.11236127 0.44814196]\n",
            "Min/max predictions: 1.843100653786678e-05 0.9412164092063904\n",
            "Loss for this batch: 0.3609\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.03394385 0.19730085 0.20539045 0.06647626 0.00662373 0.0122597\n",
            " 0.77480423 0.01786904 0.03840495 0.36761802]\n",
            "Min/max predictions: 0.0001914789609145373 0.8237473964691162\n",
            "Loss for this batch: 0.3119\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [7.4516970e-01 3.6866199e-02 5.9651482e-01 4.2042616e-03 2.0650144e-01\n",
            " 4.6780362e-04 6.3834816e-02 2.4591766e-03 2.2150713e-01 1.2156681e-04]\n",
            "Min/max predictions: 0.00012156680895714089 0.9131618738174438\n",
            "Loss for this batch: 0.3909\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10308256 0.06126135 0.8535611  0.38708842 0.53024876 0.00692507\n",
            " 0.0849492  0.01360662 0.00340827 0.16505854]\n",
            "Min/max predictions: 0.00017101688717957586 0.9685847163200378\n",
            "Loss for this batch: 0.4051\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02261052 0.02397184 0.1290802  0.05703355 0.0644845  0.17261706\n",
            " 0.00695619 0.00145343 0.32567725 0.01546175]\n",
            "Min/max predictions: 6.887437484692782e-05 0.9741777777671814\n",
            "Loss for this batch: 0.3433\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [8.4874856e-01 1.1481892e-02 1.7412428e-02 9.8075271e-02 2.3158938e-01\n",
            " 3.8256559e-01 1.5697887e-02 1.8078269e-01 4.7004863e-04 6.4734177e-04]\n",
            "Min/max predictions: 0.00012160983169451356 0.9569776654243469\n",
            "Loss for this batch: 0.3330\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [5.6319463e-01 1.0535099e-01 4.9383573e-02 4.0443228e-03 4.1786811e-04\n",
            " 3.1483728e-03 1.6090585e-01 1.1998118e-01 7.4820840e-01 2.6369080e-01]\n",
            "Min/max predictions: 0.00011152262595715001 0.9557016491889954\n",
            "Loss for this batch: 0.3588\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04928943 0.81641334 0.00951627 0.10204584 0.23089789 0.45124057\n",
            " 0.05001392 0.01969569 0.02467592 0.85819733]\n",
            "Min/max predictions: 2.525919262552634e-05 0.9015148282051086\n",
            "Loss for this batch: 0.3289\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.3588979  0.10425567 0.03172196 0.6013717  0.08907492 0.00188803\n",
            " 0.5725326  0.05208105 0.00102773 0.55241585]\n",
            "Min/max predictions: 2.893446981033776e-05 0.8770098686218262\n",
            "Loss for this batch: 0.2694\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.18477364 0.03460537 0.11402409 0.46031043 0.01085338 0.01356684\n",
            " 0.08099011 0.05211761 0.0264981  0.6425151 ]\n",
            "Min/max predictions: 0.000205452655791305 0.9651240110397339\n",
            "Loss for this batch: 0.3664\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.58004254 0.06277982 0.3761438  0.05425375 0.00239681 0.3444666\n",
            " 0.31379902 0.4769466  0.21796958 0.00158993]\n",
            "Min/max predictions: 6.946610665181652e-05 0.9555938243865967\n",
            "Loss for this batch: 0.3374\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.07717663 0.12484745 0.1537532  0.01317129 0.03117239 0.35124183\n",
            " 0.10458873 0.15232895 0.66870654 0.15001261]\n",
            "Min/max predictions: 0.00023511027393396944 0.9272827506065369\n",
            "Loss for this batch: 0.3017\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.01504938 0.26413578 0.13807406 0.04991928 0.669406   0.30669945\n",
            " 0.857675   0.03568269 0.21814074 0.32581875]\n",
            "Min/max predictions: 3.8944777770666406e-05 0.8816182017326355\n",
            "Loss for this batch: 0.3503\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.21380168 0.03429783 0.60225123 0.19277449 0.54319966 0.07804112\n",
            " 0.00898522 0.00249122 0.02978998 0.41100574]\n",
            "Min/max predictions: 0.00012093840632587671 0.9057263135910034\n",
            "Loss for this batch: 0.3593\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01247976 0.00557856 0.74345386 0.01219783 0.64781475 0.7730925\n",
            " 0.09484851 0.29187503 0.02762605 0.1268364 ]\n",
            "Min/max predictions: 6.640981155214831e-05 0.9401201009750366\n",
            "Loss for this batch: 0.3010\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06540293 0.27706894 0.35968035 0.03674402 0.00417162 0.03806677\n",
            " 0.00305427 0.3348778  0.00875411 0.03864792]\n",
            "Min/max predictions: 4.938151687383652e-05 0.8356068730354309\n",
            "Loss for this batch: 0.3765\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [2.4798753e-02 7.3171429e-02 5.8094078e-01 6.8708664e-01 7.5299853e-01\n",
            " 6.9735385e-04 6.6489793e-02 4.3084064e-01 7.3382378e-01 3.4941888e-01]\n",
            "Min/max predictions: 0.00011443960829637945 0.9066833257675171\n",
            "Loss for this batch: 0.3379\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.65612453 0.01574318 0.00953824 0.5769551  0.1184287  0.07500287\n",
            " 0.00280562 0.03733654 0.22712974 0.02731256]\n",
            "Min/max predictions: 0.00014453096082434058 0.7871557474136353\n",
            "Loss for this batch: 0.3357\n",
            "Sample labels: [1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.4118388  0.12636703 0.2346718  0.37610748 0.0157538  0.74403316\n",
            " 0.03877769 0.55728817 0.48183003 0.00263601]\n",
            "Min/max predictions: 0.00011889143206644803 0.8690797686576843\n",
            "Loss for this batch: 0.3490\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [1.8906634e-01 2.7176630e-01 4.9992517e-02 1.7790617e-01 3.0451471e-03\n",
            " 2.8119961e-02 2.5930786e-01 1.8175659e-04 6.3803661e-01 4.0601413e-03]\n",
            "Min/max predictions: 5.651544779539108e-05 0.9812496304512024\n",
            "Loss for this batch: 0.3274\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.07198185 0.1069197  0.00572672 0.00725869 0.37437564 0.1278783\n",
            " 0.1802216  0.20008497 0.14924316 0.01907551]\n",
            "Min/max predictions: 0.00032841420033946633 0.8173499703407288\n",
            "Loss for this batch: 0.3703\n",
            "Sample labels: [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.3556044e-02 1.8480647e-01 4.4840574e-01 7.6862592e-01 1.3879021e-01\n",
            " 1.0927223e-02 1.9272724e-02 1.5720780e-01 7.1455556e-04 2.1827310e-02]\n",
            "Min/max predictions: 0.00023869937285780907 0.8928468823432922\n",
            "Loss for this batch: 0.3544\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00174938 0.01161303 0.01903796 0.33905956 0.10574757 0.53840476\n",
            " 0.00117391 0.0447178  0.14522044 0.16969159]\n",
            "Min/max predictions: 0.00014275424473453313 0.7816742658615112\n",
            "Loss for this batch: 0.3124\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [9.8769742e-01 2.8858942e-01 4.4750211e-01 1.8765118e-02 1.2751722e-01\n",
            " 1.9021516e-05 1.6025060e-01 1.5616925e-01 8.0254987e-02 4.9766314e-01]\n",
            "Min/max predictions: 1.902151598187629e-05 0.987697422504425\n",
            "Loss for this batch: 0.3351\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.08209908 0.10600317 0.03153454 0.03571275 0.0095152  0.22802357\n",
            " 0.08889856 0.13110076 0.08075492 0.56198436]\n",
            "Min/max predictions: 0.0002905678120441735 0.8865814208984375\n",
            "Loss for this batch: 0.2997\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [6.93630353e-02 1.67428747e-01 2.32837527e-04 1.10235006e-01\n",
            " 1.02386981e-01 1.80464815e-02 5.65636277e-01 1.76484808e-01\n",
            " 8.27490464e-02 3.43328575e-03]\n",
            "Min/max predictions: 6.0128673794679344e-05 0.9217697381973267\n",
            "Loss for this batch: 0.3580\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [3.2350566e-02 1.6129315e-01 3.8421299e-02 2.6037305e-04 8.9728594e-04\n",
            " 4.2561579e-01 1.4186284e-01 8.2906261e-02 6.6266609e-03 6.4387339e-01]\n",
            "Min/max predictions: 5.564238381339237e-05 0.946790874004364\n",
            "Loss for this batch: 0.3256\n",
            "Sample labels: [1. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7088905  0.03243474 0.22267947 0.57211703 0.5906325  0.32664043\n",
            " 0.27295944 0.19431055 0.05856482 0.12435655]\n",
            "Min/max predictions: 1.8506212654756382e-05 0.9116989970207214\n",
            "Loss for this batch: 0.3729\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.21284086 0.6894679  0.36256832 0.00726805 0.499946   0.03788237\n",
            " 0.08696485 0.37439504 0.13118434 0.9447384 ]\n",
            "Min/max predictions: 0.00012368943134788424 0.9447383880615234\n",
            "Loss for this batch: 0.3340\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00126445 0.1046022  0.36231843 0.23846728 0.00685123 0.22453947\n",
            " 0.36196768 0.02192391 0.02881833 0.32171023]\n",
            "Min/max predictions: 0.00030968262581154704 0.8968144059181213\n",
            "Loss for this batch: 0.3208\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.4040081e-03 4.0398762e-02 4.7456095e-01 2.0091498e-02 5.2057254e-05\n",
            " 4.0844046e-02 2.3194215e-01 1.4376409e-01 4.1784765e-03 2.8314306e-03]\n",
            "Min/max predictions: 4.195304791210219e-05 0.9360519051551819\n",
            "Loss for this batch: 0.3469\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01062539 0.09401897 0.01052655 0.36212298 0.02358081 0.14957555\n",
            " 0.00271581 0.03798462 0.03011672 0.02369177]\n",
            "Min/max predictions: 0.0001489482237957418 0.8975957036018372\n",
            "Loss for this batch: 0.3434\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.5749322  0.21041696 0.03873195 0.00107638 0.04366238 0.04227434\n",
            " 0.11664723 0.09848058 0.0427038  0.8990959 ]\n",
            "Min/max predictions: 0.0007669575861655176 0.9732908606529236\n",
            "Loss for this batch: 0.3061\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [2.1803158e-04 1.2253178e-01 4.3576470e-04 2.7278948e-01 4.6210151e-02\n",
            " 3.0933000e-02 4.0319976e-01 5.6360143e-01 9.3004942e-02 6.0655969e-01]\n",
            "Min/max predictions: 0.0002180315786972642 0.9637343883514404\n",
            "Loss for this batch: 0.3321\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.27774632 0.25535375 0.000789   0.22040382 0.05374282 0.07153261\n",
            " 0.06554522 0.15733516 0.01535007 0.3706932 ]\n",
            "Min/max predictions: 0.0002740478084888309 0.9021620750427246\n",
            "Loss for this batch: 0.3877\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.26788554 0.00664947 0.3027282  0.01202147 0.05198685 0.38384807\n",
            " 0.7857139  0.21082664 0.18917492 0.5259684 ]\n",
            "Min/max predictions: 0.0004753905232064426 0.9119463562965393\n",
            "Loss for this batch: 0.3042\n",
            "Sample labels: [1. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.7232196  0.3242806  0.34422868 0.28109387 0.20816433 0.5068093\n",
            " 0.5611822  0.20326488 0.00181405 0.00828859]\n",
            "Min/max predictions: 0.0004565084236674011 0.9067826271057129\n",
            "Loss for this batch: 0.3359\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [5.6174851e-04 7.9232931e-01 3.4282297e-01 2.2674970e-01 1.2662738e-02\n",
            " 2.0907890e-02 3.6798015e-01 5.5274379e-01 1.1963859e-02 3.0978268e-01]\n",
            "Min/max predictions: 0.00027389448950998485 0.8349840641021729\n",
            "Loss for this batch: 0.3262\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.24083504 0.38057318 0.03402132 0.05116485 0.44084665 0.07234457\n",
            " 0.0682813  0.08079012 0.17537087 0.01052558]\n",
            "Min/max predictions: 7.066379475872964e-05 0.9347544312477112\n",
            "Loss for this batch: 0.3132\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.02302959 0.63158125 0.00141084 0.00506147 0.00277982 0.0643963\n",
            " 0.817231   0.00283846 0.13313523 0.22014834]\n",
            "Min/max predictions: 0.00033190962858498096 0.8755128979682922\n",
            "Loss for this batch: 0.3254\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.16810255 0.58840877 0.00426568 0.15829155 0.14874949 0.04563044\n",
            " 0.01427732 0.03770993 0.37815434 0.8172102 ]\n",
            "Min/max predictions: 0.00018761825049296021 0.9761326909065247\n",
            "Loss for this batch: 0.3527\n",
            "Sample labels: [0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00574657 0.51899475 0.21450527 0.7569361  0.04751805 0.56669325\n",
            " 0.3659523  0.09512346 0.01897582 0.46654928]\n",
            "Min/max predictions: 6.274519546423107e-05 0.9400535225868225\n",
            "Loss for this batch: 0.3074\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.14305648 0.19741055 0.13127726 0.31904155 0.16865396 0.11992633\n",
            " 0.5731985  0.0479908  0.30530566 0.73981917]\n",
            "Min/max predictions: 4.7891222493490204e-05 0.8944151401519775\n",
            "Loss for this batch: 0.3609\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05320975 0.04958744 0.6691865  0.33897662 0.31264597 0.1614527\n",
            " 0.03510174 0.17151964 0.01848849 0.06606437]\n",
            "Min/max predictions: 0.00011140602873638272 0.9277678728103638\n",
            "Loss for this batch: 0.3002\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23985127 0.06317361 0.00482608 0.01653409 0.0076453  0.13816352\n",
            " 0.01060692 0.10002236 0.02492359 0.14720643]\n",
            "Min/max predictions: 0.0001131100652855821 0.9533913135528564\n",
            "Loss for this batch: 0.3277\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.20906056 0.00850383 0.17042366 0.7911709  0.21424052 0.27767253\n",
            " 0.00110877 0.83725613 0.11919876 0.08270717]\n",
            "Min/max predictions: 2.269262222398538e-05 0.9537274241447449\n",
            "Loss for this batch: 0.3951\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.9653757  0.00593088 0.00250756 0.5341321  0.2819286  0.22549208\n",
            " 0.3920088  0.57017905 0.5163675  0.10304141]\n",
            "Min/max predictions: 0.0005108077893964946 0.9653757214546204\n",
            "Loss for this batch: 0.3030\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.32597575 0.6421697  0.04676576 0.01169472 0.00095461 0.19269404\n",
            " 0.2099418  0.0224742  0.00548779 0.47823837]\n",
            "Min/max predictions: 0.0002522555005270988 0.8606152534484863\n",
            "Loss for this batch: 0.3309\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2983571  0.00610959 0.01167249 0.03111835 0.2325961  0.08889856\n",
            " 0.08776312 0.24654022 0.6778362  0.04349417]\n",
            "Min/max predictions: 8.681771578267217e-05 0.964117705821991\n",
            "Loss for this batch: 0.3062\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23226482 0.01111505 0.03454262 0.09710147 0.6155676  0.24186766\n",
            " 0.05854058 0.13152415 0.15924588 0.19248058]\n",
            "Min/max predictions: 8.253406849689782e-05 0.9755436182022095\n",
            "Loss for this batch: 0.4129\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01252493 0.12956095 0.23069051 0.01179602 0.23556648 0.05989948\n",
            " 0.3562257  0.08563054 0.00260213 0.44521588]\n",
            "Min/max predictions: 0.0002270741097163409 0.9355204701423645\n",
            "Loss for this batch: 0.3540\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.1504325  0.00469395 0.10655598 0.00710712 0.2324342  0.03732273\n",
            " 0.10145161 0.45136073 0.4945396  0.4783246 ]\n",
            "Min/max predictions: 0.00018690404249355197 0.8739309310913086\n",
            "Loss for this batch: 0.3806\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.0032409  0.12453406 0.03486825 0.12669262 0.02003747 0.17474842\n",
            " 0.2808693  0.50925034 0.22222835 0.00242921]\n",
            "Min/max predictions: 4.7620163968531415e-05 0.9007746577262878\n",
            "Loss for this batch: 0.3747\n",
            "Sample labels: [0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01157413 0.36544174 0.33735666 0.19716962 0.12841924 0.36343613\n",
            " 0.03694575 0.68789    0.03775763 0.01350676]\n",
            "Min/max predictions: 0.00015122033073566854 0.915080189704895\n",
            "Loss for this batch: 0.3137\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4572698  0.01754339 0.00403673 0.5736893  0.00576405 0.34107676\n",
            " 0.2584791  0.2956149  0.42637658 0.2622596 ]\n",
            "Min/max predictions: 5.056002555647865e-05 0.9561214447021484\n",
            "Loss for this batch: 0.3745\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01478239 0.46352062 0.0477684  0.7551118  0.41444504 0.10794254\n",
            " 0.13859817 0.20709516 0.5570878  0.30387464]\n",
            "Min/max predictions: 7.995111081982031e-05 0.8147265911102295\n",
            "Loss for this batch: 0.3591\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [5.0730016e-03 8.1173681e-02 1.0140984e-02 2.2155274e-02 1.5723251e-01\n",
            " 2.8422028e-01 2.2386011e-01 7.4596040e-02 1.0102844e-04 7.6218742e-01]\n",
            "Min/max predictions: 0.00010102843953063712 0.9449658989906311\n",
            "Loss for this batch: 0.3326\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.35434765 0.07267351 0.29026604 0.78019047 0.00103664 0.03321709\n",
            " 0.19212112 0.00552477 0.10596665 0.38288203]\n",
            "Min/max predictions: 0.0002316459867870435 0.8983453512191772\n",
            "Loss for this batch: 0.2959\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.02826539 0.00467762 0.00124039 0.02821912 0.00118243 0.04447487\n",
            " 0.00347649 0.50317764 0.23195384 0.04757399]\n",
            "Min/max predictions: 5.1938048272859305e-05 0.9720388650894165\n",
            "Loss for this batch: 0.2745\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01366199 0.46108028 0.08147687 0.30993876 0.00911875 0.06344754\n",
            " 0.09736168 0.5600908  0.01353089 0.5284097 ]\n",
            "Min/max predictions: 4.2020517867058516e-05 0.9862361550331116\n",
            "Loss for this batch: 0.2988\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.06667905 0.39135125 0.03347945 0.21773854 0.19137366 0.39068577\n",
            " 0.03112299 0.3238762  0.14616723 0.18957186]\n",
            "Min/max predictions: 0.00012040368892485276 0.8784072399139404\n",
            "Loss for this batch: 0.3134\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.26690328 0.24561624 0.13990225 0.31445643 0.5473332  0.03610391\n",
            " 0.5632459  0.36270195 0.08920138 0.2953478 ]\n",
            "Min/max predictions: 0.00012773908383678645 0.9023022055625916\n",
            "Loss for this batch: 0.3519\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.00108786 0.02488766 0.13628222 0.03865096 0.00318362 0.03319691\n",
            " 0.01310712 0.4936293  0.48232788 0.22740422]\n",
            "Min/max predictions: 0.0006141718477010727 0.9026737213134766\n",
            "Loss for this batch: 0.3338\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.82401544 0.41196468 0.3273934  0.14281447 0.32354897 0.08969317\n",
            " 0.15944359 0.2446024  0.10970543 0.51039696]\n",
            "Min/max predictions: 0.00013063097139820457 0.8277738690376282\n",
            "Loss for this batch: 0.3463\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.53634316 0.02732645 0.06951054 0.83687633 0.5653046  0.43620637\n",
            " 0.45227712 0.00332754 0.00149831 0.19377802]\n",
            "Min/max predictions: 0.00031641125679016113 0.9179227948188782\n",
            "Loss for this batch: 0.2878\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [2.9445429e-05 4.7270439e-04 7.7386804e-02 1.7268419e-03 5.2969557e-01\n",
            " 7.1504843e-01 2.0500209e-02 6.6010945e-02 8.5980609e-02 3.1989332e-02]\n",
            "Min/max predictions: 2.9445429390762e-05 0.8808428049087524\n",
            "Loss for this batch: 0.3285\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.23090984 0.11037977 0.36203685 0.24660693 0.2841607  0.4269485\n",
            " 0.3534941  0.86120796 0.6503929  0.4876458 ]\n",
            "Min/max predictions: 0.0002366131666349247 0.9531326293945312\n",
            "Loss for this batch: 0.3415\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.05635768 0.00523859 0.05919255 0.01544594 0.5172063  0.60291964\n",
            " 0.00478667 0.03253518 0.54285675 0.03276745]\n",
            "Min/max predictions: 0.00020173453958705068 0.9531123638153076\n",
            "Loss for this batch: 0.2954\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [8.8465018e-03 1.0002440e-02 3.9003339e-01 2.5313633e-04 2.0440271e-01\n",
            " 2.6578742e-01 3.7866820e-02 2.7194859e-02 5.1014608e-01 7.5672008e-02]\n",
            "Min/max predictions: 5.034314017393626e-05 0.9696059823036194\n",
            "Loss for this batch: 0.3356\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.03334321 0.01605071 0.65710604 0.67122716 0.7643415  0.28033817\n",
            " 0.13864467 0.01153091 0.11782596 0.35030577]\n",
            "Min/max predictions: 3.324163844808936e-05 0.8749141693115234\n",
            "Loss for this batch: 0.3481\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01425282 0.1876082  0.1308918  0.26515755 0.07406703 0.62905025\n",
            " 0.06385438 0.00286141 0.01070267 0.00147154]\n",
            "Min/max predictions: 0.00024391354236286134 0.7741132974624634\n",
            "Loss for this batch: 0.3549\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [9.2676306e-01 7.8281603e-04 2.3897700e-02 1.7956419e-02 4.6915165e-01\n",
            " 6.0013067e-02 8.5405111e-01 7.3867701e-02 1.9750431e-02 3.1554520e-01]\n",
            "Min/max predictions: 0.0003220871731173247 0.9527505040168762\n",
            "Loss for this batch: 0.3489\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [7.5225368e-02 2.0355766e-01 1.2617367e-02 5.7060983e-02 1.3756873e-01\n",
            " 8.2906298e-03 4.6528911e-04 5.1776707e-01 1.8919529e-01 6.0199314e-01]\n",
            "Min/max predictions: 0.00012146171502536163 0.9664453864097595\n",
            "Loss for this batch: 0.4165\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03173932 0.22185549 0.07444404 0.34570974 0.03357385 0.2000223\n",
            " 0.19637239 0.12174205 0.72012895 0.18260115]\n",
            "Min/max predictions: 0.00023866751871537417 0.8416780829429626\n",
            "Loss for this batch: 0.3788\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.42227143 0.42479926 0.07462081 0.00490912 0.00043602 0.13779584\n",
            " 0.27680913 0.04331138 0.06063166 0.1435171 ]\n",
            "Min/max predictions: 0.000195124521269463 0.858793318271637\n",
            "Loss for this batch: 0.3323\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.05911978 0.00421299 0.45036826 0.08762439 0.09578352 0.04643071\n",
            " 0.3398418  0.56826305 0.12440243 0.00683032]\n",
            "Min/max predictions: 5.916277223150246e-05 0.929145097732544\n",
            "Loss for this batch: 0.3439\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03744231 0.557055   0.0123263  0.01880274 0.02558346 0.05601339\n",
            " 0.09536856 0.40702248 0.04187535 0.29414153]\n",
            "Min/max predictions: 2.2738828192814253e-05 0.7964733839035034\n",
            "Loss for this batch: 0.2666\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [9.8760458e-05 1.2602168e-02 6.5969825e-01 8.5157510e-03 1.6130406e-01\n",
            " 1.3241059e-01 3.2667330e-01 1.5811266e-02 1.3856462e-01 6.5135546e-02]\n",
            "Min/max predictions: 3.185062814736739e-05 0.9715372323989868\n",
            "Loss for this batch: 0.3767\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.24784027 0.23022358 0.32295433 0.13935748 0.03702187 0.04674099\n",
            " 0.06717359 0.3173751  0.26522735 0.02957643]\n",
            "Min/max predictions: 5.661264367518015e-05 0.9274982810020447\n",
            "Loss for this batch: 0.2950\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.04410506 0.18249255 0.01197899 0.11096617 0.00486951 0.14827688\n",
            " 0.5876281  0.50112206 0.16637287 0.28001422]\n",
            "Min/max predictions: 8.643112232675776e-05 0.9144725799560547\n",
            "Loss for this batch: 0.2873\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.37304658 0.63553804 0.4675747  0.6879956  0.14266843 0.09653518\n",
            " 0.0763719  0.03462951 0.10742811 0.1106439 ]\n",
            "Min/max predictions: 0.0003031528613064438 0.9110156297683716\n",
            "Loss for this batch: 0.3309\n",
            "Sample labels: [0. 1. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.01207048 0.9560829  0.26346353 0.03075257 0.07211441 0.46031848\n",
            " 0.9004135  0.04336407 0.57018816 0.00217689]\n",
            "Min/max predictions: 9.181843051919714e-05 0.9560828804969788\n",
            "Loss for this batch: 0.3836\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00754508 0.759935   0.0630653  0.07136425 0.6084179  0.00780151\n",
            " 0.7167828  0.01323541 0.5082161  0.13220564]\n",
            "Min/max predictions: 0.0003820428973995149 0.9214442372322083\n",
            "Loss for this batch: 0.3362\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14631358 0.00382792 0.13607678 0.28648636 0.00713721 0.20852727\n",
            " 0.01265893 0.00190172 0.192263   0.59336656]\n",
            "Min/max predictions: 6.946610665181652e-05 0.8492037057876587\n",
            "Loss for this batch: 0.3737\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07765803 0.05474855 0.00715073 0.02722158 0.1068666  0.01261474\n",
            " 0.15862906 0.01479383 0.01579273 0.1008794 ]\n",
            "Min/max predictions: 7.274069503182545e-05 0.9578982591629028\n",
            "Loss for this batch: 0.3271\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03114895 0.28145885 0.0023052  0.5195004  0.29473415 0.24662665\n",
            " 0.00973279 0.02306411 0.23943505 0.13795634]\n",
            "Min/max predictions: 0.00020486600988078862 0.8699581623077393\n",
            "Loss for this batch: 0.3508\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10730065 0.43833774 0.01144694 0.21897238 0.19428252 0.34710854\n",
            " 0.44934425 0.22297706 0.15333113 0.03026899]\n",
            "Min/max predictions: 0.00013485472300089896 0.9297453761100769\n",
            "Loss for this batch: 0.3652\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.6215053  0.01041084 0.42151302 0.09898847 0.0547314  0.5509942\n",
            " 0.20309013 0.0195424  0.18574737 0.01775468]\n",
            "Min/max predictions: 0.0001708072959445417 0.8659983277320862\n",
            "Loss for this batch: 0.2935\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.13599837 0.00528385 0.06915755 0.27582678 0.38933763 0.02707352\n",
            " 0.04569846 0.09642986 0.01612518 0.15317799]\n",
            "Min/max predictions: 7.001136691542342e-05 0.893905520439148\n",
            "Loss for this batch: 0.3542\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.08257581 0.15189052 0.00373028 0.00400665 0.8707709  0.26767185\n",
            " 0.09165526 0.18792152 0.4567672  0.55438846]\n",
            "Min/max predictions: 4.084515603608452e-05 0.9766085147857666\n",
            "Loss for this batch: 0.3750\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04617466 0.11888185 0.04983654 0.77377933 0.00113225 0.00812733\n",
            " 0.00100088 0.2849812  0.23899029 0.05840043]\n",
            "Min/max predictions: 1.878709917946253e-05 0.9335261583328247\n",
            "Loss for this batch: 0.3354\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [7.8034091e-01 5.5224965e-03 2.4464092e-01 2.9948682e-01 9.8841436e-02\n",
            " 4.2951810e-03 4.3591809e-01 3.8869018e-04 2.4006234e-01 7.0288348e-01]\n",
            "Min/max predictions: 0.0002796633052639663 0.9521270990371704\n",
            "Loss for this batch: 0.2961\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.33828285 0.1546982  0.09471723 0.00112493 0.28200608 0.48573744\n",
            " 0.08717097 0.09920479 0.00446325 0.25897634]\n",
            "Min/max predictions: 6.522068724734709e-05 0.8767418265342712\n",
            "Loss for this batch: 0.3400\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10763712 0.57959193 0.00341451 0.01758216 0.01716371 0.02639871\n",
            " 0.00637388 0.01657861 0.02202092 0.09554428]\n",
            "Min/max predictions: 5.823220999445766e-05 0.9463756084442139\n",
            "Loss for this batch: 0.3211\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.5727450e-01 2.9464227e-01 1.3565452e-01 2.7149791e-04 1.7489712e-01\n",
            " 4.9334101e-02 7.4533060e-02 9.9244192e-02 4.1682476e-01 8.4376276e-02]\n",
            "Min/max predictions: 4.05881910410244e-05 0.8913045525550842\n",
            "Loss for this batch: 0.3326\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [8.2654834e-02 3.0820485e-02 3.2714099e-02 3.9292198e-01 2.5969844e-02\n",
            " 3.1738530e-04 3.1991786e-01 2.8399485e-01 2.9492292e-01 3.8773501e-01]\n",
            "Min/max predictions: 2.8388374630594626e-05 0.8940315842628479\n",
            "Loss for this batch: 0.3127\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06060681 0.00052347 0.04516805 0.00358903 0.17932984 0.02623073\n",
            " 0.00139662 0.00206372 0.01047355 0.03253549]\n",
            "Min/max predictions: 0.00022038054885342717 0.953823447227478\n",
            "Loss for this batch: 0.3486\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            "Sample predictions: [0.3225362  0.04064872 0.02134468 0.06026937 0.13499369 0.19018601\n",
            " 0.3457627  0.5735641  0.686306   0.25982276]\n",
            "Min/max predictions: 0.0002165771002182737 0.9443154335021973\n",
            "Loss for this batch: 0.2366\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.11987189 0.4546915  0.33221927 0.24252512 0.00156679 0.0016841\n",
            " 0.09066999 0.45727274 0.04142088 0.17597033]\n",
            "Min/max predictions: 4.887572504230775e-05 0.942106306552887\n",
            "Loss for this batch: 0.3457\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03758564 0.23231447 0.00442448 0.0114934  0.05591089 0.72223943\n",
            " 0.01901339 0.6495542  0.15100497 0.28420165]\n",
            "Min/max predictions: 0.0001995463971979916 0.9575453996658325\n",
            "Loss for this batch: 0.3984\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.67229086 0.4702611  0.09346981 0.55742085 0.6013842  0.00371468\n",
            " 0.4728633  0.1011849  0.19593555 0.92433673]\n",
            "Min/max predictions: 0.00015701244410593063 0.9354162216186523\n",
            "Loss for this batch: 0.3197\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.5266059  0.04644651 0.07836563 0.00950877 0.09053264 0.01502665\n",
            " 0.0279962  0.26525912 0.39780152 0.15370457]\n",
            "Min/max predictions: 0.0003089972597081214 0.945080578327179\n",
            "Loss for this batch: 0.4116\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.00967109 0.0146231  0.00222196 0.08786434 0.44021466 0.00603647\n",
            " 0.18196629 0.10886887 0.03406736 0.3417456 ]\n",
            "Min/max predictions: 0.0001404933718731627 0.8990193009376526\n",
            "Loss for this batch: 0.3374\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.14760664 0.16145928 0.88690555 0.36176446 0.04241544 0.0055462\n",
            " 0.03822574 0.02833757 0.359339   0.13077591]\n",
            "Min/max predictions: 5.266693915473297e-05 0.8869055509567261\n",
            "Loss for this batch: 0.4048\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.1582898  0.5662257  0.5979958  0.5948593  0.23296419 0.21084529\n",
            " 0.02187795 0.42179415 0.38417774 0.0290367 ]\n",
            "Min/max predictions: 5.0630857003852725e-05 0.9489728808403015\n",
            "Loss for this batch: 0.3441\n",
            "Sample labels: [0. 1. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.0350684  0.57527393 0.15011415 0.7460914  0.5579762  0.01548208\n",
            " 0.00215379 0.01031418 0.5626457  0.01239032]\n",
            "Min/max predictions: 3.363561336300336e-05 0.9422631859779358\n",
            "Loss for this batch: 0.3471\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.265636   0.00311223 0.02707997 0.01761206 0.6714631  0.03231015\n",
            " 0.01552598 0.29300383 0.02176057 0.00270082]\n",
            "Min/max predictions: 0.00026483938563615084 0.8928402066230774\n",
            "Loss for this batch: 0.3575\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.02387208 0.15391667 0.29629347 0.00707315 0.14416523 0.00329997\n",
            " 0.37640437 0.38901243 0.01231336 0.50418335]\n",
            "Min/max predictions: 5.034314017393626e-05 0.9465537071228027\n",
            "Loss for this batch: 0.3462\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.08666852 0.12428804 0.2614785  0.00638644 0.02839473 0.1070871\n",
            " 0.15758543 0.17173494 0.0016901  0.05479185]\n",
            "Min/max predictions: 1.0648429451975971e-05 0.880146324634552\n",
            "Loss for this batch: 0.3315\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10609949 0.30362138 0.06319338 0.01921971 0.03462515 0.00083849\n",
            " 0.02511853 0.07170336 0.00563934 0.6570954 ]\n",
            "Min/max predictions: 0.00015663981321267784 0.9460266828536987\n",
            "Loss for this batch: 0.3190\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            "Sample predictions: [0.53914833 0.12419454 0.12400006 0.5459277  0.02403845 0.26008806\n",
            " 0.2069669  0.80395114 0.00378831 0.5925089 ]\n",
            "Min/max predictions: 3.437260238570161e-05 0.9286497235298157\n",
            "Loss for this batch: 0.3764\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07780921 0.09566176 0.13105278 0.3923698  0.07531292 0.22956042\n",
            " 0.49261054 0.27628407 0.00399686 0.03788088]\n",
            "Min/max predictions: 2.9590513804578222e-05 0.9759721159934998\n",
            "Loss for this batch: 0.3929\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.01296602 0.65992224 0.14287788 0.00323289 0.0051198  0.0026601\n",
            " 0.39391783 0.12229096 0.00094799 0.64316064]\n",
            "Min/max predictions: 0.00011969384650001302 0.8784221410751343\n",
            "Loss for this batch: 0.3705\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
            "Sample predictions: [0.0266864  0.14903061 0.00609768 0.5826571  0.38635948 0.41785735\n",
            " 0.00263072 0.62253183 0.02972584 0.4033735 ]\n",
            "Min/max predictions: 9.134106221608818e-06 0.9102247953414917\n",
            "Loss for this batch: 0.3419\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [3.1154245e-01 2.0413636e-03 2.8464384e-04 8.0059059e-03 4.1340262e-01\n",
            " 1.3579643e-01 3.6486959e-01 5.0664234e-01 1.5017951e-01 5.2479595e-01]\n",
            "Min/max predictions: 0.000202417591935955 0.8942620158195496\n",
            "Loss for this batch: 0.3594\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [3.1551474e-01 2.6381031e-01 2.6456869e-01 2.8767873e-04 2.5425738e-01\n",
            " 1.2244614e-01 1.5583201e-01 9.3530819e-02 2.3072967e-01 4.5651130e-02]\n",
            "Min/max predictions: 0.00015103603072930127 0.9129114151000977\n",
            "Loss for this batch: 0.4500\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            "Sample predictions: [0.6064418  0.00543454 0.03852351 0.8147032  0.00675028 0.44127405\n",
            " 0.3708609  0.1982137  0.52804184 0.8487439 ]\n",
            "Min/max predictions: 0.00010805564670590684 0.9798552989959717\n",
            "Loss for this batch: 0.3724\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.1232746  0.7092706  0.27849868 0.05661859 0.20954137 0.6110409\n",
            " 0.08107714 0.457698   0.24866463 0.2630823 ]\n",
            "Min/max predictions: 0.00015232033911161125 0.9159961938858032\n",
            "Loss for this batch: 0.3276\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.47381818 0.02404755 0.00574921 0.35648325 0.10488163 0.00494475\n",
            " 0.00613427 0.21677487 0.04094306 0.00782208]\n",
            "Min/max predictions: 1.1488964446471073e-05 0.8952990174293518\n",
            "Loss for this batch: 0.3213\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02766107 0.02718567 0.01215067 0.01237475 0.0287329  0.00663007\n",
            " 0.10733598 0.07801318 0.01041816 0.5154095 ]\n",
            "Min/max predictions: 0.00016571501328144222 0.9663395285606384\n",
            "Loss for this batch: 0.3051\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.736715   0.42435887 0.12152056 0.9417595  0.11699998 0.00283064\n",
            " 0.14619882 0.00684263 0.05084548 0.43561321]\n",
            "Min/max predictions: 5.4667991207679734e-05 0.9417595267295837\n",
            "Loss for this batch: 0.3287\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00387537 0.00179988 0.00149517 0.63076985 0.05666479 0.24349026\n",
            " 0.5122467  0.7502723  0.2584675  0.01765228]\n",
            "Min/max predictions: 0.00011254038690822199 0.9698613286018372\n",
            "Loss for this batch: 0.3469\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2796722  0.17702022 0.00492935 0.26503938 0.01606891 0.5344893\n",
            " 0.00137156 0.04627016 0.608141   0.39376548]\n",
            "Min/max predictions: 1.955980587808881e-05 0.942060649394989\n",
            "Loss for this batch: 0.3547\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.04926144 0.02239783 0.04137928 0.07476313 0.06502301 0.07814722\n",
            " 0.21334983 0.00093766 0.18142807 0.1907746 ]\n",
            "Min/max predictions: 0.0002663031336851418 0.8384309411048889\n",
            "Loss for this batch: 0.3145\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6765259  0.00411941 0.00479789 0.00291353 0.03771475 0.01131964\n",
            " 0.06034482 0.52232414 0.00746069 0.04574502]\n",
            "Min/max predictions: 2.5997613192885183e-05 0.8484246730804443\n",
            "Loss for this batch: 0.3723\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.00188394 0.07203552 0.04975462 0.9550515  0.35742968 0.10938061\n",
            " 0.61679274 0.36694583 0.09511477 0.02794832]\n",
            "Min/max predictions: 5.9118894569110125e-05 0.9550514817237854\n",
            "Loss for this batch: 0.3028\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10532507 0.00928999 0.02893745 0.05911656 0.5834093  0.01084905\n",
            " 0.07061564 0.4035011  0.9224386  0.41187873]\n",
            "Min/max predictions: 0.0003238292410969734 0.9668552279472351\n",
            "Loss for this batch: 0.3128\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
            "Sample predictions: [0.48910365 0.7519427  0.0240107  0.00939669 0.42903587 0.00658588\n",
            " 0.73233515 0.17661688 0.54940504 0.44738412]\n",
            "Min/max predictions: 1.4033376828592736e-05 0.9721229076385498\n",
            "Loss for this batch: 0.3252\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00385182 0.0032363  0.31038556 0.15158714 0.06794435 0.00253551\n",
            " 0.01993894 0.70433253 0.01310938 0.01776502]\n",
            "Min/max predictions: 0.00014330139674711972 0.8629326224327087\n",
            "Loss for this batch: 0.2869\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.09805948 0.10996755 0.0122304  0.00098839 0.5827409  0.33759695\n",
            " 0.12236246 0.45292968 0.37578437 0.37510332]\n",
            "Min/max predictions: 6.808954640291631e-05 0.9784014821052551\n",
            "Loss for this batch: 0.3395\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.38762364 0.28556195 0.00375408 0.43072596 0.23508802 0.31049898\n",
            " 0.6249946  0.0127437  0.00919716 0.00192805]\n",
            "Min/max predictions: 9.685118129709736e-05 0.7976532578468323\n",
            "Loss for this batch: 0.2882\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2860348  0.46431863 0.22694878 0.35725984 0.00824536 0.012123\n",
            " 0.20048511 0.01221226 0.01316413 0.01335953]\n",
            "Min/max predictions: 4.2076535464730114e-05 0.9880654811859131\n",
            "Loss for this batch: 0.3011\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.14952101 0.32579508 0.21600023 0.20095274 0.00174704 0.17173895\n",
            " 0.5704846  0.34601337 0.2554824  0.0056396 ]\n",
            "Min/max predictions: 8.689540845807642e-05 0.8407506346702576\n",
            "Loss for this batch: 0.3425\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10176514 0.13077536 0.02789359 0.2737061  0.24155408 0.57707334\n",
            " 0.00118996 0.03738521 0.12540375 0.12798756]\n",
            "Min/max predictions: 5.252409027889371e-05 0.8557760715484619\n",
            "Loss for this batch: 0.3623\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.53872645 0.25687164 0.00791974 0.5083487  0.55806094 0.35428524\n",
            " 0.2856435  0.0451633  0.01729036 0.04658868]\n",
            "Min/max predictions: 0.00012581853661686182 0.9640517830848694\n",
            "Loss for this batch: 0.2914\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.01348827 0.04777285 0.02480421 0.2303287  0.07339058 0.00739693\n",
            " 0.06125341 0.69301325 0.01269221 0.17277814]\n",
            "Min/max predictions: 2.3223085008794442e-05 0.8757936954498291\n",
            "Loss for this batch: 0.2610\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00149658 0.08128179 0.04049708 0.26891184 0.0971598  0.06431163\n",
            " 0.03803783 0.90996796 0.45052528 0.36955547]\n",
            "Min/max predictions: 7.394101703539491e-05 0.9486170411109924\n",
            "Loss for this batch: 0.3153\n",
            "Sample labels: [1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.45392486 0.08944187 0.77831674 0.10515667 0.00462216 0.82521534\n",
            " 0.09645319 0.07817349 0.586911   0.00132543]\n",
            "Min/max predictions: 0.00018330661987420171 0.9015692472457886\n",
            "Loss for this batch: 0.3285\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.13844918 0.0327644  0.0586626  0.7245447  0.34592837 0.00877679\n",
            " 0.00216454 0.25435126 0.52948403 0.26001313]\n",
            "Min/max predictions: 0.00038132444024086 0.9239538311958313\n",
            "Loss for this batch: 0.4098\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.14639047 0.7534868  0.21777989 0.21325202 0.06233834 0.14100063\n",
            " 0.48347777 0.14903487 0.403888   0.06458237]\n",
            "Min/max predictions: 0.00031639012740924954 0.8695671558380127\n",
            "Loss for this batch: 0.2791\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.26074544 0.16952394 0.03194978 0.31581786 0.01596535 0.03252527\n",
            " 0.01635939 0.16091704 0.13649534 0.04681152]\n",
            "Min/max predictions: 0.0005778708728030324 0.8592510223388672\n",
            "Loss for this batch: 0.4020\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.0357678e-03 3.9035499e-01 3.1802312e-01 2.7710071e-01 9.8306105e-02\n",
            " 1.7537076e-04 1.4125648e-02 3.2470247e-01 3.0888587e-01 4.0579978e-03]\n",
            "Min/max predictions: 0.00014374946476891637 0.9449267387390137\n",
            "Loss for this batch: 0.3326\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [2.3757549e-01 6.5837186e-03 2.4309823e-01 1.0927258e-02 6.5153825e-01\n",
            " 5.8860966e-04 8.2917958e-02 1.9145624e-01 1.5123024e-03 1.0351524e-01]\n",
            "Min/max predictions: 0.00016148974827956408 0.9136331677436829\n",
            "Loss for this batch: 0.2899\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.04761823 0.14363533 0.00105976 0.00156417 0.03273696 0.2117405\n",
            " 0.2887301  0.02038788 0.02347335 0.00516172]\n",
            "Min/max predictions: 0.00010198612289968878 0.9759336113929749\n",
            "Loss for this batch: 0.3518\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.40633792 0.00238833 0.03715356 0.09440216 0.6938958  0.04576631\n",
            " 0.01306634 0.8835852  0.0391269  0.02050677]\n",
            "Min/max predictions: 1.8675382307264954e-05 0.9066171050071716\n",
            "Loss for this batch: 0.3088\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [2.5560567e-01 5.3871580e-04 3.1131643e-01 5.3476468e-03 2.0832433e-03\n",
            " 3.4428255e-03 2.0740795e-01 3.3896931e-02 8.2117841e-02 5.4815984e-01]\n",
            "Min/max predictions: 0.00021925903274677694 0.9071180820465088\n",
            "Loss for this batch: 0.3617\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.08089746 0.12502624 0.13329141 0.01392439 0.00715264 0.46188176\n",
            " 0.10376915 0.5833179  0.29152876 0.27249676]\n",
            "Min/max predictions: 0.0003887625935021788 0.90925133228302\n",
            "Loss for this batch: 0.3487\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19350052 0.00836665 0.03118235 0.17423497 0.16567819 0.01514818\n",
            " 0.14537106 0.0394889  0.0123792  0.0759794 ]\n",
            "Min/max predictions: 0.0002491610066499561 0.9114811420440674\n",
            "Loss for this batch: 0.3153\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.4778947  0.08184271 0.00313864 0.26395118 0.02123182 0.20890024\n",
            " 0.00402264 0.04300109 0.37547764 0.34645975]\n",
            "Min/max predictions: 0.00029580731643363833 0.8883809447288513\n",
            "Loss for this batch: 0.3795\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.0260885e-02 5.8771217e-01 2.4294779e-02 2.7562531e-02 8.7010467e-01\n",
            " 2.7349059e-02 4.3619961e-01 2.2236390e-02 4.1851812e-04 1.1527500e-03]\n",
            "Min/max predictions: 9.405607124790549e-05 0.9437172412872314\n",
            "Loss for this batch: 0.4148\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15553135 0.19849657 0.01010778 0.00378201 0.10389171 0.68207896\n",
            " 0.31013885 0.08451395 0.01355936 0.03926495]\n",
            "Min/max predictions: 0.0002753589069470763 0.958268404006958\n",
            "Loss for this batch: 0.4043\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.17087893 0.00388793 0.09132651 0.24160175 0.38701805 0.00204082\n",
            " 0.00952851 0.33552262 0.76905936 0.11076487]\n",
            "Min/max predictions: 0.00014453096082434058 0.901448667049408\n",
            "Loss for this batch: 0.3636\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.05165001 0.44379756 0.0959861  0.04743229 0.74124616 0.06547748\n",
            " 0.2604496  0.00522899 0.44492674 0.34495145]\n",
            "Min/max predictions: 0.00026447855634614825 0.9943102598190308\n",
            "Loss for this batch: 0.3061\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.3374112  0.09805509 0.00307682 0.48058075 0.9105342  0.45823568\n",
            " 0.01043289 0.56755465 0.14013967 0.01723133]\n",
            "Min/max predictions: 0.00018516587442718446 0.9590750932693481\n",
            "Loss for this batch: 0.3384\n",
            "Sample labels: [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.46506542 0.5781542  0.08296721 0.35565192 0.09063791 0.36299816\n",
            " 0.05750301 0.05066555 0.003018   0.71099603]\n",
            "Min/max predictions: 8.822969539323822e-05 0.8972300291061401\n",
            "Loss for this batch: 0.3168\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.03463355 0.04002212 0.2558205  0.00790829 0.01354104 0.16735186\n",
            " 0.03489202 0.27587458 0.8214023  0.09592338]\n",
            "Min/max predictions: 0.0001255798852071166 0.8947687149047852\n",
            "Loss for this batch: 0.3008\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.48546836 0.09331042 0.0022701  0.7970637  0.0132318  0.22068663\n",
            " 0.29562312 0.05409764 0.25727722 0.09305895]\n",
            "Min/max predictions: 2.8573855161084794e-05 0.9071762561798096\n",
            "Loss for this batch: 0.2830\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.01120926 0.43258122 0.82440925 0.31508735 0.04770244 0.64831084\n",
            " 0.18505207 0.00966936 0.18836944 0.02253669]\n",
            "Min/max predictions: 9.51837282627821e-05 0.8801206946372986\n",
            "Loss for this batch: 0.3341\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04094306 0.46096784 0.12122832 0.4162994  0.01086732 0.14673339\n",
            " 0.07521357 0.00159248 0.37737668 0.02050266]\n",
            "Min/max predictions: 3.495898636174388e-05 0.8997511863708496\n",
            "Loss for this batch: 0.3637\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.34854314 0.11059966 0.18184419 0.28958312 0.2880313  0.16348511\n",
            " 0.05456593 0.7325172  0.22380796 0.863437  ]\n",
            "Min/max predictions: 6.482669414253905e-05 0.9342247843742371\n",
            "Loss for this batch: 0.3463\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.28143093 0.31776452 0.03168064 0.03089611 0.02653095 0.01887042\n",
            " 0.08637991 0.10015434 0.28442326 0.48208484]\n",
            "Min/max predictions: 4.250248457537964e-05 0.9828197360038757\n",
            "Loss for this batch: 0.3451\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.54497546 0.01769418 0.00654476 0.26386827 0.01046943 0.14172862\n",
            " 0.00632942 0.02724261 0.10969649 0.13876356]\n",
            "Min/max predictions: 0.0002449632156640291 0.8404616117477417\n",
            "Loss for this batch: 0.3764\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [0.28972498 0.31919536 0.00493001 0.05498439 0.00109874 0.00492621\n",
            " 0.04615766 0.29848522 0.33195302 0.06680482]\n",
            "Min/max predictions: 8.682947373017669e-05 0.9448614120483398\n",
            "Loss for this batch: 0.2635\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.18520625 0.47912398 0.20786248 0.04545574 0.46152386 0.5428991\n",
            " 0.07582114 0.19630381 0.02818939 0.3153128 ]\n",
            "Min/max predictions: 0.00014460802776739 0.9027243256568909\n",
            "Loss for this batch: 0.3360\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.32216012 0.30169895 0.00736312 0.05577639 0.00656973 0.14309905\n",
            " 0.10018567 0.35778376 0.37599483 0.09552841]\n",
            "Min/max predictions: 2.335982208023779e-05 0.8966132998466492\n",
            "Loss for this batch: 0.3670\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.08031849 0.42705587 0.7961685  0.02686716 0.29020697 0.00171262\n",
            " 0.0008479  0.52453417 0.3473775  0.10295764]\n",
            "Min/max predictions: 5.44070171599742e-05 0.9632745385169983\n",
            "Loss for this batch: 0.3037\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07493389 0.06662371 0.24170595 0.46407261 0.00409516 0.28683287\n",
            " 0.02000327 0.12610626 0.29750443 0.31410873]\n",
            "Min/max predictions: 5.392138700699434e-05 0.9386221766471863\n",
            "Loss for this batch: 0.3645\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.3180839e-01 2.0310190e-03 5.4171455e-01 1.4887024e-01 4.9541998e-03\n",
            " 6.7473669e-03 1.5908794e-01 1.0272377e-01 5.7259125e-01 3.5009498e-04]\n",
            "Min/max predictions: 0.000343340914696455 0.8882119655609131\n",
            "Loss for this batch: 0.3243\n",
            "Sample labels: [0. 0. 1. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.01746023 0.2435889  0.71425146 0.4045209  0.39449555 0.50330627\n",
            " 0.07868185 0.78380656 0.12090873 0.08589116]\n",
            "Min/max predictions: 0.0002583761524874717 0.9619580507278442\n",
            "Loss for this batch: 0.3978\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.12678225 0.13446417 0.8098099  0.03055664 0.09325186 0.05881833\n",
            " 0.00140305 0.1498599  0.77485114 0.16573542]\n",
            "Min/max predictions: 0.00012552397674880922 0.9738613367080688\n",
            "Loss for this batch: 0.3069\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01396959 0.03750944 0.86502934 0.00655702 0.5469898  0.03301677\n",
            " 0.0385366  0.24033132 0.00927091 0.04057886]\n",
            "Min/max predictions: 0.00029621145222336054 0.9632374048233032\n",
            "Loss for this batch: 0.3388\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.20899262 0.01124035 0.00567678 0.0586194  0.31267664 0.35268557\n",
            " 0.01406188 0.07476313 0.5304946  0.00629286]\n",
            "Min/max predictions: 8.206562051782385e-05 0.9324326515197754\n",
            "Loss for this batch: 0.3581\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.33672357 0.02111683 0.14023274 0.8263445  0.39137468 0.4124082\n",
            " 0.28604937 0.02303157 0.00128853 0.02195356]\n",
            "Min/max predictions: 3.618593837018125e-05 0.8733277916908264\n",
            "Loss for this batch: 0.3184\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.2593904  0.19471617 0.03506578 0.2350881  0.00927331 0.5252907\n",
            " 0.23633815 0.22501321 0.31226328 0.17758462]\n",
            "Min/max predictions: 0.00019842415349557996 0.9620976448059082\n",
            "Loss for this batch: 0.3211\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.12659587 0.0655049  0.00790007 0.38563284 0.00119978 0.6746337\n",
            " 0.28670555 0.12670381 0.50144076 0.23724034]\n",
            "Min/max predictions: 0.00012247494305483997 0.9350484013557434\n",
            "Loss for this batch: 0.3393\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [4.5824105e-01 1.0283385e-01 2.9518202e-01 2.0656246e-04 1.6911145e-01\n",
            " 8.8379785e-02 5.4891700e-01 6.0981095e-02 6.1049449e-01 9.8616281e-04]\n",
            "Min/max predictions: 0.00013689974730368704 0.8745739459991455\n",
            "Loss for this batch: 0.3433\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.09273785 0.22563331 0.11558459 0.00248713 0.00541437 0.06573185\n",
            " 0.02626904 0.1512825  0.43469876 0.01937195]\n",
            "Min/max predictions: 0.00013724388554692268 0.962395429611206\n",
            "Loss for this batch: 0.3789\n",
            "Sample labels: [1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4184889  0.5299421  0.20841873 0.03114243 0.00266693 0.2603822\n",
            " 0.00492892 0.00717206 0.03705837 0.6583674 ]\n",
            "Min/max predictions: 7.053351873764768e-05 0.9581161737442017\n",
            "Loss for this batch: 0.3366\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.15931284 0.24355248 0.08657853 0.38369784 0.9079829  0.05305634\n",
            " 0.38960236 0.0395     0.19954073 0.13319309]\n",
            "Min/max predictions: 0.0001787584478734061 0.9861617088317871\n",
            "Loss for this batch: 0.3582\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.40695795 0.23298684 0.11094281 0.00068724 0.39244902 0.5787817\n",
            " 0.03590985 0.00184696 0.0344024  0.00378489]\n",
            "Min/max predictions: 0.00015095251728780568 0.9176899194717407\n",
            "Loss for this batch: 0.3479\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.73202026 0.7786996  0.51029456 0.05902013 0.69493365 0.00245463\n",
            " 0.10143369 0.00531288 0.00158151 0.00333846]\n",
            "Min/max predictions: 6.3229315856006e-05 0.8980864882469177\n",
            "Loss for this batch: 0.3646\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [2.1927727e-03 4.7012074e-03 2.5212011e-01 2.1016559e-01 6.4477336e-01\n",
            " 9.0225730e-03 5.0430679e-01 7.7117974e-01 1.2814499e-01 5.2520834e-05]\n",
            "Min/max predictions: 2.9451608497765847e-05 0.9593741297721863\n",
            "Loss for this batch: 0.2878\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [3.2711285e-01 6.8704849e-01 2.3473352e-01 4.1886494e-01 2.6291093e-01\n",
            " 2.1215908e-04 1.0160081e-02 2.3862782e-01 5.0947678e-01 1.1575423e-01]\n",
            "Min/max predictions: 5.971524296910502e-05 0.965556800365448\n",
            "Loss for this batch: 0.3357\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00098995 0.01686091 0.3989413  0.00395125 0.712063   0.5987054\n",
            " 0.5224939  0.18365443 0.0037246  0.2524437 ]\n",
            "Min/max predictions: 0.000251870253123343 0.9217856526374817\n",
            "Loss for this batch: 0.3252\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.49035758 0.14033055 0.66352355 0.31489187 0.67858225 0.01884366\n",
            " 0.5032041  0.23337956 0.04584192 0.15302049]\n",
            "Min/max predictions: 0.0001203323045047 0.9752189517021179\n",
            "Loss for this batch: 0.3172\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03411841 0.00756627 0.60493284 0.10109235 0.09967507 0.07568099\n",
            " 0.00176683 0.20490389 0.75999266 0.02474902]\n",
            "Min/max predictions: 0.00016533542657271028 0.9350618124008179\n",
            "Loss for this batch: 0.3481\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.02112905 0.06916603 0.23485681 0.3380506  0.50161463 0.01545271\n",
            " 0.01049918 0.21643494 0.148598   0.8029529 ]\n",
            "Min/max predictions: 0.00016049944679252803 0.9517356753349304\n",
            "Loss for this batch: 0.4101\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00552278 0.20715821 0.6299939  0.29403293 0.7408894  0.00276386\n",
            " 0.00265427 0.04131493 0.7608894  0.24782446]\n",
            "Min/max predictions: 0.00013458341709338129 0.8644075989723206\n",
            "Loss for this batch: 0.3125\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3252815  0.05442627 0.289202   0.14021535 0.1450799  0.06511563\n",
            " 0.0665679  0.14480716 0.08616357 0.31808287]\n",
            "Min/max predictions: 3.928716978407465e-05 0.898382306098938\n",
            "Loss for this batch: 0.3652\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3712362  0.02094005 0.03244129 0.0160706  0.11626634 0.02826067\n",
            " 0.03563852 0.03890998 0.00782853 0.08836041]\n",
            "Min/max predictions: 0.00019649360910989344 0.9480626583099365\n",
            "Loss for this batch: 0.3661\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.60727715 0.23174277 0.21320675 0.05775495 0.38488218 0.02281716\n",
            " 0.11542705 0.20773803 0.15749623 0.36219516]\n",
            "Min/max predictions: 0.00019451245316304266 0.8833699226379395\n",
            "Loss for this batch: 0.3516\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01094571 0.01704168 0.04595606 0.02327841 0.00699309 0.35913083\n",
            " 0.1741526  0.11161613 0.01403476 0.5340905 ]\n",
            "Min/max predictions: 0.0003295326023362577 0.902137815952301\n",
            "Loss for this batch: 0.3253\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.2898629  0.12601797 0.02402687 0.03720352 0.09766494 0.02667041\n",
            " 0.44140494 0.15940191 0.03244189 0.44420692]\n",
            "Min/max predictions: 6.727746222168207e-05 0.9235782027244568\n",
            "Loss for this batch: 0.3942\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 1. 0. 1. 1.]\n",
            "Sample predictions: [0.01025877 0.23799625 0.2651453  0.69867337 0.00096263 0.6813004\n",
            " 0.6156358  0.02135707 0.6770306  0.68538386]\n",
            "Min/max predictions: 5.876170325791463e-05 0.8421816825866699\n",
            "Loss for this batch: 0.3262\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02687164 0.2623975  0.03946187 0.15304945 0.01837002 0.44756892\n",
            " 0.53982025 0.00395502 0.05407652 0.10080906]\n",
            "Min/max predictions: 4.260864807292819e-05 0.8917044401168823\n",
            "Loss for this batch: 0.3902\n",
            "Sample labels: [0. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [5.5740353e-02 8.3421713e-01 3.2726489e-02 4.3628230e-03 6.9229954e-01\n",
            " 2.5256595e-04 1.2856780e-01 6.5876497e-03 4.3447960e-02 1.1887052e-01]\n",
            "Min/max predictions: 0.0001554233895149082 0.8342171311378479\n",
            "Loss for this batch: 0.3105\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.01653114 0.00409745 0.03739155 0.00215645 0.37544858 0.01352201\n",
            " 0.16509618 0.08354004 0.4966731  0.29554197]\n",
            "Min/max predictions: 0.00027699352358467877 0.9811820387840271\n",
            "Loss for this batch: 0.3464\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.2615746  0.13988078 0.3584891  0.00372275 0.195396   0.01912407\n",
            " 0.00225229 0.4616313  0.30270392 0.04488732]\n",
            "Min/max predictions: 5.1927152526332065e-05 0.8312135338783264\n",
            "Loss for this batch: 0.3005\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3579863  0.29057807 0.62250084 0.42125016 0.01170104 0.01236043\n",
            " 0.5282096  0.21714205 0.07068399 0.20267116]\n",
            "Min/max predictions: 0.0002044823340838775 0.9149574637413025\n",
            "Loss for this batch: 0.3381\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19627333 0.02304122 0.19801375 0.00054549 0.00222611 0.35742423\n",
            " 0.22817357 0.00434006 0.00426148 0.01790412]\n",
            "Min/max predictions: 4.537423956207931e-05 0.8826859593391418\n",
            "Loss for this batch: 0.3883\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.15364881 0.11800813 0.0337428  0.01675935 0.0013362  0.01677893\n",
            " 0.5065352  0.08864482 0.1914924  0.23841196]\n",
            "Min/max predictions: 4.2881674744421616e-05 0.9286386966705322\n",
            "Loss for this batch: 0.3826\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06118312 0.14246021 0.11865952 0.02504181 0.01314391 0.3318418\n",
            " 0.20342004 0.14810535 0.03049029 0.63783526]\n",
            "Min/max predictions: 0.00030265148961916566 0.9013834595680237\n",
            "Loss for this batch: 0.3634\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.01823751 0.31524882 0.5510267  0.7570335  0.0192569  0.00136452\n",
            " 0.5549272  0.5373194  0.02294628 0.03311069]\n",
            "Min/max predictions: 0.00030338557553477585 0.9362168908119202\n",
            "Loss for this batch: 0.4205\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.34394792 0.25463352 0.05373992 0.46382514 0.3035121  0.70756364\n",
            " 0.08425557 0.05050689 0.05313241 0.00979628]\n",
            "Min/max predictions: 0.0002857126237358898 0.9421088695526123\n",
            "Loss for this batch: 0.2780\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00079382 0.6053248  0.07368507 0.01886393 0.00102414 0.00438788\n",
            " 0.5630903  0.08942194 0.12144868 0.3475409 ]\n",
            "Min/max predictions: 0.0001014572917483747 0.8574779033660889\n",
            "Loss for this batch: 0.3705\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.27704147 0.1945899  0.44398922 0.05286164 0.6490481  0.01694041\n",
            " 0.5928267  0.01938732 0.25285402 0.24730487]\n",
            "Min/max predictions: 8.817543857730925e-05 0.9409517049789429\n",
            "Loss for this batch: 0.3669\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7015523  0.6242265  0.00113738 0.11875551 0.06206135 0.14915651\n",
            " 0.2286967  0.37104285 0.01020222 0.3058414 ]\n",
            "Min/max predictions: 5.5478398280683905e-05 0.9476671814918518\n",
            "Loss for this batch: 0.3592\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.15805706 0.03388013 0.4725767  0.00417585 0.1572807  0.593186\n",
            " 0.4274536  0.00098179 0.00137689 0.04637472]\n",
            "Min/max predictions: 1.958550637937151e-05 0.9532613158226013\n",
            "Loss for this batch: 0.3376\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.03493672 0.243991   0.05790733 0.00304943 0.00652026 0.00065266\n",
            " 0.22357109 0.112826   0.35479504 0.40915066]\n",
            "Min/max predictions: 8.003685798030347e-05 0.8555960059165955\n",
            "Loss for this batch: 0.2797\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00794796 0.03571964 0.04474242 0.02293063 0.14424488 0.2146377\n",
            " 0.00983439 0.31178862 0.10857607 0.13874134]\n",
            "Min/max predictions: 8.72331511345692e-05 0.9607178568840027\n",
            "Loss for this batch: 0.3221\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.08768358 0.00173704 0.00657673 0.07368943 0.49455807 0.7702445\n",
            " 0.00499634 0.3832132  0.20454827 0.21132039]\n",
            "Min/max predictions: 7.729890785412863e-05 0.915473461151123\n",
            "Loss for this batch: 0.3722\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23744605 0.27791205 0.3593115  0.4203974  0.03299499 0.00542592\n",
            " 0.19976449 0.05700909 0.0756415  0.00372286]\n",
            "Min/max predictions: 0.00014139166160020977 0.9718248844146729\n",
            "Loss for this batch: 0.3596\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.70192415 0.02438209 0.15184364 0.28121126 0.1261605  0.0162892\n",
            " 0.13427158 0.25879717 0.5188427  0.1998375 ]\n",
            "Min/max predictions: 6.109799141995609e-05 0.8840920925140381\n",
            "Loss for this batch: 0.3306\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2031878  0.06004162 0.23891586 0.26160544 0.00487674 0.00884712\n",
            " 0.07656587 0.02642652 0.1973266  0.0528881 ]\n",
            "Min/max predictions: 0.00011764481314457953 0.93635493516922\n",
            "Loss for this batch: 0.3071\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [4.0374142e-01 3.2388911e-01 3.2019552e-03 2.2135885e-01 3.6252127e-04\n",
            " 2.7498344e-01 1.5086162e-01 5.1698560e-01 1.3587505e-02 2.8356385e-01]\n",
            "Min/max predictions: 1.79618364199996e-05 0.973282516002655\n",
            "Loss for this batch: 0.2983\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.46769878 0.23601839 0.55730724 0.15449367 0.04579858 0.08164232\n",
            " 0.00545789 0.00226894 0.29187503 0.7413661 ]\n",
            "Min/max predictions: 7.08707666490227e-05 0.9228782653808594\n",
            "Loss for this batch: 0.2677\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.32712328 0.20002745 0.01893882 0.45697585 0.36244553 0.29684123\n",
            " 0.29297674 0.01763655 0.6231261  0.1529468 ]\n",
            "Min/max predictions: 2.4585715436842293e-05 0.8603467345237732\n",
            "Loss for this batch: 0.3341\n",
            "Sample labels: [1. 0. 0. 1. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.5463827  0.02850443 0.00573992 0.57395005 0.3461161  0.3998457\n",
            " 0.328842   0.00320126 0.69596213 0.05012548]\n",
            "Min/max predictions: 2.776655492198188e-05 0.931416928768158\n",
            "Loss for this batch: 0.3306\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02817483 0.4165327  0.3147269  0.6309469  0.7232336  0.03637538\n",
            " 0.3483618  0.09428681 0.24457687 0.09320087]\n",
            "Min/max predictions: 5.9842943301191553e-05 0.8330117464065552\n",
            "Loss for this batch: 0.3882\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.43424132 0.01058325 0.04422573 0.37713358 0.11984145 0.0028495\n",
            " 0.68913656 0.26577196 0.02307322 0.26208943]\n",
            "Min/max predictions: 5.381422670325264e-05 0.9232779741287231\n",
            "Loss for this batch: 0.3390\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.5386204e-01 1.1402751e-02 2.2050562e-04 1.7384002e-02 2.0095255e-02\n",
            " 2.5362524e-01 2.9932979e-02 4.4524400e-03 1.5448140e-02 9.2674226e-02]\n",
            "Min/max predictions: 5.0283644668525085e-05 0.9434643983840942\n",
            "Loss for this batch: 0.3637\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.12969425 0.02869584 0.40747124 0.09655453 0.00658924 0.64164186\n",
            " 0.46257427 0.00465207 0.00273956 0.2709154 ]\n",
            "Min/max predictions: 9.664342360338196e-05 0.8736125230789185\n",
            "Loss for this batch: 0.3138\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [3.3478123e-01 4.3378714e-01 1.4033030e-04 4.9732372e-01 8.4651923e-01\n",
            " 1.9992813e-02 1.2388926e-01 5.1863068e-01 1.4615273e-01 8.6572105e-03]\n",
            "Min/max predictions: 0.0001403303031111136 0.918531060218811\n",
            "Loss for this batch: 0.3636\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19413151 0.33723587 0.3659866  0.03567403 0.10170788 0.34466723\n",
            " 0.00538111 0.0091429  0.00479096 0.17439829]\n",
            "Min/max predictions: 0.00029208543128333986 0.9476529955863953\n",
            "Loss for this batch: 0.3825\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.01858484 0.23026466 0.08322761 0.04646314 0.46432444 0.0805527\n",
            " 0.43697563 0.0128944  0.5261781  0.78654885]\n",
            "Min/max predictions: 0.00013795225822832435 0.9378499388694763\n",
            "Loss for this batch: 0.3088\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.7034912e-04 5.5034630e-02 1.1879013e-04 9.0308646e-03 5.4954821e-01\n",
            " 1.7718682e-01 2.2420256e-01 1.1431905e-02 2.2759214e-02 2.5974128e-01]\n",
            "Min/max predictions: 0.00011879012890858576 0.8993726372718811\n",
            "Loss for this batch: 0.3128\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.01037466 0.00148432 0.04755083 0.00224664 0.01777475 0.01698741\n",
            " 0.28659517 0.24725534 0.00686393 0.03994818]\n",
            "Min/max predictions: 8.078623795881867e-05 0.9130034446716309\n",
            "Loss for this batch: 0.3172\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [9.3936801e-01 2.1234537e-02 2.0060096e-02 1.2597998e-03 4.3916774e-01\n",
            " 9.0062183e-01 1.3310316e-03 1.9494746e-03 1.9163439e-04 5.7315445e-01]\n",
            "Min/max predictions: 0.0001916343899210915 0.9453167915344238\n",
            "Loss for this batch: 0.3833\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.557573   0.31723723 0.5137488  0.5859603  0.2874707  0.5598522\n",
            " 0.0598574  0.4370733  0.00266371 0.02225031]\n",
            "Min/max predictions: 0.00014952968922443688 0.9340695738792419\n",
            "Loss for this batch: 0.3610\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3036132  0.12156461 0.5786516  0.16110136 0.15525112 0.29213414\n",
            " 0.00669969 0.15031041 0.3881358  0.00323644]\n",
            "Min/max predictions: 0.00014769587141927332 0.8539941906929016\n",
            "Loss for this batch: 0.3127\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [3.2923493e-01 7.6002479e-01 6.8873243e-04 4.6479437e-01 3.0284604e-01\n",
            " 2.9498111e-02 4.1696778e-01 3.8779381e-01 3.1776133e-03 9.6417591e-04]\n",
            "Min/max predictions: 0.000270133838057518 0.9642075896263123\n",
            "Loss for this batch: 0.3742\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [7.4346373e-03 2.1574321e-01 6.7030227e-01 5.1184348e-04 6.2268670e-03\n",
            " 4.3575205e-02 1.5158978e-02 6.3880157e-01 5.5338955e-01 1.5975749e-01]\n",
            "Min/max predictions: 2.9901557354605757e-05 0.9130515456199646\n",
            "Loss for this batch: 0.3192\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.06026751 0.28281617 0.1636746  0.37691087 0.40540084 0.4784262\n",
            " 0.02223087 0.0553607  0.01551951 0.7406271 ]\n",
            "Min/max predictions: 5.41400549991522e-05 0.9188637733459473\n",
            "Loss for this batch: 0.3612\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.36034244 0.10029509 0.13474375 0.00884574 0.16951652 0.35887584\n",
            " 0.92628014 0.31771737 0.15848505 0.45662647]\n",
            "Min/max predictions: 0.00011532133794389665 0.9801976680755615\n",
            "Loss for this batch: 0.2778\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0083215  0.5282898  0.18792175 0.04106182 0.7297806  0.29310352\n",
            " 0.02814958 0.40358925 0.05498439 0.0174666 ]\n",
            "Min/max predictions: 4.2508196202106774e-05 0.9229791760444641\n",
            "Loss for this batch: 0.4065\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01100056 0.02363947 0.2184784  0.6008665  0.45985895 0.01610151\n",
            " 0.10563701 0.12464485 0.33488718 0.0036399 ]\n",
            "Min/max predictions: 0.00021345197455957532 0.9663848280906677\n",
            "Loss for this batch: 0.2862\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [5.5075055e-01 7.0983588e-05 1.3546108e-01 6.0048050e-01 6.0494745e-01\n",
            " 3.7984353e-02 7.0179349e-01 1.6291259e-01 1.2558988e-01 5.4601863e-02]\n",
            "Min/max predictions: 1.558236726850737e-05 0.9465742111206055\n",
            "Loss for this batch: 0.3236\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00597847 0.01899038 0.0033699  0.3907903  0.00315383 0.21974555\n",
            " 0.0948969  0.22607732 0.40634063 0.04647294]\n",
            "Min/max predictions: 2.2887878003530204e-05 0.9752258658409119\n",
            "Loss for this batch: 0.3928\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.2949752  0.00917264 0.19327204 0.341713   0.06918658 0.02072449\n",
            " 0.37094894 0.43124965 0.5243999  0.11340515]\n",
            "Min/max predictions: 0.00021141475008334965 0.8324153423309326\n",
            "Loss for this batch: 0.3470\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [1.4213644e-01 1.9464715e-04 4.7051042e-01 1.5850335e-01 2.1633987e-01\n",
            " 8.6434828e-03 9.6789487e-02 5.8953457e-02 9.8173702e-03 2.3200670e-01]\n",
            "Min/max predictions: 1.834731847338844e-05 0.942389965057373\n",
            "Loss for this batch: 0.3282\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00143785 0.404781   0.03114843 0.27999175 0.7356777  0.00226968\n",
            " 0.14878893 0.06768392 0.6311562  0.00580139]\n",
            "Min/max predictions: 0.00024831484188325703 0.8062734007835388\n",
            "Loss for this batch: 0.3577\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07196814 0.04351656 0.00091708 0.52720124 0.22354883 0.13123848\n",
            " 0.30809456 0.40629104 0.02065586 0.23334593]\n",
            "Min/max predictions: 4.078008350916207e-05 0.8871254920959473\n",
            "Loss for this batch: 0.3838\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [3.9136127e-01 5.9890962e-01 1.7139422e-02 1.6890034e-02 3.0455714e-01\n",
            " 2.2039993e-01 5.1560980e-01 4.0671558e-04 2.2398730e-01 2.5269494e-03]\n",
            "Min/max predictions: 0.00017718080198392272 0.8773377537727356\n",
            "Loss for this batch: 0.3317\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00689974 0.45308638 0.0369271  0.02922295 0.5156469  0.00355317\n",
            " 0.03313652 0.2549763  0.6307392  0.02043619]\n",
            "Min/max predictions: 7.776491838740185e-05 0.9473820924758911\n",
            "Loss for this batch: 0.3458\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.684428   0.00435195 0.09163059 0.8355664  0.01233894 0.14736623\n",
            " 0.00632289 0.06089228 0.0183681  0.097555  ]\n",
            "Min/max predictions: 3.9368220313917845e-05 0.8846850991249084\n",
            "Loss for this batch: 0.3395\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04552463 0.05221635 0.02217346 0.11768281 0.0648383  0.44935212\n",
            " 0.35675454 0.00860921 0.02895726 0.10143998]\n",
            "Min/max predictions: 0.00017214180843438953 0.8474240899085999\n",
            "Loss for this batch: 0.3356\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
            "Sample predictions: [0.47414106 0.18975993 0.04751841 0.1458024  0.07800604 0.12619281\n",
            " 0.46447325 0.5465415  0.0010163  0.14444731]\n",
            "Min/max predictions: 8.685216016601771e-05 0.8858542442321777\n",
            "Loss for this batch: 0.3233\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02365464 0.0803841  0.00799462 0.24890913 0.5003869  0.00086414\n",
            " 0.26536235 0.0796871  0.177391   0.01473534]\n",
            "Min/max predictions: 6.22645384282805e-05 0.9471836686134338\n",
            "Loss for this batch: 0.3163\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.27468607 0.128145   0.03150339 0.13763264 0.39057532 0.70538026\n",
            " 0.00416408 0.45490724 0.4215675  0.06253663]\n",
            "Min/max predictions: 0.00024735572515055537 0.9659391641616821\n",
            "Loss for this batch: 0.3230\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.19850485 0.00493739 0.1108039  0.08361394 0.00947704 0.12317912\n",
            " 0.05872407 0.28608713 0.16403455 0.18476059]\n",
            "Min/max predictions: 0.00010000085603678599 0.8616635799407959\n",
            "Loss for this batch: 0.3486\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00408132 0.196555   0.4028684  0.21628536 0.02140555 0.22785337\n",
            " 0.02566616 0.01241911 0.04445542 0.3101701 ]\n",
            "Min/max predictions: 0.00018610505503602326 0.9448405504226685\n",
            "Loss for this batch: 0.3162\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.5772484  0.0332778  0.05003737 0.01141155 0.38143998 0.38483286\n",
            " 0.2484882  0.00519519 0.3337926  0.01190781]\n",
            "Min/max predictions: 0.00012235285248607397 0.8831420540809631\n",
            "Loss for this batch: 0.3186\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00766089 0.14981951 0.01787866 0.5474636  0.254997   0.06045402\n",
            " 0.4149764  0.12400176 0.01261933 0.2036379 ]\n",
            "Min/max predictions: 0.0001518643693998456 0.9795033931732178\n",
            "Loss for this batch: 0.3346\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00623096 0.00809866 0.01210317 0.01616328 0.44763842 0.09756743\n",
            " 0.10116066 0.1531552  0.00122032 0.0075832 ]\n",
            "Min/max predictions: 4.3537500459933653e-05 0.787849485874176\n",
            "Loss for this batch: 0.3566\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [5.2849445e-03 4.5401528e-01 3.2153264e-01 4.3676529e-02 1.9607311e-02\n",
            " 5.5878624e-02 3.5035674e-04 2.4056591e-02 6.9425986e-03 5.0559312e-02]\n",
            "Min/max predictions: 2.9879978683311492e-05 0.8675369024276733\n",
            "Loss for this batch: 0.3590\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.47816548 0.03016414 0.17926359 0.00315135 0.00441986 0.12485509\n",
            " 0.17168947 0.2830466  0.51709217 0.30554777]\n",
            "Min/max predictions: 5.043491910328157e-05 0.9746568202972412\n",
            "Loss for this batch: 0.3620\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [1.0560674e-01 1.4453096e-04 2.3894250e-01 3.0832030e-02 1.3891044e-01\n",
            " 3.4260388e-02 6.2342966e-01 5.8826134e-03 7.2039133e-03 1.1706905e-01]\n",
            "Min/max predictions: 2.550577664806042e-05 0.9379902482032776\n",
            "Loss for this batch: 0.3249\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.5724798  0.04477483 0.00196767 0.04792096 0.00429384 0.20156097\n",
            " 0.03006965 0.00448233 0.9556636  0.13665041]\n",
            "Min/max predictions: 3.1759303965372965e-05 0.9556636214256287\n",
            "Loss for this batch: 0.3721\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00821339 0.012225   0.2584767  0.39043686 0.3594239  0.14273362\n",
            " 0.0014617  0.3460182  0.03023308 0.20039271]\n",
            "Min/max predictions: 0.00018067803466692567 0.9250413775444031\n",
            "Loss for this batch: 0.3092\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.43289492 0.00736262 0.1217877  0.25584748 0.64072883 0.00677083\n",
            " 0.45701435 0.5821209  0.49611905 0.03898328]\n",
            "Min/max predictions: 7.685247692279518e-05 0.976936399936676\n",
            "Loss for this batch: 0.3140\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.17128445 0.3825578  0.07878817 0.0601915  0.51056695 0.04463302\n",
            " 0.26468295 0.56872636 0.03226295 0.01226411]\n",
            "Min/max predictions: 9.612780559109524e-05 0.8244962096214294\n",
            "Loss for this batch: 0.3256\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.58180654 0.08455499 0.15261304 0.01300381 0.215056   0.27202553\n",
            " 0.02491796 0.05422988 0.07241806 0.01246055]\n",
            "Min/max predictions: 1.2905009498354048e-05 0.908542275428772\n",
            "Loss for this batch: 0.2906\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.48778468 0.01531295 0.48360932 0.62767655 0.26136103 0.5559235\n",
            " 0.5896536  0.00106106 0.00374512 0.03485606]\n",
            "Min/max predictions: 0.00018812539929058403 0.7860886454582214\n",
            "Loss for this batch: 0.3623\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.6786240e-01 2.8941834e-01 2.6136618e-03 4.1403866e-01 9.6704972e-01\n",
            " 1.4169882e-02 6.9614779e-03 1.0797944e-01 3.2118645e-03 4.9234153e-05]\n",
            "Min/max predictions: 4.923415326629765e-05 0.9854915142059326\n",
            "Loss for this batch: 0.3190\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0159046  0.02845717 0.16295412 0.3882893  0.1490184  0.01821718\n",
            " 0.43270358 0.0880206  0.04031511 0.5121068 ]\n",
            "Min/max predictions: 7.89949408499524e-05 0.8521869778633118\n",
            "Loss for this batch: 0.3163\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.00509999 0.17614082 0.05783229 0.7967488  0.4195398  0.00922855\n",
            " 0.03570392 0.02165927 0.04612847 0.7352318 ]\n",
            "Min/max predictions: 0.00010548970749368891 0.9392350912094116\n",
            "Loss for this batch: 0.3543\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.11048561 0.01075499 0.09710474 0.05512743 0.11804859 0.01326179\n",
            " 0.04854627 0.03607047 0.07959239 0.15185513]\n",
            "Min/max predictions: 0.00016602121468167752 0.9298969507217407\n",
            "Loss for this batch: 0.2582\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.08850312 0.03622799 0.06222526 0.17404772 0.00370678 0.0155294\n",
            " 0.02027751 0.21008106 0.3993057  0.47755343]\n",
            "Min/max predictions: 0.0001141709290095605 0.9451406598091125\n",
            "Loss for this batch: 0.3478\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
            "Sample predictions: [0.33840698 0.7439676  0.5735998  0.19805323 0.01619441 0.09138205\n",
            " 0.495457   0.11459698 0.30542734 0.12009576]\n",
            "Min/max predictions: 0.00015588110545650125 0.8754794001579285\n",
            "Loss for this batch: 0.3941\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04054629 0.00999435 0.21722007 0.11346509 0.00084508 0.09497345\n",
            " 0.00276068 0.05247926 0.44785953 0.01137128]\n",
            "Min/max predictions: 0.00029836324392817914 0.8899604678153992\n",
            "Loss for this batch: 0.3581\n",
            "Sample labels: [0. 1. 1. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00456134 0.5821824  0.22868688 0.40305194 0.19435555 0.35871756\n",
            " 0.1064255  0.04991931 0.02054552 0.3934711 ]\n",
            "Min/max predictions: 0.00016223534476011992 0.9327852725982666\n",
            "Loss for this batch: 0.3335\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.41321173 0.02472796 0.10966592 0.02283501 0.37027356 0.20309491\n",
            " 0.00151679 0.31820264 0.7421974  0.34773895]\n",
            "Min/max predictions: 0.0001878970069810748 0.7995606064796448\n",
            "Loss for this batch: 0.3931\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.04349237 0.32859194 0.3474289  0.07042174 0.33785063 0.24527892\n",
            " 0.14255942 0.6969973  0.5809855  0.02535315]\n",
            "Min/max predictions: 0.00019000525935553014 0.8889958262443542\n",
            "Loss for this batch: 0.3759\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.16909803 0.1980582  0.00725073 0.20198816 0.03141139 0.03102808\n",
            " 0.05527731 0.12631941 0.00392158 0.0016938 ]\n",
            "Min/max predictions: 0.00014447419380303472 0.8715746402740479\n",
            "Loss for this batch: 0.3368\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.11412148 0.22058947 0.04803785 0.24586561 0.30624148 0.24496537\n",
            " 0.01696348 0.12630849 0.9389095  0.82535964]\n",
            "Min/max predictions: 0.00013278958795126528 0.9834761619567871\n",
            "Loss for this batch: 0.3835\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.4213905  0.22124153 0.00978152 0.29587215 0.00644519 0.39159805\n",
            " 0.12237735 0.14014515 0.03346291 0.60169023]\n",
            "Min/max predictions: 0.0001962004171218723 0.9314501285552979\n",
            "Loss for this batch: 0.3046\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.59912503e-03 3.26951891e-01 3.86165321e-01 2.30002776e-01\n",
            " 3.01017612e-01 2.41632983e-01 3.61263342e-02 2.33169631e-04\n",
            " 3.62257749e-01 1.24748096e-01]\n",
            "Min/max predictions: 4.491809522733092e-05 0.9077697992324829\n",
            "Loss for this batch: 0.3617\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.79801077 0.01250732 0.07298853 0.11328698 0.13607912 0.03277952\n",
            " 0.15056708 0.08457249 0.14471154 0.04827397]\n",
            "Min/max predictions: 4.69575788883958e-05 0.7980107665061951\n",
            "Loss for this batch: 0.3703\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.44349825 0.07637385 0.31334236 0.2623346  0.3134325  0.00655666\n",
            " 0.30847678 0.3029055  0.34784842 0.07090911]\n",
            "Min/max predictions: 0.0003342294949106872 0.9423272013664246\n",
            "Loss for this batch: 0.3470\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.41407076 0.05833335 0.21698296 0.079388   0.12136052 0.00107283\n",
            " 0.0102476  0.75314224 0.00227009 0.1612175 ]\n",
            "Min/max predictions: 0.0001560783275635913 0.8522925972938538\n",
            "Loss for this batch: 0.3543\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.29372257 0.3232164  0.16683763 0.03680073 0.37462434 0.4016872\n",
            " 0.21283208 0.20635004 0.02392347 0.22720826]\n",
            "Min/max predictions: 7.8588753240183e-05 0.9358339905738831\n",
            "Loss for this batch: 0.3250\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.1119563  0.08255752 0.13520041 0.43475896 0.01399437 0.26925182\n",
            " 0.32392636 0.00053372 0.00561081 0.507025  ]\n",
            "Min/max predictions: 4.0216080378741026e-05 0.9205577969551086\n",
            "Loss for this batch: 0.3414\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.27610743 0.21588449 0.13994555 0.0023121  0.8183039  0.00329858\n",
            " 0.50070435 0.13509108 0.03558606 0.01115812]\n",
            "Min/max predictions: 5.241886537987739e-05 0.9301913380622864\n",
            "Loss for this batch: 0.3849\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.4284398e-01 7.9979545e-01 1.9577451e-02 2.4374534e-05 5.2565050e-01\n",
            " 3.9308041e-02 2.7532661e-01 3.4375302e-03 5.5312496e-02 3.6462903e-02]\n",
            "Min/max predictions: 2.4374534405069426e-05 0.8320613503456116\n",
            "Loss for this batch: 0.4311\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.2890545  0.00153259 0.01528522 0.01548683 0.00381228 0.08729321\n",
            " 0.02123182 0.34191677 0.00124022 0.12373524]\n",
            "Min/max predictions: 0.0003277170762885362 0.9564720392227173\n",
            "Loss for this batch: 0.2835\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00869902 0.01235929 0.00104638 0.68730193 0.8118489  0.00490944\n",
            " 0.3976623  0.26140627 0.01913307 0.4314759 ]\n",
            "Min/max predictions: 0.00023321856861002743 0.8383576273918152\n",
            "Loss for this batch: 0.3739\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7196695  0.00309013 0.13630137 0.1127439  0.4008147  0.00613082\n",
            " 0.04093811 0.02612045 0.02152991 0.0148991 ]\n",
            "Min/max predictions: 7.917050243122503e-05 0.8927727341651917\n",
            "Loss for this batch: 0.3394\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.07368011 0.00299004 0.001292   0.73315734 0.8954268  0.01107948\n",
            " 0.1049598  0.00829716 0.36642626 0.62545794]\n",
            "Min/max predictions: 2.4439541448373348e-05 0.8954268097877502\n",
            "Loss for this batch: 0.2947\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14692399 0.00234638 0.02726222 0.04834365 0.3071033  0.01272044\n",
            " 0.08312828 0.01339379 0.03690948 0.3957872 ]\n",
            "Min/max predictions: 0.000220501417061314 0.8538611531257629\n",
            "Loss for this batch: 0.3787\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.09457933 0.43777117 0.18810242 0.06963415 0.09990447 0.13118881\n",
            " 0.00109618 0.10295428 0.04405326 0.01039837]\n",
            "Min/max predictions: 0.0005248381639830768 0.8962914943695068\n",
            "Loss for this batch: 0.3846\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03441174 0.33031297 0.00059049 0.06744943 0.09711849 0.09180831\n",
            " 0.00359224 0.4182959  0.14438136 0.4115445 ]\n",
            "Min/max predictions: 8.71305528562516e-05 0.9377278685569763\n",
            "Loss for this batch: 0.4048\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03818046 0.00136611 0.20524344 0.35232064 0.01246623 0.07682587\n",
            " 0.20105667 0.18420404 0.16595124 0.1375022 ]\n",
            "Min/max predictions: 8.52658849908039e-05 0.9116084575653076\n",
            "Loss for this batch: 0.3185\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0970787  0.4009369  0.501408   0.17559855 0.18216944 0.42743084\n",
            " 0.11030459 0.03227571 0.00794047 0.17363612]\n",
            "Min/max predictions: 0.00019455883011687547 0.9253740906715393\n",
            "Loss for this batch: 0.3535\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.2584558  0.06409722 0.30674595 0.19631578 0.17239724 0.8611539\n",
            " 0.00362594 0.58448565 0.00309829 0.2942117 ]\n",
            "Min/max predictions: 0.00011142302537336946 0.9377835392951965\n",
            "Loss for this batch: 0.3046\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [1.2617195e-02 2.8642201e-01 1.5001228e-02 1.8848017e-01 1.6040215e-01\n",
            " 3.3772722e-04 6.0751527e-03 7.6443367e-03 3.5358521e-01 3.8044325e-01]\n",
            "Min/max predictions: 0.0001656384119996801 0.9732786417007446\n",
            "Loss for this batch: 0.3960\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.40351912 0.08523431 0.18501624 0.59670895 0.00621728 0.27177164\n",
            " 0.6961123  0.44965336 0.00148432 0.35888332]\n",
            "Min/max predictions: 7.363837357843295e-05 0.9552881717681885\n",
            "Loss for this batch: 0.3060\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [3.6609480e-01 7.1443729e-02 1.0668575e-03 5.0073349e-01 1.8183579e-01\n",
            " 3.9368758e-01 4.1706959e-04 1.5084738e-03 3.2874516e-03 1.3663067e-01]\n",
            "Min/max predictions: 0.0001588266168255359 0.9245167374610901\n",
            "Loss for this batch: 0.3036\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07419518 0.10722766 0.08133198 0.06472505 0.2608755  0.00894065\n",
            " 0.4478363  0.01310279 0.11675772 0.6007071 ]\n",
            "Min/max predictions: 6.741130346199498e-05 0.9372064471244812\n",
            "Loss for this batch: 0.3750\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [0.00328051 0.0036696  0.02962417 0.00422001 0.04302886 0.00963005\n",
            " 0.65016836 0.43313578 0.20659567 0.30339918]\n",
            "Min/max predictions: 7.483545050490648e-05 0.9642356634140015\n",
            "Loss for this batch: 0.3418\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01755635 0.18804    0.06706931 0.37591085 0.00403848 0.10923428\n",
            " 0.5521882  0.5743367  0.14282998 0.00308115]\n",
            "Min/max predictions: 0.00017094188660848886 0.9335688352584839\n",
            "Loss for this batch: 0.2925\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07212271 0.00883572 0.0031636  0.41147688 0.06815016 0.06976769\n",
            " 0.12826383 0.21083371 0.26813564 0.11017253]\n",
            "Min/max predictions: 6.699837285850663e-06 0.907014012336731\n",
            "Loss for this batch: 0.3021\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0006438  0.02681168 0.03462768 0.16142976 0.08804172 0.05136941\n",
            " 0.0108522  0.00207647 0.02920507 0.05236011]\n",
            "Min/max predictions: 7.816445577191189e-05 0.8588353395462036\n",
            "Loss for this batch: 0.3110\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23867388 0.8222331  0.73571336 0.11771166 0.37546033 0.17793198\n",
            " 0.00143124 0.00113865 0.10353319 0.08629738]\n",
            "Min/max predictions: 5.476666410686448e-05 0.8848873972892761\n",
            "Loss for this batch: 0.3062\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.5350467  0.0111878  0.72817045 0.00680294 0.46075505 0.49136874\n",
            " 0.01484257 0.03699329 0.34392402 0.16154656]\n",
            "Min/max predictions: 0.00033885843004100025 0.9762104749679565\n",
            "Loss for this batch: 0.3117\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0934753  0.02227949 0.31484175 0.3531819  0.01420418 0.02666442\n",
            " 0.17247397 0.28210542 0.3570889  0.30946317]\n",
            "Min/max predictions: 7.9262514191214e-05 0.9716795682907104\n",
            "Loss for this batch: 0.2940\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.37027186 0.01490014 0.18756673 0.00975815 0.17826943 0.4871525\n",
            " 0.3040878  0.4315957  0.00989002 0.15243278]\n",
            "Min/max predictions: 0.0003487550129648298 0.9404359459877014\n",
            "Loss for this batch: 0.2882\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02506199 0.00186552 0.03748104 0.58478576 0.27076262 0.05927981\n",
            " 0.21572429 0.02869902 0.28645056 0.08024522]\n",
            "Min/max predictions: 1.834731847338844e-05 0.8883796334266663\n",
            "Loss for this batch: 0.3453\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.20689946 0.0044154  0.297911   0.11655434 0.6112235  0.07746398\n",
            " 0.42744893 0.28468952 0.05362876 0.04057281]\n",
            "Min/max predictions: 0.00022884964710101485 0.9398419260978699\n",
            "Loss for this batch: 0.3016\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [2.6925355e-01 6.0237265e-01 2.4966875e-02 3.0862851e-02 5.3278863e-04\n",
            " 6.4704764e-01 8.1075113e-03 2.6760623e-01 2.9492879e-01 3.8765531e-02]\n",
            "Min/max predictions: 4.983956750947982e-05 0.8920294046401978\n",
            "Loss for this batch: 0.3887\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.05656907 0.17373352 0.5793077  0.01050861 0.00410995 0.07742314\n",
            " 0.28691497 0.2375598  0.06413848 0.01259509]\n",
            "Min/max predictions: 0.00012198822514619678 0.9463222026824951\n",
            "Loss for this batch: 0.3421\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.66488030e-04 2.92363286e-01 3.24768543e-01 1.14157274e-01\n",
            " 1.43746957e-01 2.88179338e-01 6.92396611e-02 1.33950114e-01\n",
            " 1.02972947e-01 5.65729022e-01]\n",
            "Min/max predictions: 8.983417501440272e-05 0.9320409297943115\n",
            "Loss for this batch: 0.2998\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.20488511 0.5788506  0.5756644  0.10700872 0.07685381 0.2954411\n",
            " 0.01130567 0.06261902 0.14742376 0.04757399]\n",
            "Min/max predictions: 8.885686111170799e-05 0.9487173557281494\n",
            "Loss for this batch: 0.3281\n",
            "Sample labels: [1. 1. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.8172609  0.61232555 0.00674627 0.28678384 0.5052958  0.12618758\n",
            " 0.11437744 0.05406949 0.42575964 0.04651744]\n",
            "Min/max predictions: 0.0002394153125351295 0.9298228621482849\n",
            "Loss for this batch: 0.4055\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02169134 0.07840218 0.18208402 0.08161554 0.02986111 0.00106976\n",
            " 0.04275836 0.06574114 0.18744694 0.00591588]\n",
            "Min/max predictions: 3.802104038186371e-05 0.9914013743400574\n",
            "Loss for this batch: 0.3610\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.32064196 0.01664605 0.11770469 0.11599328 0.24581437 0.06427824\n",
            " 0.06741288 0.2208168  0.00508701 0.11983387]\n",
            "Min/max predictions: 9.700906230136752e-05 0.78011155128479\n",
            "Loss for this batch: 0.3093\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.40510872 0.20589814 0.00527511 0.6834941  0.38749564 0.02909102\n",
            " 0.0399751  0.65207803 0.18121445 0.57684684]\n",
            "Min/max predictions: 8.008762233657762e-05 0.9412583112716675\n",
            "Loss for this batch: 0.3098\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.5486744  0.02490956 0.31327525 0.09881055 0.00755604 0.31451604\n",
            " 0.22325048 0.18932664 0.238005   0.6407417 ]\n",
            "Min/max predictions: 0.00014081150584388524 0.9008121490478516\n",
            "Loss for this batch: 0.3023\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.11179645 0.00691819 0.0118327  0.00187625 0.01971301 0.53279\n",
            " 0.08701017 0.24104913 0.5062769  0.02648659]\n",
            "Min/max predictions: 5.8188576076645404e-05 0.7976850867271423\n",
            "Loss for this batch: 0.3929\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [2.0213196e-02 4.1256350e-02 1.3849568e-02 3.9117532e-03 4.1494284e-05\n",
            " 2.4121748e-01 2.6433632e-01 2.4367478e-01 1.3865107e-01 6.3174948e-02]\n",
            "Min/max predictions: 4.14942842326127e-05 0.9491602182388306\n",
            "Loss for this batch: 0.3369\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03159167 0.25593412 0.22047652 0.12308834 0.30933937 0.04202433\n",
            " 0.01025601 0.6210372  0.04546157 0.09856094]\n",
            "Min/max predictions: 0.0001054871900123544 0.8315760493278503\n",
            "Loss for this batch: 0.3711\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.39393452 0.56511116 0.11660982 0.00539467 0.34410396 0.04135118\n",
            " 0.28689125 0.5325806  0.00821781 0.30961102]\n",
            "Min/max predictions: 0.00018503122555557638 0.8722178339958191\n",
            "Loss for this batch: 0.3555\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00854547 0.00738224 0.1908266  0.21665335 0.15946592 0.24760398\n",
            " 0.00921659 0.06065548 0.00329084 0.01105479]\n",
            "Min/max predictions: 1.9496988898026757e-05 0.8834168314933777\n",
            "Loss for this batch: 0.2754\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02697683 0.00367865 0.0015342  0.03829322 0.01629365 0.03017258\n",
            " 0.22154926 0.3389672  0.05617152 0.00550234]\n",
            "Min/max predictions: 0.0001438984036212787 0.9428976774215698\n",
            "Loss for this batch: 0.2712\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.10359485 0.5288077  0.08937923 0.00336366 0.22713067 0.1345924\n",
            " 0.06690184 0.25626892 0.5568221  0.3720078 ]\n",
            "Min/max predictions: 0.00014239926531445235 0.8333874344825745\n",
            "Loss for this batch: 0.3733\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [2.2212495e-04 2.6052378e-02 7.0891716e-02 8.3025619e-02 1.3620271e-02\n",
            " 4.3785781e-01 1.1979839e-02 1.3554487e-01 2.0021248e-01 5.4641402e-01]\n",
            "Min/max predictions: 0.00022212494513951242 0.9319122433662415\n",
            "Loss for this batch: 0.3158\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01197437 0.19113562 0.5672072  0.00710036 0.12227031 0.03676343\n",
            " 0.35769928 0.6329809  0.0750007  0.480057  ]\n",
            "Min/max predictions: 0.00026162847643718123 0.9044286012649536\n",
            "Loss for this batch: 0.3245\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0227135  0.02508247 0.18500589 0.15589644 0.02094848 0.65239555\n",
            " 0.00425876 0.15468395 0.43598482 0.00771757]\n",
            "Min/max predictions: 0.000334600277710706 0.9220298528671265\n",
            "Loss for this batch: 0.3222\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.24924946 0.29121    0.04680457 0.5392221  0.02563546 0.25578994\n",
            " 0.00675859 0.00837716 0.19783697 0.6295126 ]\n",
            "Min/max predictions: 0.0001264316524611786 0.8997529149055481\n",
            "Loss for this batch: 0.3017\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00409779 0.46557462 0.15149258 0.4165657  0.11943916 0.6626166\n",
            " 0.06193801 0.07094351 0.6158941  0.37437502]\n",
            "Min/max predictions: 1.2890864127257373e-05 0.9139828681945801\n",
            "Loss for this batch: 0.3040\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.60385644 0.25753772 0.08815981 0.3298305  0.4449939  0.37973687\n",
            " 0.4700125  0.15825328 0.11420063 0.00553161]\n",
            "Min/max predictions: 0.00021183646458666772 0.8932089805603027\n",
            "Loss for this batch: 0.3455\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.00328396 0.07096474 0.08480845 0.02430908 0.45672512 0.01518513\n",
            " 0.02275817 0.69521236 0.6847215  0.01659567]\n",
            "Min/max predictions: 0.00015036927652545273 0.8878743052482605\n",
            "Loss for this batch: 0.3368\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.6467758  0.04538613 0.14989518 0.37572548 0.00557789 0.40104583\n",
            " 0.0052926  0.7430477  0.47221977 0.3654482 ]\n",
            "Min/max predictions: 6.288907752605155e-05 0.859614908695221\n",
            "Loss for this batch: 0.2994\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.26724878 0.6522827  0.50398785 0.00359363 0.25578898 0.13008752\n",
            " 0.1725082  0.10194688 0.0110691  0.02717933]\n",
            "Min/max predictions: 5.3396586736198515e-05 0.9058058261871338\n",
            "Loss for this batch: 0.3444\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.05264515 0.09637785 0.83980465 0.46356943 0.57762754 0.02519509\n",
            " 0.00476122 0.5789903  0.05538311 0.00288828]\n",
            "Min/max predictions: 4.369705857243389e-05 0.8696675300598145\n",
            "Loss for this batch: 0.3694\n",
            "Sample labels: [1. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [8.0534136e-01 2.4615246e-01 4.8669800e-01 4.9098767e-04 6.1657083e-01\n",
            " 7.5669384e-01 1.4264005e-01 8.3681345e-01 1.4448939e-02 1.2978196e-01]\n",
            "Min/max predictions: 0.00019908705144189298 0.957422137260437\n",
            "Loss for this batch: 0.3744\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.4783953e-01 1.2656084e-01 1.7729771e-01 2.9611921e-01 2.9344609e-01\n",
            " 1.0901273e-02 3.2273636e-04 4.6497159e-02 4.7272827e-02 1.3422489e-02]\n",
            "Min/max predictions: 4.4301425077719614e-05 0.9883127808570862\n",
            "Loss for this batch: 0.3164\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.3397394e-04 1.3596484e-01 1.1974208e-02 1.2756388e-01 1.4583436e-03\n",
            " 3.4270534e-01 4.8168339e-02 5.7497688e-02 2.3599993e-02 7.3672719e-02]\n",
            "Min/max predictions: 2.657292498042807e-05 0.9882434010505676\n",
            "Loss for this batch: 0.2792\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.5344629  0.00927139 0.15139085 0.42539564 0.21743016 0.3304325\n",
            " 0.28630695 0.05120375 0.00730212 0.006243  ]\n",
            "Min/max predictions: 8.769085980020463e-05 0.9748456478118896\n",
            "Loss for this batch: 0.3537\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.05098398 0.0048062  0.30984867 0.0094175  0.0126297  0.88404727\n",
            " 0.21581376 0.12694687 0.08338025 0.19848637]\n",
            "Min/max predictions: 0.0001656709355302155 0.8921049237251282\n",
            "Loss for this batch: 0.3525\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.00863743 0.08537812 0.5483265  0.13386756 0.21211891 0.17171027\n",
            " 0.39875033 0.79355395 0.03681478 0.11267962]\n",
            "Min/max predictions: 6.301959365373477e-05 0.824187695980072\n",
            "Loss for this batch: 0.2569\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.08274085 0.0116235  0.0480571  0.00540566 0.15415797 0.01422068\n",
            " 0.15392135 0.30550826 0.34872523 0.25247115]\n",
            "Min/max predictions: 7.557959179393947e-05 0.9545621275901794\n",
            "Loss for this batch: 0.3859\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0084759  0.00069086 0.294821   0.54356647 0.01638041 0.07437985\n",
            " 0.02996216 0.09044665 0.00335921 0.4100723 ]\n",
            "Min/max predictions: 1.1789129530370701e-05 0.9622902274131775\n",
            "Loss for this batch: 0.3220\n",
            "Sample labels: [0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2134821  0.8002863  0.67364925 0.00406476 0.44957936 0.00128066\n",
            " 0.13958582 0.05901973 0.09661382 0.00453896]\n",
            "Min/max predictions: 3.957766239182092e-05 0.8296118974685669\n",
            "Loss for this batch: 0.3687\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.1251324  0.01003275 0.1527647  0.11568681 0.0009072  0.08601771\n",
            " 0.38277608 0.00158245 0.01758987 0.10374065]\n",
            "Min/max predictions: 0.0001646320743020624 0.8610786199569702\n",
            "Loss for this batch: 0.3525\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01447036 0.00693355 0.01991918 0.13279808 0.00417905 0.32773268\n",
            " 0.04435934 0.5443034  0.6968968  0.14946698]\n",
            "Min/max predictions: 7.210711191873997e-05 0.9700694680213928\n",
            "Loss for this batch: 0.3378\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.75219417 0.39811578 0.40777537 0.02181931 0.01605286 0.0076777\n",
            " 0.29518083 0.0024852  0.0021244  0.72865736]\n",
            "Min/max predictions: 0.00019007139781024307 0.9648513793945312\n",
            "Loss for this batch: 0.3037\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.24284546 0.23455578 0.3264874  0.31322348 0.1346489  0.00420017\n",
            " 0.12074401 0.03442013 0.43561026 0.59185433]\n",
            "Min/max predictions: 0.0002604989567771554 0.9840289950370789\n",
            "Loss for this batch: 0.3203\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [4.3565777e-01 4.8206323e-01 1.3284105e-01 5.1620364e-04 6.0114842e-02\n",
            " 4.2021048e-01 1.0337251e-01 2.4689677e-01 5.8070606e-01 4.8772588e-01]\n",
            "Min/max predictions: 1.1488964446471073e-05 0.9272717833518982\n",
            "Loss for this batch: 0.3533\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.23918985 0.32636568 0.5500695  0.05798315 0.05791725 0.57863194\n",
            " 0.17986575 0.81077427 0.10871185 0.6485242 ]\n",
            "Min/max predictions: 1.9573446479626e-05 0.9441023468971252\n",
            "Loss for this batch: 0.3120\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [1.5364109e-01 8.3141375e-01 6.7679589e-03 8.5816473e-02 3.9980857e-04\n",
            " 2.9645637e-02 3.8187379e-01 1.8289578e-01 4.7156364e-01 2.3140411e-01]\n",
            "Min/max predictions: 6.66187479509972e-05 0.9890571236610413\n",
            "Loss for this batch: 0.3578\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.09686261 0.15533578 0.01900632 0.32706383 0.05571744 0.31335747\n",
            " 0.00155093 0.001438   0.00395347 0.00501311]\n",
            "Min/max predictions: 7.030587585177273e-05 0.9587017893791199\n",
            "Loss for this batch: 0.3314\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.7192824  0.10225549 0.24927698 0.01356427 0.00267231 0.00994743\n",
            " 0.726243   0.00121788 0.0025858  0.04174471]\n",
            "Min/max predictions: 0.0002964024315588176 0.9430699348449707\n",
            "Loss for this batch: 0.3635\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.06363681 0.08982136 0.0039258  0.00504371 0.00448145 0.0222007\n",
            " 0.44957536 0.00955946 0.7752981  0.00203119]\n",
            "Min/max predictions: 0.0001985030685318634 0.9432045817375183\n",
            "Loss for this batch: 0.3320\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.14745192 0.5435076  0.12137364 0.42732087 0.00754542 0.5838909\n",
            " 0.3176317  0.00283435 0.12070748 0.15964797]\n",
            "Min/max predictions: 1.3581945495388936e-05 0.911707878112793\n",
            "Loss for this batch: 0.2952\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00128478 0.0014433  0.7737866  0.0015992  0.00176629 0.27238014\n",
            " 0.05815367 0.02773795 0.17169297 0.08231729]\n",
            "Min/max predictions: 0.00012146658264100552 0.8968421816825867\n",
            "Loss for this batch: 0.4213\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [3.5596961e-01 2.4651904e-02 2.5641652e-02 3.6524519e-01 8.0624260e-03\n",
            " 3.0523565e-01 4.7911215e-01 2.3847395e-04 1.7817053e-01 1.2409514e-01]\n",
            "Min/max predictions: 0.00015351950423792005 0.9338012337684631\n",
            "Loss for this batch: 0.3321\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.20576908 0.24501173 0.01304082 0.00667493 0.11782044 0.04198005\n",
            " 0.09797604 0.2741285  0.00039136 0.0023906 ]\n",
            "Min/max predictions: 0.00015451166837010533 0.8460850715637207\n",
            "Loss for this batch: 0.3143\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.31265214 0.05911642 0.00752326 0.00813362 0.13371451 0.05917336\n",
            " 0.00221521 0.57165116 0.03783909 0.5208114 ]\n",
            "Min/max predictions: 6.286947609623894e-05 0.9720887541770935\n",
            "Loss for this batch: 0.3450\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.20420513 0.1703124  0.4774047  0.428191   0.20185563 0.17283456\n",
            " 0.05435292 0.42016333 0.13043898 0.11744888]\n",
            "Min/max predictions: 4.278511187294498e-05 0.850037157535553\n",
            "Loss for this batch: 0.3565\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03427936 0.01442803 0.3677278  0.0361024  0.08278918 0.3319387\n",
            " 0.02296918 0.0023718  0.00981345 0.07032067]\n",
            "Min/max predictions: 0.00018248001288156956 0.9367436766624451\n",
            "Loss for this batch: 0.3637\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.4921842  0.02274045 0.17213263 0.17201541 0.5534326  0.37020335\n",
            " 0.00905357 0.6650565  0.01825971 0.34319398]\n",
            "Min/max predictions: 0.00018246540275868028 0.8599640727043152\n",
            "Loss for this batch: 0.3514\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
            "Sample predictions: [2.1265969e-02 2.7550766e-01 1.9528100e-01 2.7348158e-01 1.2855718e-01\n",
            " 8.5959589e-01 7.4206677e-04 5.1523161e-01 3.3992562e-01 5.8284199e-01]\n",
            "Min/max predictions: 3.920438393834047e-05 0.9143447279930115\n",
            "Loss for this batch: 0.3139\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10560801 0.01265746 0.13346158 0.03270613 0.7861509  0.28402323\n",
            " 0.00935874 0.00552813 0.01216109 0.00319977]\n",
            "Min/max predictions: 0.00010640115942806005 0.8619926571846008\n",
            "Loss for this batch: 0.3388\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10585858 0.09258168 0.39946666 0.35083324 0.14386587 0.7015956\n",
            " 0.00088985 0.04908017 0.556754   0.0288718 ]\n",
            "Min/max predictions: 6.542126357089728e-05 0.9079290628433228\n",
            "Loss for this batch: 0.3235\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.54309654 0.01270369 0.03167001 0.28683037 0.07661582 0.0168005\n",
            " 0.38250896 0.00680752 0.152808   0.00170153]\n",
            "Min/max predictions: 0.00013228142051957548 0.8360040783882141\n",
            "Loss for this batch: 0.3048\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03179219 0.21706003 0.40791854 0.2985277  0.00703734 0.01666998\n",
            " 0.04595493 0.00062532 0.59123486 0.20668904]\n",
            "Min/max predictions: 9.89640029729344e-05 0.9429824948310852\n",
            "Loss for this batch: 0.3215\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00172705 0.0020052  0.02454092 0.03211648 0.2959543  0.33625707\n",
            " 0.10302924 0.18646404 0.01822096 0.02117411]\n",
            "Min/max predictions: 0.00034773326478898525 0.8080386519432068\n",
            "Loss for this batch: 0.3176\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.47584215 0.10865799 0.3810084  0.33340976 0.86391735 0.10643782\n",
            " 0.07306934 0.01100311 0.26957995 0.07345892]\n",
            "Min/max predictions: 5.811182563775219e-05 0.863917350769043\n",
            "Loss for this batch: 0.3200\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.04163378 0.19084957 0.06406827 0.33864844 0.07790358 0.0301961\n",
            " 0.29861107 0.34335294 0.20159937 0.00247207]\n",
            "Min/max predictions: 3.657464185380377e-05 0.7978541254997253\n",
            "Loss for this batch: 0.3435\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00356629 0.6928737  0.21412835 0.3800013  0.07473253 0.26924112\n",
            " 0.4754954  0.3935596  0.25476968 0.1418422 ]\n",
            "Min/max predictions: 0.0006321633700281382 0.849427342414856\n",
            "Loss for this batch: 0.3636\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.24655782 0.15024894 0.14720643 0.00193786 0.07182245 0.08047061\n",
            " 0.21071225 0.25897124 0.06824244 0.25537047]\n",
            "Min/max predictions: 8.003188850125298e-05 0.8343623280525208\n",
            "Loss for this batch: 0.3396\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.5712569e-04 6.9951229e-02 2.0013862e-04 5.3440160e-01 2.7496365e-01\n",
            " 1.1242352e-03 9.0616420e-03 1.2229376e-02 4.0065061e-02 1.0077411e-02]\n",
            "Min/max predictions: 0.00010949657007586211 0.9468711614608765\n",
            "Loss for this batch: 0.3071\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01103925 0.01553963 0.00446228 0.1715344  0.19983844 0.11358701\n",
            " 0.00184468 0.08536697 0.2955698  0.3088936 ]\n",
            "Min/max predictions: 0.00014642778842244297 0.9300917387008667\n",
            "Loss for this batch: 0.3371\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.6581389  0.2635919  0.4886908  0.24098665 0.00753339 0.00312701\n",
            " 0.4350842  0.13549247 0.00117632 0.47175524]\n",
            "Min/max predictions: 0.00015942807658575475 0.9797619581222534\n",
            "Loss for this batch: 0.4078\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [4.2867097e-03 1.7496331e-02 8.7515078e-03 4.8497158e-01 7.2362120e-03\n",
            " 2.6932143e-04 1.2379853e-01 8.5711616e-01 5.5204177e-01 3.9910874e-01]\n",
            "Min/max predictions: 2.5492134227533825e-05 0.8965842127799988\n",
            "Loss for this batch: 0.3567\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.08206458 0.74351716 0.00877397 0.10265525 0.453789   0.00675013\n",
            " 0.44861022 0.47448573 0.0571171  0.00158969]\n",
            "Min/max predictions: 0.0001408166135661304 0.9510770440101624\n",
            "Loss for this batch: 0.2603\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00220634 0.1498148  0.16748363 0.06513533 0.30784753 0.41439557\n",
            " 0.00523369 0.55058366 0.02395139 0.13741049]\n",
            "Min/max predictions: 0.000443280121544376 0.8598068952560425\n",
            "Loss for this batch: 0.2957\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [1.6604591e-04 3.5272157e-01 2.8238809e-02 1.9496922e-01 1.7810503e-02\n",
            " 2.3962785e-01 1.9275216e-02 9.5212400e-02 2.3195349e-01 3.7812151e-02]\n",
            "Min/max predictions: 0.00014293866115622222 0.7803427577018738\n",
            "Loss for this batch: 0.3565\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02676345 0.00217196 0.27863503 0.4117879  0.01377747 0.2694368\n",
            " 0.00045669 0.00295958 0.43841675 0.00174147]\n",
            "Min/max predictions: 4.170493775745854e-05 0.9292412400245667\n",
            "Loss for this batch: 0.3358\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.02084327 0.08968683 0.00690255 0.04413433 0.2204842  0.34748253\n",
            " 0.07724829 0.24139485 0.00055701 0.19110538]\n",
            "Min/max predictions: 1.364608124276856e-05 0.8890779614448547\n",
            "Loss for this batch: 0.3612\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [3.0058168e-04 1.5845845e-02 1.9201679e-02 3.0530876e-01 3.0616358e-01\n",
            " 1.4849252e-01 3.8622600e-01 2.1065831e-02 1.9060165e-02 3.2235989e-01]\n",
            "Min/max predictions: 0.00015086315397638828 0.9011151790618896\n",
            "Loss for this batch: 0.3058\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.03308145 0.5391041  0.07815046 0.38148513 0.02318834 0.12655257\n",
            " 0.72285825 0.24218147 0.5758665  0.07159191]\n",
            "Min/max predictions: 4.0659626392880455e-05 0.9727004766464233\n",
            "Loss for this batch: 0.2762\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.43258595 0.35492897 0.02669927 0.0724024  0.10957354 0.09580995\n",
            " 0.7397025  0.01026431 0.41278583 0.04071547]\n",
            "Min/max predictions: 0.00012537074508145452 0.9491260647773743\n",
            "Loss for this batch: 0.3320\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.19642216 0.00567436 0.27044502 0.12991372 0.00840336 0.9610313\n",
            " 0.4297822  0.37644762 0.14067288 0.10511228]\n",
            "Min/max predictions: 0.00023040585801936686 0.9610313177108765\n",
            "Loss for this batch: 0.3546\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.02581028 0.3395228  0.38076755 0.01432536 0.10323174 0.19234975\n",
            " 0.00487407 0.05353938 0.4182157  0.552459  ]\n",
            "Min/max predictions: 0.00015993372653611004 0.8841546177864075\n",
            "Loss for this batch: 0.3123\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [6.8523746e-05 1.1594740e-01 1.4394602e-03 1.2103384e-02 9.6683390e-02\n",
            " 2.3659036e-01 1.6239540e-01 1.4661030e-03 7.0238978e-01 5.9784465e-02]\n",
            "Min/max predictions: 6.852374644950032e-05 0.819496214389801\n",
            "Loss for this batch: 0.3663\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.38250038 0.58260113 0.24558742 0.08381912 0.01929862 0.01590165\n",
            " 0.01078141 0.357315   0.07967965 0.3447684 ]\n",
            "Min/max predictions: 4.918872218695469e-05 0.8659775853157043\n",
            "Loss for this batch: 0.3746\n",
            "Sample labels: [1. 0. 1. 0. 0. 1. 1. 1. 1. 0.]\n",
            "Sample predictions: [7.1308458e-01 1.8390499e-01 6.7903143e-01 2.4590205e-04 5.3308317e-03\n",
            " 6.5795189e-01 3.8865858e-01 5.2937108e-01 5.2595848e-01 2.0037356e-03]\n",
            "Min/max predictions: 1.3442723684420343e-05 0.9575453996658325\n",
            "Loss for this batch: 0.3618\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [3.0992737e-01 8.3855063e-01 3.6777530e-02 8.4904926e-03 2.2374433e-01\n",
            " 1.6595582e-02 7.9583925e-01 5.8835071e-01 6.1212247e-04 6.0019481e-01]\n",
            "Min/max predictions: 3.974749051849358e-05 0.9147239923477173\n",
            "Loss for this batch: 0.3276\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03854096 0.00339069 0.30949554 0.17552991 0.00263911 0.3969614\n",
            " 0.01319934 0.07476444 0.4981907  0.24929872]\n",
            "Min/max predictions: 5.949533442617394e-05 0.8816612362861633\n",
            "Loss for this batch: 0.3014\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01460625 0.34002435 0.18075542 0.04105001 0.18810834 0.11655673\n",
            " 0.20510677 0.05544862 0.00141328 0.00777948]\n",
            "Min/max predictions: 0.0001880479248939082 0.9641559720039368\n",
            "Loss for this batch: 0.4157\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00498038 0.04194753 0.10142896 0.10239914 0.00938882 0.5773003\n",
            " 0.4400269  0.2420273  0.02489249 0.05700314]\n",
            "Min/max predictions: 0.0001939259673235938 0.9927858710289001\n",
            "Loss for this batch: 0.3729\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.5962286e-01 3.4694800e-01 1.8328401e-01 8.4878892e-02 3.4352595e-01\n",
            " 3.5965569e-02 4.4305605e-04 1.0876092e-03 1.8827054e-01 1.8794699e-02]\n",
            "Min/max predictions: 9.89791878964752e-05 0.9470080733299255\n",
            "Loss for this batch: 0.3460\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.09652562 0.24114808 0.05873488 0.09418165 0.30796874 0.22371793\n",
            " 0.0018025  0.03736434 0.45012698 0.1813208 ]\n",
            "Min/max predictions: 3.504015694488771e-05 0.9751654267311096\n",
            "Loss for this batch: 0.3025\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.1247287  0.00517202 0.67985463 0.11728498 0.6030024  0.17406602\n",
            " 0.2194485  0.08615125 0.01381142 0.01784972]\n",
            "Min/max predictions: 5.881271863472648e-05 0.9149400591850281\n",
            "Loss for this batch: 0.3599\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.18082213 0.1044256  0.0008764  0.66556793 0.04433781 0.18841387\n",
            " 0.2547752  0.31592128 0.01405917 0.15779164]\n",
            "Min/max predictions: 0.00022385816555470228 0.940759003162384\n",
            "Loss for this batch: 0.3186\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01831581 0.09114184 0.15315494 0.00229316 0.37523088 0.12157623\n",
            " 0.20758006 0.05752462 0.37381813 0.397039  ]\n",
            "Min/max predictions: 0.00011772359721362591 0.9529328942298889\n",
            "Loss for this batch: 0.3367\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01459088 0.02250007 0.24478193 0.38811845 0.00603659 0.06370756\n",
            " 0.14551888 0.6404273  0.2745647  0.25767124]\n",
            "Min/max predictions: 0.0001394253340549767 0.966187059879303\n",
            "Loss for this batch: 0.3383\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.0010267  0.3854702  0.1154349  0.01784988 0.84987843 0.00407926\n",
            " 0.07034695 0.02868756 0.47769597 0.78363734]\n",
            "Min/max predictions: 0.00017796744941733778 0.8884772062301636\n",
            "Loss for this batch: 0.3558\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15648866 0.00775379 0.01316543 0.9132595  0.31905672 0.11438128\n",
            " 0.12754449 0.02305074 0.24542728 0.0091596 ]\n",
            "Min/max predictions: 0.00021907410700805485 0.9308933019638062\n",
            "Loss for this batch: 0.3075\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [5.3648925e-01 2.4467399e-03 3.0482027e-01 1.2853755e-01 1.9369350e-04\n",
            " 1.6460391e-02 2.1700114e-01 2.5383592e-01 4.5588400e-02 5.5028993e-01]\n",
            "Min/max predictions: 3.904424738720991e-05 0.9678577780723572\n",
            "Loss for this batch: 0.3361\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02742063 0.39819565 0.05405078 0.358839   0.00514249 0.6888387\n",
            " 0.40596125 0.28825715 0.06357616 0.18944886]\n",
            "Min/max predictions: 0.00017823731468524784 0.9424607157707214\n",
            "Loss for this batch: 0.3598\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0088597  0.6814302  0.00404032 0.00811431 0.00895532 0.06290001\n",
            " 0.01349871 0.03297785 0.3759169  0.1168347 ]\n",
            "Min/max predictions: 6.66849737172015e-05 0.8409330248832703\n",
            "Loss for this batch: 0.3471\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07770684 0.75772184 0.5230674  0.00113506 0.17250024 0.22781847\n",
            " 0.005151   0.02138034 0.37207884 0.37082627]\n",
            "Min/max predictions: 0.00034256570506840944 0.9298173189163208\n",
            "Loss for this batch: 0.3352\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00826764 0.6989738  0.5094638  0.17787325 0.2043256  0.4380337\n",
            " 0.16512614 0.5013137  0.00171969 0.45927417]\n",
            "Min/max predictions: 7.381244358839467e-05 0.9525424838066101\n",
            "Loss for this batch: 0.3904\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.15512519 0.22910646 0.89328516 0.00125055 0.9012781  0.5918396\n",
            " 0.55158305 0.06402235 0.00286776 0.00527166]\n",
            "Min/max predictions: 0.00015057530254125595 0.9056231379508972\n",
            "Loss for this batch: 0.3681\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.02491694 0.682076   0.60496336 0.01213635 0.01134332 0.0095064\n",
            " 0.32261103 0.6066871  0.1796668  0.00330583]\n",
            "Min/max predictions: 5.4667991207679734e-05 0.9179716110229492\n",
            "Loss for this batch: 0.3193\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07778399 0.1248882  0.66364413 0.00240256 0.2867151  0.31054217\n",
            " 0.00469    0.00880586 0.10554406 0.00079593]\n",
            "Min/max predictions: 0.00019980169599875808 0.9205338954925537\n",
            "Loss for this batch: 0.3249\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.09521094 0.27306646 0.02295957 0.53060937 0.08634492 0.09477559\n",
            " 0.21116553 0.19783704 0.03260476 0.02748522]\n",
            "Min/max predictions: 4.7891222493490204e-05 0.9489949941635132\n",
            "Loss for this batch: 0.3162\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [2.2933718e-04 3.5376552e-01 2.8150466e-03 4.1008690e-01 2.0582989e-02\n",
            " 6.8743044e-04 8.6371101e-02 1.7341003e-01 6.2655771e-01 4.0258211e-03]\n",
            "Min/max predictions: 0.0001569094747537747 0.9062970876693726\n",
            "Loss for this batch: 0.3130\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06190726 0.27867305 0.4570986  0.44389266 0.36165667 0.01635784\n",
            " 0.00118922 0.03297461 0.12029225 0.3991714 ]\n",
            "Min/max predictions: 0.000994223984889686 0.743194043636322\n",
            "Loss for this batch: 0.3640\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02448985 0.00403328 0.48133194 0.01250405 0.01345099 0.00907234\n",
            " 0.55442184 0.10930461 0.074081   0.00740673]\n",
            "Min/max predictions: 0.00021440329146571457 0.9631021022796631\n",
            "Loss for this batch: 0.3098\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            "Sample predictions: [0.01995643 0.0206152  0.05051537 0.7114346  0.06704336 0.39510164\n",
            " 0.50924855 0.46641752 0.00155017 0.5825644 ]\n",
            "Min/max predictions: 8.048504969337955e-05 0.8443534970283508\n",
            "Loss for this batch: 0.3999\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.0080043  0.304278   0.01370326 0.75333625 0.01996    0.00700726\n",
            " 0.00139169 0.06316388 0.6023442  0.07837287]\n",
            "Min/max predictions: 0.00021018207189626992 0.7972272634506226\n",
            "Loss for this batch: 0.3421\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "Sample predictions: [0.04561462 0.2951045  0.5177624  0.3947422  0.05423723 0.12728731\n",
            " 0.77506775 0.60968107 0.11122508 0.18984783]\n",
            "Min/max predictions: 0.00013933163427282125 0.8789554834365845\n",
            "Loss for this batch: 0.3420\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10602326 0.11133479 0.22429277 0.2166392  0.31299806 0.11546431\n",
            " 0.2544662  0.00077288 0.00605792 0.02814063]\n",
            "Min/max predictions: 8.154684473993257e-05 0.9461572170257568\n",
            "Loss for this batch: 0.3075\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00212886 0.5089119  0.34338498 0.31853074 0.04010747 0.14367047\n",
            " 0.67984813 0.01704043 0.42810458 0.21643879]\n",
            "Min/max predictions: 4.439175972947851e-05 0.8305498361587524\n",
            "Loss for this batch: 0.3425\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.12058448 0.13297096 0.34260264 0.24340823 0.00858019 0.00161966\n",
            " 0.04357662 0.11979274 0.22180884 0.29567635]\n",
            "Min/max predictions: 0.00030955334659665823 0.9739843606948853\n",
            "Loss for this batch: 0.3633\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.33063295 0.02112305 0.30121866 0.27180558 0.24520694 0.06582876\n",
            " 0.5973688  0.06093341 0.00102498 0.3782317 ]\n",
            "Min/max predictions: 4.895128950010985e-05 0.9314960241317749\n",
            "Loss for this batch: 0.3571\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07372276 0.07875898 0.00327994 0.49444807 0.46357882 0.00252294\n",
            " 0.20846578 0.09802707 0.07825516 0.24126504]\n",
            "Min/max predictions: 7.57484303903766e-05 0.901393711566925\n",
            "Loss for this batch: 0.3306\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6265601  0.02741308 0.04031727 0.18002418 0.0482718  0.7171718\n",
            " 0.15132716 0.02063953 0.12906629 0.04206577]\n",
            "Min/max predictions: 3.8243149901973084e-05 0.8663145899772644\n",
            "Loss for this batch: 0.2549\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.54130924 0.641153   0.01034418 0.02366252 0.03057257 0.02165774\n",
            " 0.03990996 0.10032535 0.00697951 0.00366208]\n",
            "Min/max predictions: 0.00019839010201394558 0.8283500075340271\n",
            "Loss for this batch: 0.2791\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07872102 0.05282467 0.02883912 0.03185477 0.01237484 0.2628766\n",
            " 0.14084184 0.05567632 0.0357395  0.08543649]\n",
            "Min/max predictions: 0.00015023282321635634 0.82183837890625\n",
            "Loss for this batch: 0.3536\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.03613256 0.00988227 0.05564952 0.17434713 0.11064295 0.0278556\n",
            " 0.7741383  0.00560627 0.2139153  0.02062049]\n",
            "Min/max predictions: 1.9534283637767658e-05 0.961420476436615\n",
            "Loss for this batch: 0.3056\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [6.7042076e-04 3.8550076e-01 5.6178141e-01 6.5350366e-01 1.7718080e-04\n",
            " 5.2260915e-03 6.7298770e-02 3.5592902e-02 5.7025117e-01 2.0798708e-03]\n",
            "Min/max predictions: 0.00017718080198392272 0.8841203451156616\n",
            "Loss for this batch: 0.3146\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.5435481  0.0764887  0.03160132 0.08812965 0.10556997 0.02687234\n",
            " 0.00509944 0.04159016 0.51402515 0.03379938]\n",
            "Min/max predictions: 6.0998208937235177e-05 0.9842243194580078\n",
            "Loss for this batch: 0.2720\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.6874703  0.03705107 0.3011032  0.07153324 0.01987878 0.00590389\n",
            " 0.12623015 0.4443795  0.18586703 0.4365359 ]\n",
            "Min/max predictions: 0.000110873173980508 0.897673487663269\n",
            "Loss for this batch: 0.3307\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [8.2286757e-01 2.1806773e-02 1.9816266e-01 2.1673666e-01 1.5415001e-01\n",
            " 3.4982616e-01 1.3793561e-02 9.4626139e-06 2.1918738e-01 2.3098698e-01]\n",
            "Min/max predictions: 9.462613888899796e-06 0.9892521500587463\n",
            "Loss for this batch: 0.4000\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.25978854 0.02207732 0.20560314 0.1297552  0.28468952 0.19814985\n",
            " 0.49332094 0.41897875 0.04567683 0.1988117 ]\n",
            "Min/max predictions: 0.00026700712624005973 0.9221019148826599\n",
            "Loss for this batch: 0.3769\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.1136569e-03 9.9507555e-02 6.6408493e-02 1.7023033e-02 6.8405840e-05\n",
            " 9.4612267e-05 6.9851391e-03 2.2832702e-03 8.5619604e-03 1.7430148e-01]\n",
            "Min/max predictions: 6.840583955636248e-05 0.9753012657165527\n",
            "Loss for this batch: 0.3269\n",
            "Sample labels: [1. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [7.5574297e-01 3.2602721e-01 4.2321178e-01 4.2294480e-02 6.2485132e-04\n",
            " 3.1872246e-01 1.3863524e-02 3.3921620e-01 4.6000821e-03 2.0834255e-01]\n",
            "Min/max predictions: 0.00018606371304485947 0.9793000817298889\n",
            "Loss for this batch: 0.3338\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.01890543 0.20292369 0.00340495 0.76711553 0.05730383 0.10191461\n",
            " 0.20667396 0.01015318 0.13398686 0.708216  ]\n",
            "Min/max predictions: 0.00010342885070713237 0.9630292654037476\n",
            "Loss for this batch: 0.3111\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19472174 0.00272418 0.00587758 0.01237431 0.04286404 0.00628045\n",
            " 0.00900154 0.0083678  0.00465266 0.26923025]\n",
            "Min/max predictions: 3.960605681641027e-05 0.8864963054656982\n",
            "Loss for this batch: 0.2950\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00099517 0.66356    0.00950894 0.01163315 0.38826728 0.40841916\n",
            " 0.03378899 0.15446387 0.29058748 0.16262951]\n",
            "Min/max predictions: 3.9126996853156015e-05 0.7982783317565918\n",
            "Loss for this batch: 0.3422\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.18396586 0.23492731 0.28980735 0.25652957 0.08355097 0.21953015\n",
            " 0.1889993  0.01092997 0.00110368 0.00220723]\n",
            "Min/max predictions: 6.727746222168207e-05 0.9555938243865967\n",
            "Loss for this batch: 0.3337\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.4898718e-02 2.0285610e-02 4.6851718e-01 3.0232812e-03 1.0404167e-01\n",
            " 6.3072771e-01 8.5633285e-03 2.4489735e-04 3.3550691e-02 4.2967761e-01]\n",
            "Min/max predictions: 0.0001594213827047497 0.9560990929603577\n",
            "Loss for this batch: 0.3335\n",
            "Sample labels: [0. 1. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
            "Sample predictions: [0.17222874 0.8139704  0.28577462 0.06878245 0.17493732 0.00867316\n",
            " 0.03005761 0.50808346 0.51420414 0.7146136 ]\n",
            "Min/max predictions: 0.00021725174156017601 0.883040726184845\n",
            "Loss for this batch: 0.3635\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00617262 0.3890432  0.02238649 0.22365466 0.12435715 0.74066687\n",
            " 0.15882209 0.3101833  0.00482123 0.01845322]\n",
            "Min/max predictions: 0.00013402249896898866 0.9321205019950867\n",
            "Loss for this batch: 0.2976\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.6781554  0.5145639  0.53730094 0.14118624 0.00731032 0.54013956\n",
            " 0.2056214  0.6338205  0.57023937 0.66031903]\n",
            "Min/max predictions: 0.0003304546989966184 0.8739405870437622\n",
            "Loss for this batch: 0.3074\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [3.7655962e-01 2.0329924e-01 1.6107117e-01 4.7145981e-02 1.0826351e-01\n",
            " 7.3028773e-01 4.6811715e-01 7.5756121e-01 3.4910845e-04 3.0182672e-01]\n",
            "Min/max predictions: 5.89271257922519e-05 0.9171352982521057\n",
            "Loss for this batch: 0.3720\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.03454592 0.4407601  0.25124678 0.17248552 0.02159719 0.6253064\n",
            " 0.40290314 0.22073144 0.32476854 0.18548088]\n",
            "Min/max predictions: 0.00012121827603550628 0.894953727722168\n",
            "Loss for this batch: 0.3197\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.12938654 0.3592697  0.33178    0.00186983 0.01955386 0.3088132\n",
            " 0.0185607  0.20566681 0.00507957 0.00539713]\n",
            "Min/max predictions: 0.00027011733618564904 0.9799755215644836\n",
            "Loss for this batch: 0.3517\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.0656811  0.03350776 0.07834223 0.78847927 0.08558495 0.36524194\n",
            " 0.01291947 0.6047438  0.15756054 0.00754428]\n",
            "Min/max predictions: 0.00012326028081588447 0.9450448155403137\n",
            "Loss for this batch: 0.3154\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.07257071 0.18118352 0.2620087  0.04386572 0.0058293  0.15477161\n",
            " 0.15355851 0.09288313 0.00088889 0.71827763]\n",
            "Min/max predictions: 1.4353550795931369e-05 0.8931556344032288\n",
            "Loss for this batch: 0.3195\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.0397471e-02 3.2991832e-01 8.4375091e-02 4.3480998e-01 3.3648720e-01\n",
            " 7.5776942e-02 3.4942783e-02 1.7043632e-02 5.7827353e-01 1.3156226e-04]\n",
            "Min/max predictions: 4.789719241671264e-06 0.9542262554168701\n",
            "Loss for this batch: 0.2930\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.21791881 0.01000144 0.01061781 0.8442182  0.00338994 0.09901306\n",
            " 0.00663546 0.02800104 0.00523881 0.02544995]\n",
            "Min/max predictions: 0.00018194194126408547 0.9228172302246094\n",
            "Loss for this batch: 0.3240\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
            "Sample predictions: [7.4942172e-02 2.3309585e-02 1.9989012e-01 5.6701773e-01 7.2763944e-01\n",
            " 5.0998700e-01 3.2468772e-04 6.5260291e-01 3.9609528e-01 3.4238368e-01]\n",
            "Min/max predictions: 0.00015271970187313855 0.9278960227966309\n",
            "Loss for this batch: 0.3112\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.30394337 0.01054272 0.02595377 0.14908703 0.19792074 0.00219526\n",
            " 0.00957242 0.13453048 0.10319891 0.21258622]\n",
            "Min/max predictions: 4.03534977522213e-05 0.8557208180427551\n",
            "Loss for this batch: 0.3334\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00943497 0.59419423 0.00755835 0.09926606 0.06359991 0.00490324\n",
            " 0.02094632 0.21733879 0.23112951 0.4524924 ]\n",
            "Min/max predictions: 4.453409928828478e-05 0.9590988159179688\n",
            "Loss for this batch: 0.3955\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.01928591 0.0772491  0.00800652 0.01689282 0.3223284  0.01453249\n",
            " 0.01489713 0.6133705  0.61270064 0.00537944]\n",
            "Min/max predictions: 0.00011089981853729114 0.9401234984397888\n",
            "Loss for this batch: 0.3256\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.05069811 0.07923852 0.1049144  0.17477997 0.25891843 0.25202036\n",
            " 0.10237879 0.00602705 0.6815637  0.01666174]\n",
            "Min/max predictions: 6.179339106893167e-05 0.9908301830291748\n",
            "Loss for this batch: 0.3178\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.35952166 0.28438812 0.00359401 0.29252374 0.68523806 0.04766467\n",
            " 0.37816346 0.21557048 0.10599583 0.01004906]\n",
            "Min/max predictions: 0.00012550315295811743 0.9793155193328857\n",
            "Loss for this batch: 0.3250\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.05921602 0.02783016 0.04680805 0.62025523 0.02621909 0.107176\n",
            " 0.20214331 0.07216852 0.00170309 0.4990922 ]\n",
            "Min/max predictions: 0.00016969707212410867 0.9139465093612671\n",
            "Loss for this batch: 0.2867\n",
            "Sample labels: [1. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6093121  0.29225367 0.18834029 0.17023252 0.5274154  0.6085336\n",
            " 0.01440863 0.22312383 0.10540725 0.06005279]\n",
            "Min/max predictions: 0.00020115850202273577 0.9365626573562622\n",
            "Loss for this batch: 0.3753\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0369767  0.39976797 0.00147673 0.00424094 0.25607508 0.00745514\n",
            " 0.00491883 0.00147858 0.00253153 0.49016538]\n",
            "Min/max predictions: 8.032900950638577e-05 0.9535317420959473\n",
            "Loss for this batch: 0.3649\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.12808886 0.26259664 0.21760002 0.03229521 0.39544132 0.0033955\n",
            " 0.03005634 0.21412873 0.00180047 0.01307251]\n",
            "Min/max predictions: 7.971549348440021e-05 0.9663022756576538\n",
            "Loss for this batch: 0.2890\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [2.0914783e-03 9.0517640e-02 4.1177876e-02 7.9425183e-05 2.8795022e-01\n",
            " 6.3672878e-02 1.5980431e-01 4.5680148e-03 5.1204002e-01 8.0663115e-02]\n",
            "Min/max predictions: 7.94251827755943e-05 0.9273355603218079\n",
            "Loss for this batch: 0.3527\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00407418 0.02033485 0.3563537  0.0014819  0.24436156 0.01556561\n",
            " 0.01509304 0.5432651  0.06490548 0.00154535]\n",
            "Min/max predictions: 6.412852235371247e-05 0.9739055633544922\n",
            "Loss for this batch: 0.3113\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            "Sample predictions: [0.2592895  0.5556962  0.31782457 0.01696869 0.03688661 0.3220066\n",
            " 0.52156484 0.73517394 0.6846261  0.1220616 ]\n",
            "Min/max predictions: 0.00021348107839003205 0.8379305005073547\n",
            "Loss for this batch: 0.3499\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.44213593 0.19632776 0.06866268 0.05374921 0.46415097 0.02230843\n",
            " 0.8159324  0.3774331  0.00104444 0.46399745]\n",
            "Min/max predictions: 0.0002702876226976514 0.8853155374526978\n",
            "Loss for this batch: 0.3837\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23447895 0.02282359 0.03139644 0.10348441 0.69155145 0.6065269\n",
            " 0.06790944 0.44686407 0.05807986 0.01698392]\n",
            "Min/max predictions: 0.00018654708401300013 0.9534415006637573\n",
            "Loss for this batch: 0.4358\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.44721487 0.39574483 0.0224744  0.00744073 0.06615915 0.69103\n",
            " 0.01352805 0.00593361 0.26467484 0.04052833]\n",
            "Min/max predictions: 5.911641346756369e-05 0.9265009164810181\n",
            "Loss for this batch: 0.3515\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02731756 0.1108716  0.11789747 0.0382105  0.06120593 0.14939165\n",
            " 0.02508712 0.01155118 0.02039287 0.0104486 ]\n",
            "Min/max predictions: 4.923415326629765e-05 0.9603884220123291\n",
            "Loss for this batch: 0.3081\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14245245 0.87847364 0.10462784 0.02793161 0.09982777 0.38815188\n",
            " 0.41330346 0.34455884 0.24904078 0.41068333]\n",
            "Min/max predictions: 0.00015049493231344968 0.980782151222229\n",
            "Loss for this batch: 0.3117\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.22370943 0.43423197 0.39651906 0.72913665 0.2432904  0.20148689\n",
            " 0.11776975 0.02150357 0.590585   0.5887772 ]\n",
            "Min/max predictions: 0.0001180358522105962 0.930984377861023\n",
            "Loss for this batch: 0.3407\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.42035237 0.22915171 0.00637531 0.43355766 0.04533641 0.00281591\n",
            " 0.2153686  0.5672636  0.34300515 0.00262482]\n",
            "Min/max predictions: 6.991122791077942e-05 0.857858419418335\n",
            "Loss for this batch: 0.3629\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.7186415e-04 1.1268274e-01 1.9379845e-03 1.9479973e-02 6.6599530e-01\n",
            " 2.0395236e-01 7.2553182e-01 1.3063233e-02 1.7059509e-01 9.0677850e-02]\n",
            "Min/max predictions: 0.00010315001418348402 0.8190588355064392\n",
            "Loss for this batch: 0.3197\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [6.3157296e-03 9.5879012e-01 4.0355586e-03 9.5506897e-03 2.0013765e-02\n",
            " 4.4961426e-01 2.3732206e-01 5.7640183e-04 4.8889932e-03 6.2494650e-02]\n",
            "Min/max predictions: 0.00012902212620247155 0.958790123462677\n",
            "Loss for this batch: 0.4065\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [3.7432325e-01 2.7274867e-04 1.1209153e-03 1.2057546e-01 3.7030762e-01\n",
            " 8.6322892e-01 9.0895697e-02 5.9840642e-03 6.4945483e-01 1.2961080e-02]\n",
            "Min/max predictions: 9.15261116460897e-05 0.9118901491165161\n",
            "Loss for this batch: 0.3225\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.14462219 0.3491062  0.1796434  0.2636194  0.35635826 0.07300671\n",
            " 0.24774553 0.29755425 0.01742318 0.1418936 ]\n",
            "Min/max predictions: 0.00012233394954819232 0.9088260531425476\n",
            "Loss for this batch: 0.3990\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.15344079 0.16272323 0.37111712 0.5633532  0.00180275 0.01911679\n",
            " 0.36014974 0.32104084 0.06947536 0.0788415 ]\n",
            "Min/max predictions: 0.00022666486620437354 0.9789090156555176\n",
            "Loss for this batch: 0.3694\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07992695 0.17592023 0.48315006 0.01567812 0.74351895 0.31307346\n",
            " 0.00429413 0.14177121 0.01571746 0.02180841]\n",
            "Min/max predictions: 0.00016160511586349458 0.9316729307174683\n",
            "Loss for this batch: 0.3186\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.38052085 0.59183216 0.25981945 0.2038251  0.40788656 0.14308888\n",
            " 0.01085759 0.2457577  0.01223544 0.3093319 ]\n",
            "Min/max predictions: 0.0002369470603298396 0.8622906804084778\n",
            "Loss for this batch: 0.3083\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.16861786 0.00348184 0.02327471 0.20726421 0.4677449  0.38325578\n",
            " 0.07378156 0.12315469 0.793446   0.04632306]\n",
            "Min/max predictions: 4.7264158638427034e-05 0.8888488411903381\n",
            "Loss for this batch: 0.3198\n",
            "Sample labels: [1. 0. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.13500756 0.04918978 0.2533475  0.2691544  0.16761947 0.6444765\n",
            " 0.02526878 0.55919427 0.0289601  0.56967217]\n",
            "Min/max predictions: 2.8574429961736314e-05 0.9010575413703918\n",
            "Loss for this batch: 0.3270\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.5956592  0.8555471  0.01127218 0.30001673 0.61110663 0.00424149\n",
            " 0.00123515 0.0210556  0.01012577 0.00603011]\n",
            "Min/max predictions: 8.685216016601771e-05 0.8555470705032349\n",
            "Loss for this batch: 0.2678\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.5059356  0.00966609 0.14261425 0.01627758 0.11750609 0.11706264\n",
            " 0.00393688 0.26493987 0.01661676 0.6658348 ]\n",
            "Min/max predictions: 8.82047024788335e-05 0.8766875863075256\n",
            "Loss for this batch: 0.3425\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.24488083 0.04537689 0.04366796 0.36476618 0.17868882 0.5254843\n",
            " 0.25264487 0.00706379 0.69295096 0.03836877]\n",
            "Min/max predictions: 5.098620385979302e-05 0.9115363955497742\n",
            "Loss for this batch: 0.3538\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00198577 0.48474953 0.7705736  0.27495515 0.19240935 0.25282124\n",
            " 0.21139537 0.00938004 0.05385521 0.0903326 ]\n",
            "Min/max predictions: 8.124391024466604e-05 0.915891170501709\n",
            "Loss for this batch: 0.3518\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00304716 0.06844648 0.3833428  0.33658746 0.00512423 0.11308577\n",
            " 0.02026162 0.02526762 0.25591546 0.00588976]\n",
            "Min/max predictions: 1.955980587808881e-05 0.945883572101593\n",
            "Loss for this batch: 0.3470\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.12849742 0.1618236  0.8839843  0.01946995 0.00209164 0.01490886\n",
            " 0.00938471 0.44668332 0.24689461 0.02297912]\n",
            "Min/max predictions: 0.00015929086657706648 0.897864818572998\n",
            "Loss for this batch: 0.3734\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [1.5062852e-01 5.1797223e-01 1.6819020e-01 7.0438105e-01 1.1506496e-01\n",
            " 1.5631990e-04 1.5114625e-01 4.8160539e-03 3.5090506e-01 1.1615335e-01]\n",
            "Min/max predictions: 0.00015631990390829742 0.8688082098960876\n",
            "Loss for this batch: 0.3209\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.59223026 0.07369848 0.4086456  0.00566972 0.24351029 0.00538117\n",
            " 0.29502332 0.00667049 0.26366282 0.05206454]\n",
            "Min/max predictions: 8.60883665154688e-05 0.8949989676475525\n",
            "Loss for this batch: 0.3669\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.65763324 0.5473667  0.2143115  0.43085265 0.2915689  0.12246735\n",
            " 0.01742608 0.20404693 0.11705817 0.12317596]\n",
            "Min/max predictions: 0.000129497901070863 0.9004799127578735\n",
            "Loss for this batch: 0.3872\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00146669 0.07826322 0.26677954 0.11731439 0.13862464 0.3781222\n",
            " 0.00169888 0.14283483 0.05226293 0.24723904]\n",
            "Min/max predictions: 0.0001236695097759366 0.917259931564331\n",
            "Loss for this batch: 0.3750\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15783334 0.00099094 0.3426467  0.21124606 0.53076893 0.0211626\n",
            " 0.16834758 0.06636404 0.01167562 0.00596747]\n",
            "Min/max predictions: 0.00010475513408891857 0.8854507207870483\n",
            "Loss for this batch: 0.3605\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.13696164 0.00086198 0.20897754 0.30012596 0.35709012 0.22239454\n",
            " 0.00172875 0.5501967  0.06636404 0.00547988]\n",
            "Min/max predictions: 6.165299510030309e-06 0.9246426820755005\n",
            "Loss for this batch: 0.3606\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01430064 0.70535195 0.72822714 0.12958398 0.01616966 0.28671044\n",
            " 0.06602757 0.3858758  0.0237625  0.00265238]\n",
            "Min/max predictions: 0.000300584826618433 0.9223832488059998\n",
            "Loss for this batch: 0.3518\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.13454856e-01 1.30879447e-01 1.87333971e-02 1.83658287e-01\n",
            " 1.74053217e-04 1.40263056e-02 7.14896666e-03 8.88205990e-02\n",
            " 2.71363501e-02 3.58206895e-03]\n",
            "Min/max predictions: 0.00017405321705155075 0.91338050365448\n",
            "Loss for this batch: 0.3669\n",
            "Sample labels: [0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01937373 0.16946784 0.16834836 0.3647271  0.18865685 0.47101662\n",
            " 0.04841743 0.1468969  0.32231396 0.0480242 ]\n",
            "Min/max predictions: 5.256407894194126e-05 0.9006394147872925\n",
            "Loss for this batch: 0.3313\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.00341976 0.00290231 0.00567255 0.10665917 0.03444231 0.268616\n",
            " 0.28475064 0.02051962 0.17082472 0.29719597]\n",
            "Min/max predictions: 0.00024676823522895575 0.8233017921447754\n",
            "Loss for this batch: 0.3657\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.03112131 0.00094691 0.80280846 0.28046352 0.0047734  0.77430415\n",
            " 0.39597037 0.56398374 0.02493114 0.18900359]\n",
            "Min/max predictions: 0.0001302730815950781 0.9651515483856201\n",
            "Loss for this batch: 0.2698\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.21508929 0.00971784 0.16055067 0.19723356 0.41850433 0.0936021\n",
            " 0.03195599 0.01036012 0.18330665 0.02543723]\n",
            "Min/max predictions: 9.824085282161832e-05 0.9326537847518921\n",
            "Loss for this batch: 0.3369\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.14518704 0.04115482 0.505748   0.29529086 0.20735866 0.47677273\n",
            " 0.74191374 0.00579482 0.52772987 0.25241545]\n",
            "Min/max predictions: 3.0318366043502465e-05 0.9744293689727783\n",
            "Loss for this batch: 0.3406\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.26814622 0.0592749  0.03606001 0.3190853  0.02181657 0.19265373\n",
            " 0.1139975  0.00391889 0.26617947 0.0093032 ]\n",
            "Min/max predictions: 5.2939656598027796e-05 0.8672465682029724\n",
            "Loss for this batch: 0.3223\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.00489928 0.21156876 0.39475742 0.01173411 0.16617773 0.14781061\n",
            " 0.01874309 0.31301102 0.47273543 0.24998724]\n",
            "Min/max predictions: 4.874297519563697e-05 0.9737957715988159\n",
            "Loss for this batch: 0.3398\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.43348014 0.00073075 0.10497611 0.19399478 0.00781433 0.00182134\n",
            " 0.00699891 0.18301876 0.07470889 0.00308913]\n",
            "Min/max predictions: 3.211733201169409e-05 0.8722940683364868\n",
            "Loss for this batch: 0.3286\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [4.3349424e-01 5.7547577e-02 3.1390867e-01 1.3573968e-01 2.8097148e-03\n",
            " 2.5889513e-01 2.6122868e-01 3.2682663e-01 2.3468219e-01 1.8320563e-04]\n",
            "Min/max predictions: 2.2641432224190794e-05 0.9637008309364319\n",
            "Loss for this batch: 0.3568\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04114393 0.00036616 0.01306658 0.1981448  0.19905879 0.01541846\n",
            " 0.01161068 0.15103476 0.00397952 0.1525405 ]\n",
            "Min/max predictions: 5.070628321846016e-05 0.9859992861747742\n",
            "Loss for this batch: 0.2984\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.10167477 0.09982973 0.3343669  0.05061302 0.09053217 0.01756132\n",
            " 0.52593935 0.5270961  0.00659278 0.07942425]\n",
            "Min/max predictions: 0.00011125486344099045 0.8497958183288574\n",
            "Loss for this batch: 0.3508\n",
            "Sample labels: [0. 1. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00996241 0.84274215 0.3748931  0.31642473 0.26261076 0.27451414\n",
            " 0.00449556 0.40170422 0.00196324 0.00532834]\n",
            "Min/max predictions: 7.44878634577617e-05 0.9860281944274902\n",
            "Loss for this batch: 0.3270\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00157494 0.4429292  0.03848412 0.0211305  0.04412083 0.8370321\n",
            " 0.3242405  0.39095104 0.00442839 0.29184082]\n",
            "Min/max predictions: 3.6741254007210955e-05 0.8999261856079102\n",
            "Loss for this batch: 0.3251\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.27449548 0.10168628 0.6721766  0.01615673 0.00104158 0.00085567\n",
            " 0.08445324 0.4876027  0.05459724 0.57639956]\n",
            "Min/max predictions: 5.190942465560511e-05 0.9872092008590698\n",
            "Loss for this batch: 0.3117\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.06959453 0.01867599 0.18438992 0.2558623  0.5495398  0.09472921\n",
            " 0.14193854 0.3086451  0.10745108 0.41039208]\n",
            "Min/max predictions: 0.00020112571655772626 0.9362786412239075\n",
            "Loss for this batch: 0.3184\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00185045 0.15899806 0.13845916 0.3256126  0.08266312 0.22210367\n",
            " 0.3319308  0.41915125 0.10062822 0.04511756]\n",
            "Min/max predictions: 3.199855927960016e-05 0.9691948294639587\n",
            "Loss for this batch: 0.3708\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17057857 0.02195091 0.6847679  0.01248495 0.00076746 0.00231741\n",
            " 0.08488677 0.261782   0.31770235 0.00422219]\n",
            "Min/max predictions: 8.056928345467895e-05 0.9339646697044373\n",
            "Loss for this batch: 0.3707\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.07034659 0.01119058 0.13202618 0.01960645 0.17551818 0.01700604\n",
            " 0.08173962 0.25376967 0.05978444 0.35119098]\n",
            "Min/max predictions: 0.0002613549877423793 0.9419333934783936\n",
            "Loss for this batch: 0.3173\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00618835 0.01633445 0.00391682 0.01194897 0.04414551 0.64402306\n",
            " 0.05104966 0.2027353  0.00221338 0.01824204]\n",
            "Min/max predictions: 0.00011089029430877417 0.8825433254241943\n",
            "Loss for this batch: 0.2831\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00373537 0.03819979 0.5772756  0.17488867 0.06265978 0.21007952\n",
            " 0.09471104 0.26503038 0.36579156 0.15216176]\n",
            "Min/max predictions: 0.00014579542039427906 0.9747833013534546\n",
            "Loss for this batch: 0.3863\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.43274912 0.553128   0.01767221 0.31064013 0.00964151 0.06986048\n",
            " 0.03426954 0.01985587 0.00071233 0.0148599 ]\n",
            "Min/max predictions: 0.00014417585043702275 0.9908007979393005\n",
            "Loss for this batch: 0.3068\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05826997 0.2998585  0.12088006 0.29142764 0.46538094 0.00316935\n",
            " 0.01136279 0.43985137 0.07978895 0.56172246]\n",
            "Min/max predictions: 0.0001873463625088334 0.955635666847229\n",
            "Loss for this batch: 0.3611\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.2745647  0.52805465 0.0126127  0.28876    0.01035086 0.00581257\n",
            " 0.10460007 0.05139899 0.83303016 0.10626791]\n",
            "Min/max predictions: 0.00010557715722825378 0.8330301642417908\n",
            "Loss for this batch: 0.3490\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.16030914 0.02453603 0.07206424 0.00094062 0.06074493 0.2744575\n",
            " 0.01446975 0.33483735 0.00117404 0.03339294]\n",
            "Min/max predictions: 0.0001497427001595497 0.8422620296478271\n",
            "Loss for this batch: 0.3175\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4065364  0.01745323 0.09772036 0.0009651  0.11447778 0.14560369\n",
            " 0.00695316 0.04918848 0.27288136 0.00719903]\n",
            "Min/max predictions: 8.802530646789819e-05 0.9049497246742249\n",
            "Loss for this batch: 0.3469\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00217163 0.00455064 0.9291968  0.2352079  0.00947265 0.03166986\n",
            " 0.13727242 0.24253902 0.10371108 0.00782079]\n",
            "Min/max predictions: 6.05346831434872e-05 0.9291967749595642\n",
            "Loss for this batch: 0.3898\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.10464561 0.0017251  0.26037848 0.0896783  0.613098   0.00888523\n",
            " 0.8257165  0.05162194 0.00427539 0.00221878]\n",
            "Min/max predictions: 0.00010921564535237849 0.9596919417381287\n",
            "Loss for this batch: 0.3598\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [2.6679516e-03 2.5208504e-04 3.9928219e-01 2.6334518e-01 1.6534801e-03\n",
            " 4.2668633e-02 2.7387168e-03 1.2714215e-01 3.3639029e-01 7.6160161e-04]\n",
            "Min/max predictions: 0.00011163232557009906 0.9730726480484009\n",
            "Loss for this batch: 0.2954\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [0.20096229 0.03665885 0.09453835 0.33361977 0.3992386  0.21076311\n",
            " 0.4897868  0.4123433  0.56661654 0.13243295]\n",
            "Min/max predictions: 0.00018721778178587556 0.9153871536254883\n",
            "Loss for this batch: 0.3916\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.6980789  0.00698325 0.38426176 0.045091   0.13225517 0.00585174\n",
            " 0.15648888 0.09975342 0.38320348 0.76361775]\n",
            "Min/max predictions: 0.00016022588533814996 0.9593609571456909\n",
            "Loss for this batch: 0.3508\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.18443075 0.11119863 0.0352239  0.00137641 0.06756286 0.14361629\n",
            " 0.01074501 0.13601883 0.00926566 0.19412784]\n",
            "Min/max predictions: 0.0001329729420831427 0.9350789189338684\n",
            "Loss for this batch: 0.3048\n",
            "Sample labels: [0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00108708 0.52487403 0.47304484 0.10922848 0.7028221  0.5168119\n",
            " 0.04535987 0.05516541 0.43897015 0.02443652]\n",
            "Min/max predictions: 8.081259875325486e-05 0.9619449377059937\n",
            "Loss for this batch: 0.3465\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10478629 0.01034186 0.08055721 0.71678305 0.5679792  0.10452911\n",
            " 0.00967828 0.01210308 0.12312597 0.0036724 ]\n",
            "Min/max predictions: 0.0001013089859043248 0.8676298260688782\n",
            "Loss for this batch: 0.2844\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [4.9697676e-01 1.6961479e-01 1.6688518e-01 1.9603688e-03 4.5467421e-04\n",
            " 2.7373660e-01 3.7387067e-01 3.1522971e-02 4.5373946e-01 3.1399138e-02]\n",
            "Min/max predictions: 0.00022610601445194334 0.9601383805274963\n",
            "Loss for this batch: 0.3115\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0293913  0.01662684 0.0391559  0.09927911 0.5841036  0.00828618\n",
            " 0.0013036  0.08416443 0.09575965 0.36300856]\n",
            "Min/max predictions: 2.4349395971512422e-05 0.9034987092018127\n",
            "Loss for this batch: 0.3763\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.20633543 0.3191126  0.5064629  0.00332367 0.0337523  0.12758914\n",
            " 0.00322265 0.00408135 0.20320691 0.54491484]\n",
            "Min/max predictions: 0.00011493227066239342 0.9545484185218811\n",
            "Loss for this batch: 0.3249\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.06977663 0.7198007  0.7626548  0.03530907 0.11931133 0.05263815\n",
            " 0.66164154 0.13476282 0.3174265  0.30556092]\n",
            "Min/max predictions: 0.00026166116003878415 0.8699942231178284\n",
            "Loss for this batch: 0.4254\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.586294   0.06863855 0.09584492 0.14144672 0.1895848  0.7074633\n",
            " 0.17604515 0.02589269 0.6309303  0.09668663]\n",
            "Min/max predictions: 1.9582294044084847e-05 0.985927939414978\n",
            "Loss for this batch: 0.2972\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02748076 0.25760576 0.00379306 0.4514396  0.06107341 0.00096398\n",
            " 0.44285905 0.39112443 0.15067564 0.17660421]\n",
            "Min/max predictions: 4.805280696018599e-05 0.9235152006149292\n",
            "Loss for this batch: 0.3436\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14885607 0.03837222 0.13292675 0.03930461 0.3139037  0.0055501\n",
            " 0.00377229 0.2603727  0.36588505 0.03577958]\n",
            "Min/max predictions: 2.7841060727951117e-05 0.9664992094039917\n",
            "Loss for this batch: 0.3108\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.09560802 0.03416062 0.26684055 0.01698111 0.16210666 0.12505284\n",
            " 0.00129166 0.7264882  0.0013981  0.00100314]\n",
            "Min/max predictions: 3.525225110934116e-05 0.9319947361946106\n",
            "Loss for this batch: 0.3420\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.03045955 0.2867891  0.1656442  0.3349871  0.5058821  0.3932662\n",
            " 0.53587896 0.5349679  0.34382913 0.24313357]\n",
            "Min/max predictions: 4.900182830169797e-05 0.8465461134910583\n",
            "Loss for this batch: 0.3601\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.7267148e-01 3.5712826e-03 3.2064298e-03 2.5966968e-02 7.2411555e-01\n",
            " 1.2166250e-03 1.2398792e-01 2.8037000e-02 2.2813557e-01 4.8515562e-04]\n",
            "Min/max predictions: 0.0003656286862678826 0.8394559025764465\n",
            "Loss for this batch: 0.3625\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.8433615  0.00432684 0.1473799  0.05465733 0.00761861 0.00324959\n",
            " 0.058594   0.1114822  0.09300832 0.16030939]\n",
            "Min/max predictions: 0.00016057108587119728 0.9053999781608582\n",
            "Loss for this batch: 0.3097\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00519036 0.13404265 0.01115239 0.70922565 0.6766341  0.00488968\n",
            " 0.00213782 0.5022305  0.00181249 0.05058379]\n",
            "Min/max predictions: 8.90630908543244e-05 0.957650899887085\n",
            "Loss for this batch: 0.3775\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10094634 0.06872214 0.5018144  0.6638566  0.07848189 0.00755231\n",
            " 0.0421866  0.13377658 0.03343052 0.37206596]\n",
            "Min/max predictions: 1.5843153960304335e-05 0.9207879900932312\n",
            "Loss for this batch: 0.4094\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [7.0625782e-04 2.4819275e-02 2.0447355e-02 2.2528505e-01 1.5776055e-01\n",
            " 1.6155608e-01 1.5129782e-01 5.8377209e-05 2.4864164e-01 6.7605573e-01]\n",
            "Min/max predictions: 4.0074966818792745e-05 0.9318404793739319\n",
            "Loss for this batch: 0.3507\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.3444541  0.16482216 0.11025339 0.00772373 0.78495467 0.06383348\n",
            " 0.11939266 0.6064335  0.1456589  0.01722426]\n",
            "Min/max predictions: 0.0001351915270788595 0.8432627320289612\n",
            "Loss for this batch: 0.3577\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6912933  0.3299697  0.06285014 0.31470665 0.00755739 0.44006205\n",
            " 0.34496173 0.03998709 0.02427607 0.26138055]\n",
            "Min/max predictions: 0.00020022736862301826 0.8334402441978455\n",
            "Loss for this batch: 0.3553\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [3.2480091e-01 3.0714780e-01 7.5471744e-02 3.0674096e-03 1.8158728e-01\n",
            " 1.5117675e-01 7.1426815e-01 1.4060568e-04 1.0043264e-03 6.9753855e-02]\n",
            "Min/max predictions: 0.00014060568355489522 0.9259358048439026\n",
            "Loss for this batch: 0.2921\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.6344669e-01 6.3161422e-03 3.6806380e-04 1.5900226e-01 9.2414014e-02\n",
            " 5.0324809e-02 6.3980776e-03 1.8373980e-01 2.4401689e-02 3.7412509e-01]\n",
            "Min/max predictions: 0.0002628874499350786 0.9060550332069397\n",
            "Loss for this batch: 0.3530\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2419298  0.03927406 0.26790762 0.1806977  0.16705191 0.5866885\n",
            " 0.18329668 0.13890444 0.01805792 0.17853175]\n",
            "Min/max predictions: 0.00021698488853871822 0.7906644344329834\n",
            "Loss for this batch: 0.2866\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [0.8200916  0.2362616  0.5157821  0.82223785 0.00923611 0.03065303\n",
            " 0.18430558 0.61602676 0.26168567 0.0028856 ]\n",
            "Min/max predictions: 0.00029764443752355874 0.9333932995796204\n",
            "Loss for this batch: 0.3295\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00858017 0.32301462 0.04577453 0.23905542 0.01416395 0.13608612\n",
            " 0.02670458 0.35691398 0.84536624 0.03265123]\n",
            "Min/max predictions: 5.948200123384595e-05 0.9513190984725952\n",
            "Loss for this batch: 0.3274\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.55695605 0.15766159 0.51489556 0.0694012  0.00539482 0.31498858\n",
            " 0.3876056  0.267615   0.4930729  0.10558899]\n",
            "Min/max predictions: 9.644217789173126e-05 0.9454939961433411\n",
            "Loss for this batch: 0.3458\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [3.7657411e-03 4.8093639e-02 4.7077489e-01 4.4505998e-01 3.7822578e-02\n",
            " 1.1597495e-05 3.8903025e-01 4.3400007e-01 5.7874266e-02 7.1124233e-02]\n",
            "Min/max predictions: 1.1597495358728338e-05 0.9706650972366333\n",
            "Loss for this batch: 0.4206\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4170822  0.19402556 0.00134209 0.00537161 0.50124747 0.00062949\n",
            " 0.61905575 0.05649678 0.381612   0.2916846 ]\n",
            "Min/max predictions: 3.8123351259855554e-05 0.9602019786834717\n",
            "Loss for this batch: 0.3734\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.03158587 0.51743174 0.33809757 0.3134954  0.2571598  0.27715072\n",
            " 0.27670854 0.47596085 0.00110863 0.34964287]\n",
            "Min/max predictions: 5.710464756703004e-05 0.9626826643943787\n",
            "Loss for this batch: 0.3171\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.52438027 0.09094939 0.667553   0.76328534 0.79101825 0.0083106\n",
            " 0.11027312 0.003398   0.62933606 0.07660975]\n",
            "Min/max predictions: 0.0003335712244734168 0.8544785976409912\n",
            "Loss for this batch: 0.3699\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.15932919 0.0163352  0.05826477 0.40339163 0.01020762 0.01306008\n",
            " 0.17565596 0.0495961  0.07953496 0.48811775]\n",
            "Min/max predictions: 0.0003685056581161916 0.7755184173583984\n",
            "Loss for this batch: 0.3853\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.36575076 0.5866255  0.06386565 0.32653692 0.553422   0.32252085\n",
            " 0.0101357  0.00692306 0.26857826 0.41339725]\n",
            "Min/max predictions: 8.618651918368414e-05 0.861827552318573\n",
            "Loss for this batch: 0.3973\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.05521908 0.9508441  0.01714813 0.02001715 0.03666428 0.02243314\n",
            " 0.00425346 0.45007327 0.22071321 0.28635198]\n",
            "Min/max predictions: 9.451100049773231e-05 0.9609825611114502\n",
            "Loss for this batch: 0.2884\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.5640754e-04 6.0275819e-02 1.4642321e-01 8.0185637e-02 4.6634849e-02\n",
            " 3.3400953e-01 2.1777911e-02 4.7038600e-01 2.6224443e-01 3.0748703e-02]\n",
            "Min/max predictions: 2.336322722840123e-05 0.9697486758232117\n",
            "Loss for this batch: 0.4040\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.73638976 0.5246856  0.18744586 0.00515896 0.46875316 0.02109917\n",
            " 0.02211897 0.0217569  0.08356848 0.0042152 ]\n",
            "Min/max predictions: 0.00010447422391735017 0.862336277961731\n",
            "Loss for this batch: 0.3666\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.00441244 0.01344342 0.16962904 0.00446541 0.00849642 0.05282715\n",
            " 0.8741621  0.00622795 0.5345545  0.16329302]\n",
            "Min/max predictions: 0.00013285456225275993 0.8741620779037476\n",
            "Loss for this batch: 0.3401\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.18315534 0.40371126 0.53493243 0.03131333 0.09261011 0.00256357\n",
            " 0.01721033 0.25619325 0.09393779 0.00438329]\n",
            "Min/max predictions: 6.113464769441634e-05 0.9133512377738953\n",
            "Loss for this batch: 0.3213\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [3.4714273e-01 1.1733242e-03 4.6197781e-01 6.4963289e-04 2.0139487e-01\n",
            " 9.9142466e-04 5.0713435e-02 7.0139766e-01 6.5967530e-01 3.4909125e-02]\n",
            "Min/max predictions: 9.241345833288506e-05 0.935926616191864\n",
            "Loss for this batch: 0.3143\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.5888079  0.2865491  0.01106783 0.11633926 0.87225527 0.00602194\n",
            " 0.0053313  0.50096387 0.00635407 0.7318622 ]\n",
            "Min/max predictions: 0.00021390651818364859 0.8900817036628723\n",
            "Loss for this batch: 0.3234\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.5736095  0.09393533 0.00655916 0.00338809 0.04983151 0.5713058\n",
            " 0.22267303 0.6109353  0.69675905 0.00789736]\n",
            "Min/max predictions: 0.00013899710029363632 0.8571726679801941\n",
            "Loss for this batch: 0.3118\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.07918409 0.06870587 0.03036498 0.01736583 0.21295108 0.03036396\n",
            " 0.13642968 0.3921605  0.01091659 0.6415686 ]\n",
            "Min/max predictions: 0.00015565429930575192 0.9691174626350403\n",
            "Loss for this batch: 0.3619\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01931896 0.16238031 0.20077701 0.04274735 0.28559005 0.34256747\n",
            " 0.3059627  0.32628927 0.63465154 0.03327297]\n",
            "Min/max predictions: 8.289831748697907e-05 0.8850533962249756\n",
            "Loss for this batch: 0.3420\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00514163 0.07272967 0.00111711 0.5897262  0.26198357 0.00758226\n",
            " 0.31207058 0.14168099 0.38425243 0.07643241]\n",
            "Min/max predictions: 4.0085400542011485e-05 0.9016522169113159\n",
            "Loss for this batch: 0.3140\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [3.2950288e-01 1.2435834e-04 8.2411848e-02 2.7175727e-03 2.6977269e-03\n",
            " 9.7590774e-02 3.4202494e-02 2.8075209e-01 4.0107369e-03 4.8366597e-01]\n",
            "Min/max predictions: 0.00010365347407059744 0.9554368257522583\n",
            "Loss for this batch: 0.4135\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.23642488 0.01788216 0.03026601 0.14432591 0.4952465  0.13446599\n",
            " 0.07856634 0.16320226 0.02019304 0.7126114 ]\n",
            "Min/max predictions: 5.8471203374210745e-05 0.9058731198310852\n",
            "Loss for this batch: 0.3305\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00803307 0.01988409 0.62771213 0.14572111 0.02997246 0.3684818\n",
            " 0.0049496  0.33075678 0.03464352 0.01074972]\n",
            "Min/max predictions: 0.00026301079196855426 0.9697364568710327\n",
            "Loss for this batch: 0.3376\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01520845 0.010613   0.06702357 0.33799082 0.24759571 0.62397826\n",
            " 0.00069302 0.01066867 0.48156235 0.01148024]\n",
            "Min/max predictions: 8.782526128925383e-05 0.9631421566009521\n",
            "Loss for this batch: 0.3652\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.21186551 0.11669555 0.00993922 0.00147721 0.00440497 0.00150679\n",
            " 0.01817073 0.8201453  0.00527242 0.035228  ]\n",
            "Min/max predictions: 0.0001120567976613529 0.862791895866394\n",
            "Loss for this batch: 0.3108\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.4859767  0.01632658 0.09595682 0.35651207 0.00992015 0.22279622\n",
            " 0.49496806 0.28687453 0.00099683 0.0789789 ]\n",
            "Min/max predictions: 8.814811735646799e-05 0.881374716758728\n",
            "Loss for this batch: 0.3134\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01340887 0.00060353 0.41169554 0.32216465 0.29800007 0.05587961\n",
            " 0.00198704 0.01293421 0.1041498  0.00051952]\n",
            "Min/max predictions: 0.00012985648936592042 0.8935866355895996\n",
            "Loss for this batch: 0.3786\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.31187442e-02 1.98613798e-05 2.41891900e-03 3.22654068e-01\n",
            " 1.00124694e-01 4.39506799e-01 1.53561821e-02 5.60713150e-02\n",
            " 1.31045461e-01 1.20847769e-01]\n",
            "Min/max predictions: 1.9861379769281484e-05 0.9364517331123352\n",
            "Loss for this batch: 0.3313\n",
            "Sample labels: [0. 0. 0. 1. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0212602  0.01086131 0.2176643  0.6290112  0.18531975 0.19461009\n",
            " 0.00301788 0.00437222 0.22582616 0.87876534]\n",
            "Min/max predictions: 9.590861009201035e-05 0.9441149830818176\n",
            "Loss for this batch: 0.4084\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.04336488 0.45365447 0.5815871  0.13828069 0.10443354 0.01667473\n",
            " 0.09255008 0.02176191 0.01373179 0.24048008]\n",
            "Min/max predictions: 0.00010456002928549424 0.9245124459266663\n",
            "Loss for this batch: 0.3432\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [3.55203694e-04 1.69326127e-01 1.75482035e-02 1.58264220e-01\n",
            " 2.67920524e-01 1.13207735e-01 6.59509599e-01 1.95778385e-02\n",
            " 7.75471190e-03 1.33551881e-01]\n",
            "Min/max predictions: 8.817543857730925e-05 0.8642317056655884\n",
            "Loss for this batch: 0.3610\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.2658200e-03 3.1540099e-01 4.5425820e-01 1.4972914e-04 9.9482961e-02\n",
            " 4.5946774e-01 4.1370258e-02 8.5136831e-02 3.2338363e-01 3.4628667e-02]\n",
            "Min/max predictions: 0.00014972913777455688 0.9803751111030579\n",
            "Loss for this batch: 0.3484\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.31470278 0.05824366 0.32036063 0.2050778  0.781604   0.37965488\n",
            " 0.00599787 0.20021382 0.4068151  0.45404914]\n",
            "Min/max predictions: 1.972097743418999e-05 0.90618497133255\n",
            "Loss for this batch: 0.3669\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.02364721 0.02057005 0.13971089 0.144129   0.0372331  0.23758744\n",
            " 0.05689801 0.29698503 0.1617585  0.54434794]\n",
            "Min/max predictions: 0.00016226366278715432 0.8577178716659546\n",
            "Loss for this batch: 0.3227\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2799083  0.0549632  0.05704848 0.03670857 0.6667048  0.13989519\n",
            " 0.06840758 0.19826733 0.03386764 0.14967398]\n",
            "Min/max predictions: 1.580152274982538e-05 0.8019176721572876\n",
            "Loss for this batch: 0.3742\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01102377 0.19030714 0.14701165 0.22548795 0.02246811 0.06401005\n",
            " 0.0020839  0.02206111 0.53897595 0.10871554]\n",
            "Min/max predictions: 6.124814535723999e-05 0.8864341974258423\n",
            "Loss for this batch: 0.2987\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.13825038 0.35539412 0.00824975 0.46795854 0.35278323 0.06306758\n",
            " 0.23710854 0.03340165 0.23224902 0.09228002]\n",
            "Min/max predictions: 0.00016545623657293618 0.8530852198600769\n",
            "Loss for this batch: 0.3302\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.0120758  0.4232851  0.16736552 0.02270774 0.00233493 0.02978682\n",
            " 0.224279   0.11859326 0.15893328 0.02726653]\n",
            "Min/max predictions: 0.0003503510670270771 0.9748968482017517\n",
            "Loss for this batch: 0.3442\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6360933  0.52693933 0.09028643 0.6172308  0.00547516 0.40035546\n",
            " 0.00087858 0.00152547 0.03791705 0.06721204]\n",
            "Min/max predictions: 0.0002341016661375761 0.8701211214065552\n",
            "Loss for this batch: 0.3203\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.35442752 0.02732179 0.19242197 0.31188107 0.00350402 0.46036237\n",
            " 0.01449724 0.00215705 0.59853786 0.21117072]\n",
            "Min/max predictions: 3.082229886786081e-05 0.9237568974494934\n",
            "Loss for this batch: 0.2867\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04849577 0.2729284  0.128049   0.11421486 0.19894458 0.05867367\n",
            " 0.12620188 0.33909205 0.00742416 0.25599882]\n",
            "Min/max predictions: 4.1944687836803496e-05 0.9320659041404724\n",
            "Loss for this batch: 0.2937\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.30719456 0.8769609  0.4140143  0.289495   0.00191753 0.36861786\n",
            " 0.57794434 0.01452997 0.23416282 0.57076144]\n",
            "Min/max predictions: 0.0001258464908460155 0.8769608736038208\n",
            "Loss for this batch: 0.3874\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.02723512 0.2652253  0.11610945 0.06304795 0.03156985 0.1323989\n",
            " 0.22792785 0.55069137 0.0013614  0.32222685]\n",
            "Min/max predictions: 2.0758923710673116e-05 0.9289861917495728\n",
            "Loss for this batch: 0.3177\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [1.8989146e-01 4.8253138e-02 1.3708343e-02 9.4275083e-03 2.6233543e-02\n",
            " 3.8125297e-01 3.0892399e-01 1.3100660e-03 7.2124285e-01 6.5336411e-05]\n",
            "Min/max predictions: 6.533641135320067e-05 0.9186815619468689\n",
            "Loss for this batch: 0.2669\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17954783 0.15289654 0.29545066 0.501779   0.46804816 0.0153613\n",
            " 0.09454767 0.00097158 0.01184522 0.3200216 ]\n",
            "Min/max predictions: 6.181773642310873e-05 0.9924547672271729\n",
            "Loss for this batch: 0.3335\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00975961 0.04116389 0.06533004 0.6452051  0.04953026 0.7163539\n",
            " 0.33212095 0.30788758 0.20212933 0.02845997]\n",
            "Min/max predictions: 0.0002723199431784451 0.9662187695503235\n",
            "Loss for this batch: 0.3175\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "Sample predictions: [0.0163379  0.01921985 0.16201748 0.0062921  0.7932134  0.3807908\n",
            " 0.1191553  0.8787044  0.62882376 0.5414508 ]\n",
            "Min/max predictions: 5.241886537987739e-05 0.967260479927063\n",
            "Loss for this batch: 0.3467\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.10558375 0.05875136 0.24743131 0.00260702 0.7284994  0.01422835\n",
            " 0.09715454 0.08240109 0.758331   0.0067693 ]\n",
            "Min/max predictions: 3.730480602825992e-05 0.9014502167701721\n",
            "Loss for this batch: 0.3353\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3482139  0.21100113 0.33610985 0.00647141 0.10406466 0.09846368\n",
            " 0.14619392 0.34804708 0.04699893 0.28942075]\n",
            "Min/max predictions: 0.0002689745160751045 0.8951838612556458\n",
            "Loss for this batch: 0.3531\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.07494893 0.00418597 0.00102431 0.16561495 0.35474554 0.34845597\n",
            " 0.00209907 0.8137272  0.00097014 0.00712937]\n",
            "Min/max predictions: 0.00015506411727983505 0.9934648871421814\n",
            "Loss for this batch: 0.3790\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.05007927 0.0039571  0.06804358 0.16062048 0.8321507  0.02489264\n",
            " 0.17257029 0.00898775 0.64352983 0.08825418]\n",
            "Min/max predictions: 0.00010579181252978742 0.9552403092384338\n",
            "Loss for this batch: 0.2931\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.00433297 0.1470713  0.20543233 0.05597468 0.6277792  0.3683324\n",
            " 0.24447112 0.07331156 0.312265   0.09756768]\n",
            "Min/max predictions: 0.0001030153434840031 0.8948323726654053\n",
            "Loss for this batch: 0.4101\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00209779 0.00222398 0.01058598 0.0799206  0.00637592 0.00365859\n",
            " 0.12599017 0.10121623 0.08239844 0.75070894]\n",
            "Min/max predictions: 0.00010617868974804878 0.8632327914237976\n",
            "Loss for this batch: 0.3358\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.25523046 0.0894419  0.07857294 0.54679734 0.00827028 0.5363847\n",
            " 0.08582462 0.16068037 0.04841118 0.35729322]\n",
            "Min/max predictions: 3.4227894502691925e-05 0.9458839893341064\n",
            "Loss for this batch: 0.3109\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17825925 0.01584356 0.00775699 0.27275443 0.00374848 0.5713646\n",
            " 0.3980356  0.01731862 0.00093428 0.01223437]\n",
            "Min/max predictions: 0.00016564535326324403 0.8737337589263916\n",
            "Loss for this batch: 0.3170\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00819416 0.48288196 0.05581177 0.15094987 0.00590953 0.02275409\n",
            " 0.6254818  0.00494132 0.15857157 0.03670667]\n",
            "Min/max predictions: 1.940509355335962e-05 0.9590849876403809\n",
            "Loss for this batch: 0.3248\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [3.6010987e-01 2.2616587e-03 8.0381989e-01 5.5906218e-01 2.8385397e-02\n",
            " 5.8714598e-01 1.2808515e-01 5.4639619e-02 5.6472830e-03 1.6913685e-04]\n",
            "Min/max predictions: 7.253303920151666e-05 0.9184713959693909\n",
            "Loss for this batch: 0.3151\n",
            "Sample labels: [0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.11449099 0.71878695 0.49274516 0.00078042 0.00423767 0.02528317\n",
            " 0.02122621 0.09513882 0.12539491 0.02257674]\n",
            "Min/max predictions: 7.00970776961185e-05 0.8161948919296265\n",
            "Loss for this batch: 0.3099\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.0995258e-01 2.0733259e-04 5.8943508e-03 2.1721026e-02 1.7496331e-02\n",
            " 1.1274019e-01 1.1271576e-01 1.8226835e-01 1.4139869e-03 1.5715057e-03]\n",
            "Min/max predictions: 2.060335100395605e-05 0.8792927861213684\n",
            "Loss for this batch: 0.3261\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0100358  0.33823165 0.15922284 0.16546065 0.06386559 0.84737027\n",
            " 0.18525377 0.0057041  0.01053833 0.3773974 ]\n",
            "Min/max predictions: 0.00019242499547544867 0.8473702669143677\n",
            "Loss for this batch: 0.3238\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00580062 0.4652538  0.08490147 0.40603763 0.6383587  0.24109724\n",
            " 0.16021988 0.35115054 0.2321757  0.09033486]\n",
            "Min/max predictions: 1.958550637937151e-05 0.9236187934875488\n",
            "Loss for this batch: 0.3608\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00841022 0.38943148 0.08042027 0.10891227 0.04500468 0.27859595\n",
            " 0.02905608 0.13899297 0.40961903 0.02660271]\n",
            "Min/max predictions: 7.698582339799032e-05 0.9383680820465088\n",
            "Loss for this batch: 0.3178\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.03579419 0.03851008 0.12897472 0.3710246  0.47703147 0.11461297\n",
            " 0.33703253 0.09659383 0.32297948 0.08757451]\n",
            "Min/max predictions: 0.00015786619042046368 0.9082358479499817\n",
            "Loss for this batch: 0.3015\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.23162311 0.11025444 0.06295965 0.11775259 0.07044287 0.05478009\n",
            " 0.06249903 0.75494283 0.11908247 0.8779311 ]\n",
            "Min/max predictions: 1.5274410543497652e-05 0.8779311180114746\n",
            "Loss for this batch: 0.3313\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01364241 0.01319988 0.0119416  0.0401694  0.10048573 0.12768854\n",
            " 0.27008817 0.00184294 0.00488938 0.00371727]\n",
            "Min/max predictions: 0.0001484062522649765 0.9174858331680298\n",
            "Loss for this batch: 0.3544\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.2234583  0.7199819  0.00153245 0.0724024  0.5851385  0.12577407\n",
            " 0.03064292 0.16492885 0.51724017 0.227211  ]\n",
            "Min/max predictions: 0.00021278161148075014 0.9173912405967712\n",
            "Loss for this batch: 0.3371\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.02425846 0.00092247 0.5571905  0.81822675 0.19269656 0.0134252\n",
            " 0.00126687 0.41185442 0.26763102 0.8705591 ]\n",
            "Min/max predictions: 8.176198753062636e-05 0.9675540924072266\n",
            "Loss for this batch: 0.3133\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.01998659 0.5208105  0.16314313 0.03729261 0.76409906 0.46617803\n",
            " 0.00647037 0.43418983 0.02204818 0.00544993]\n",
            "Min/max predictions: 0.00043747303425334394 0.8522825241088867\n",
            "Loss for this batch: 0.3648\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00385618 0.2594473  0.02662233 0.28948545 0.0570749  0.55318534\n",
            " 0.75236    0.06695236 0.35121697 0.16515726]\n",
            "Min/max predictions: 0.00010884469520533457 0.8992123603820801\n",
            "Loss for this batch: 0.3479\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.17248094 0.4358279  0.00217777 0.04078574 0.02278752 0.5177061\n",
            " 0.49110934 0.07513639 0.00679318 0.32888263]\n",
            "Min/max predictions: 9.934996342053637e-05 0.9120603799819946\n",
            "Loss for this batch: 0.3198\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01050256 0.06045916 0.06025968 0.02798198 0.1148286  0.19924666\n",
            " 0.0307916  0.11380179 0.6994187  0.18412024]\n",
            "Min/max predictions: 0.00010096777259605005 0.9675779938697815\n",
            "Loss for this batch: 0.3935\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.02272196 0.01396968 0.16582967 0.05751859 0.00990546 0.03311604\n",
            " 0.0024878  0.17823635 0.20809655 0.00343975]\n",
            "Min/max predictions: 3.928716978407465e-05 0.8693996667861938\n",
            "Loss for this batch: 0.3158\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.04562323 0.01563196 0.37555492 0.42594057 0.30266923 0.43877998\n",
            " 0.00733986 0.25330698 0.33517882 0.04041674]\n",
            "Min/max predictions: 5.001807221560739e-05 0.8942545056343079\n",
            "Loss for this batch: 0.3243\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [9.9114561e-03 1.0795867e-01 9.8122197e-01 8.3706953e-02 2.3667307e-01\n",
            " 1.2168808e-01 3.3406704e-04 3.9705443e-01 6.4889584e-03 1.0623558e-01]\n",
            "Min/max predictions: 5.1726445235544816e-05 0.9812219738960266\n",
            "Loss for this batch: 0.3375\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17583677 0.01082851 0.25131387 0.2477402  0.015545   0.07189532\n",
            " 0.09765872 0.6952342  0.17206377 0.4439887 ]\n",
            "Min/max predictions: 0.0003273585462011397 0.9843127727508545\n",
            "Loss for this batch: 0.3269\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.00736088 0.34741262 0.01331814 0.12814675 0.00858332 0.00414032\n",
            " 0.06150201 0.5184601  0.26661196 0.536223  ]\n",
            "Min/max predictions: 9.762041008798406e-05 0.9108325839042664\n",
            "Loss for this batch: 0.3971\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14453502 0.01080153 0.26934546 0.12044352 0.28937495 0.08060482\n",
            " 0.187191   0.6174353  0.05545758 0.14926432]\n",
            "Min/max predictions: 0.0003931375977117568 0.9486445784568787\n",
            "Loss for this batch: 0.3108\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.65890014 0.29481435 0.0059345  0.12042822 0.24335128 0.2545849\n",
            " 0.01843708 0.16506335 0.26197222 0.00778423]\n",
            "Min/max predictions: 3.343101343489252e-05 0.8578807711601257\n",
            "Loss for this batch: 0.3143\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.01908872 0.03445372 0.00181394 0.00615992 0.19144228 0.09386346\n",
            " 0.7327122  0.00355306 0.4098666  0.36574882]\n",
            "Min/max predictions: 4.332635216997005e-05 0.8602641820907593\n",
            "Loss for this batch: 0.3327\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.25220606 0.06490267 0.3017258  0.08739708 0.34438217 0.22220157\n",
            " 0.01035414 0.49391437 0.11123817 0.30038112]\n",
            "Min/max predictions: 0.00013985954865347594 0.9764748215675354\n",
            "Loss for this batch: 0.3822\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.16121286 0.00973805 0.49902523 0.572728   0.15371239 0.00389166\n",
            " 0.05756806 0.00102071 0.23272572 0.03441503]\n",
            "Min/max predictions: 0.0002686856605578214 0.9367163777351379\n",
            "Loss for this batch: 0.3433\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01561165 0.38870615 0.25011304 0.0588071  0.00477302 0.03880286\n",
            " 0.22768103 0.07775947 0.561769   0.0695024 ]\n",
            "Min/max predictions: 0.00025570980506017804 0.892754316329956\n",
            "Loss for this batch: 0.4046\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.02288327 0.2567921  0.0505059  0.05041414 0.25681704 0.18971965\n",
            " 0.21118625 0.34621438 0.17240934 0.3244675 ]\n",
            "Min/max predictions: 0.00018119992455467582 0.8968911170959473\n",
            "Loss for this batch: 0.3424\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.25663313 0.00467429 0.34035912 0.64485294 0.5105335  0.06314163\n",
            " 0.14603968 0.11654361 0.02415456 0.02886652]\n",
            "Min/max predictions: 9.094442066270858e-05 0.9429008364677429\n",
            "Loss for this batch: 0.3275\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.890075   0.0011563  0.4556474  0.01022168 0.77760917 0.00354438\n",
            " 0.37410197 0.42813691 0.13538408 0.06293412]\n",
            "Min/max predictions: 0.0001371328253298998 0.8900750279426575\n",
            "Loss for this batch: 0.3704\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [1.9494881e-01 5.0082654e-01 3.2490978e-01 2.0468750e-03 1.3644990e-01\n",
            " 4.5837662e-03 5.5900681e-01 4.0591195e-01 1.0917399e-02 5.3174706e-04]\n",
            "Min/max predictions: 0.00018185227236244828 0.9143819808959961\n",
            "Loss for this batch: 0.3635\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0017483  0.09700956 0.18601255 0.01205387 0.00236463 0.22710401\n",
            " 0.04111224 0.09636288 0.25964198 0.5415163 ]\n",
            "Min/max predictions: 0.0003345517907291651 0.8245329856872559\n",
            "Loss for this batch: 0.3704\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.23602074 0.05506713 0.11292382 0.03908024 0.25800905 0.00188282\n",
            " 0.07066759 0.1170664  0.42613953 0.08398647]\n",
            "Min/max predictions: 4.539899236988276e-05 0.863680899143219\n",
            "Loss for this batch: 0.3227\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3842014  0.5198279  0.00355454 0.01287168 0.14076954 0.40379292\n",
            " 0.05139288 0.1589223  0.37511364 0.05097326]\n",
            "Min/max predictions: 3.302022014395334e-05 0.9511688351631165\n",
            "Loss for this batch: 0.3795\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15015435 0.19718201 0.00936711 0.12176955 0.02739379 0.7216055\n",
            " 0.01233461 0.01003793 0.11693667 0.15579256]\n",
            "Min/max predictions: 2.2711585188517347e-05 0.9222850799560547\n",
            "Loss for this batch: 0.3362\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.3582012  0.6884855  0.3169833  0.04147142 0.13673298 0.01286365\n",
            " 0.004754   0.12902264 0.02890308 0.70991045]\n",
            "Min/max predictions: 8.718624303583056e-05 0.879091739654541\n",
            "Loss for this batch: 0.2989\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.34632742 0.38642907 0.5890748  0.35966063 0.24536358 0.40695092\n",
            " 0.24491628 0.0086096  0.55099666 0.37098125]\n",
            "Min/max predictions: 8.44655223772861e-05 0.8847951292991638\n",
            "Loss for this batch: 0.3296\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00912404 0.15937026 0.00665671 0.07617077 0.01850916 0.05719837\n",
            " 0.01146761 0.28753102 0.03138737 0.09662785]\n",
            "Min/max predictions: 0.0002739281626418233 0.8440093994140625\n",
            "Loss for this batch: 0.3042\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [4.60740700e-02 2.41605595e-01 3.10663227e-02 1.64633263e-02\n",
            " 6.09707236e-01 5.52338082e-04 1.47362441e-01 1.41308075e-02\n",
            " 4.52901304e-01 7.57780746e-02]\n",
            "Min/max predictions: 0.00017903957632370293 0.8652822971343994\n",
            "Loss for this batch: 0.3652\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.25869614 0.5245311  0.18642356 0.16976556 0.00053865 0.13176884\n",
            " 0.16051798 0.00881145 0.23309375 0.3581041 ]\n",
            "Min/max predictions: 4.362993422546424e-05 0.8890344500541687\n",
            "Loss for this batch: 0.3421\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.33596775 0.12833548 0.56755406 0.10186317 0.28559926 0.05622968\n",
            " 0.1048139  0.07455292 0.04596285 0.47353765]\n",
            "Min/max predictions: 1.878494913398754e-05 0.9084524512290955\n",
            "Loss for this batch: 0.3202\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [6.55866563e-01 1.18395455e-01 1.68265209e-01 3.19816500e-01\n",
            " 5.66200993e-04 3.62130478e-02 2.57921845e-01 2.10419163e-01\n",
            " 2.61521491e-04 5.67426622e-01]\n",
            "Min/max predictions: 0.00021797149383928627 0.9352700114250183\n",
            "Loss for this batch: 0.3727\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.079343   0.07193412 0.8055424  0.26596534 0.05875136 0.0154241\n",
            " 0.03590011 0.00162995 0.07092109 0.90153944]\n",
            "Min/max predictions: 6.887437484692782e-05 0.9015394449234009\n",
            "Loss for this batch: 0.3537\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.09214078 0.09897601 0.04301145 0.16749075 0.17084198 0.10635541\n",
            " 0.00585515 0.57462126 0.36916342 0.00888343]\n",
            "Min/max predictions: 0.00015339614765252918 0.9849430322647095\n",
            "Loss for this batch: 0.3069\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.44987997 0.37456682 0.00131225 0.39198157 0.00852691 0.00193397\n",
            " 0.02329533 0.31171763 0.00079792 0.30479592]\n",
            "Min/max predictions: 0.00014879590889904648 0.8558515906333923\n",
            "Loss for this batch: 0.3873\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.66816485 0.26025182 0.00245955 0.27857935 0.524858   0.00193363\n",
            " 0.24870096 0.05233034 0.76940084 0.0136783 ]\n",
            "Min/max predictions: 0.000178243251866661 0.8542280197143555\n",
            "Loss for this batch: 0.3003\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.78074014 0.02760896 0.04448383 0.11828806 0.03511292 0.34803873\n",
            " 0.01109752 0.10024414 0.47762874 0.11948437]\n",
            "Min/max predictions: 0.00026083618286065757 0.8787418007850647\n",
            "Loss for this batch: 0.3274\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.04801161 0.41704446 0.0327474  0.1305075  0.56762576 0.03956583\n",
            " 0.02401412 0.17495787 0.22638398 0.276653  ]\n",
            "Min/max predictions: 0.00012679724022746086 0.8642252683639526\n",
            "Loss for this batch: 0.2906\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.5067327  0.0705215  0.06109836 0.14012061 0.01141525 0.45297316\n",
            " 0.41217145 0.00661882 0.27010182 0.19390418]\n",
            "Min/max predictions: 0.00018299960356671363 0.8213245272636414\n",
            "Loss for this batch: 0.3802\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.22071725 0.00713154 0.31310275 0.23788314 0.00128094 0.00931334\n",
            " 0.6012413  0.00849255 0.00109138 0.84781337]\n",
            "Min/max predictions: 5.757017424912192e-05 0.9848554730415344\n",
            "Loss for this batch: 0.3556\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [2.0763634e-01 3.6422420e-01 3.3343334e-02 3.9346093e-01 4.7299799e-01\n",
            " 9.8459207e-02 2.9028404e-01 3.0305370e-04 4.2658594e-01 2.5679109e-01]\n",
            "Min/max predictions: 2.4178305466193706e-05 0.9010174870491028\n",
            "Loss for this batch: 0.3528\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.7982326e-01 4.4993348e-02 6.5475833e-01 2.7383283e-01 4.8252821e-01\n",
            " 1.5140294e-01 5.2590703e-04 3.4222573e-01 1.0140105e-01 1.7046089e-03]\n",
            "Min/max predictions: 0.0001808062515920028 0.9501008987426758\n",
            "Loss for this batch: 0.3422\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.4724946  0.2312034  0.09403287 0.04291686 0.00791681 0.3034757\n",
            " 0.0828023  0.01652412 0.19662514 0.42723155]\n",
            "Min/max predictions: 7.056581671349704e-05 0.9128842353820801\n",
            "Loss for this batch: 0.3204\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05026228 0.07542954 0.20176303 0.0016505  0.29107848 0.21304765\n",
            " 0.00639971 0.28683582 0.02481543 0.04617497]\n",
            "Min/max predictions: 0.00016264163423329592 0.9536381363868713\n",
            "Loss for this batch: 0.3347\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00388592 0.19953412 0.11300457 0.03751466 0.00213355 0.02367609\n",
            " 0.00023031 0.06692572 0.2279019  0.10852136]\n",
            "Min/max predictions: 0.00023031119781080633 0.9452272057533264\n",
            "Loss for this batch: 0.3534\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.5828193  0.00494909 0.00273581 0.06742851 0.04195483 0.6893544\n",
            " 0.6015427  0.03102035 0.11302131 0.35696006]\n",
            "Min/max predictions: 3.4277541999472305e-05 0.9635503888130188\n",
            "Loss for this batch: 0.2880\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.13715053 0.16084549 0.01190828 0.00617903 0.5541425  0.41912973\n",
            " 0.00077218 0.44943172 0.06164102 0.00817743]\n",
            "Min/max predictions: 3.201076833647676e-05 0.9656795263290405\n",
            "Loss for this batch: 0.3249\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [8.3511800e-02 7.7185142e-01 1.6201098e-01 6.3693681e-04 3.0326704e-02\n",
            " 8.3477646e-02 7.0368642e-01 2.9423054e-02 7.0231670e-01 2.0894501e-02]\n",
            "Min/max predictions: 0.00019916945893783122 0.9880796074867249\n",
            "Loss for this batch: 0.3294\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7343105  0.8807968  0.08581316 0.00210641 0.19402197 0.0242954\n",
            " 0.01969399 0.03703055 0.08425155 0.00678992]\n",
            "Min/max predictions: 0.0002163696481147781 0.9407013058662415\n",
            "Loss for this batch: 0.2935\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01890789 0.15506862 0.01844916 0.27874368 0.33702517 0.43334448\n",
            " 0.0525962  0.22762746 0.6368653  0.04124122]\n",
            "Min/max predictions: 0.00013116243644617498 0.9150853753089905\n",
            "Loss for this batch: 0.3852\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05922025 0.01055374 0.64705545 0.78958994 0.00216502 0.01954744\n",
            " 0.08532893 0.06460626 0.0150882  0.02102911]\n",
            "Min/max predictions: 5.263229468255304e-05 0.9491636753082275\n",
            "Loss for this batch: 0.2738\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00524599 0.18286583 0.2876491  0.03522056 0.23673959 0.04566388\n",
            " 0.5505327  0.00751359 0.16981167 0.16802481]\n",
            "Min/max predictions: 0.00019755688845179975 0.9498910903930664\n",
            "Loss for this batch: 0.3778\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.23508333 0.00338222 0.20540003 0.00717397 0.00503082 0.3116913\n",
            " 0.27376494 0.03960051 0.03416602 0.41300756]\n",
            "Min/max predictions: 0.0001844115904532373 0.9329437017440796\n",
            "Loss for this batch: 0.3424\n",
            "Sample labels: [0. 1. 1. 1. 1. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [3.6043460e-03 3.0184457e-01 5.2703446e-01 5.3020668e-01 8.4596145e-01\n",
            " 4.6129555e-01 3.2785039e-02 3.7496141e-01 5.4783893e-01 2.8393371e-04]\n",
            "Min/max predictions: 8.018115477170795e-05 0.8944848775863647\n",
            "Loss for this batch: 0.3358\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04884724 0.00290577 0.5900111  0.0136649  0.34406248 0.00475081\n",
            " 0.11533537 0.05740711 0.52172935 0.14663173]\n",
            "Min/max predictions: 0.00020387514086905867 0.8874452114105225\n",
            "Loss for this batch: 0.3203\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.0482987  0.00313035 0.06043972 0.22590181 0.00376236 0.19322373\n",
            " 0.0265952  0.00333664 0.39169708 0.05029621]\n",
            "Min/max predictions: 0.00023938814410939813 0.9781827926635742\n",
            "Loss for this batch: 0.3292\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00242623 0.04266237 0.13818781 0.02668895 0.04274619 0.10474341\n",
            " 0.17300586 0.17445199 0.00046313 0.40590754]\n",
            "Min/max predictions: 0.00012493050599005073 0.9915157556533813\n",
            "Loss for this batch: 0.3354\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02199683 0.17266493 0.04236467 0.09407794 0.01536294 0.10150075\n",
            " 0.06830619 0.01803204 0.2886865  0.00379432]\n",
            "Min/max predictions: 0.00023107357264962047 0.966952383518219\n",
            "Loss for this batch: 0.3770\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.22609814 0.43556014 0.00995997 0.01815368 0.00395627 0.2575368\n",
            " 0.0099021  0.0815179  0.01102351 0.35904244]\n",
            "Min/max predictions: 0.00011269933020230383 0.9422064423561096\n",
            "Loss for this batch: 0.3403\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [4.4875283e-02 8.6571154e-04 8.8438195e-01 4.1690646e-03 4.4646716e-01\n",
            " 4.7632519e-02 2.1800084e-01 2.0945529e-03 1.0247771e-01 9.6529827e-02]\n",
            "Min/max predictions: 2.780174145300407e-05 0.9284424185752869\n",
            "Loss for this batch: 0.3232\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00372912 0.00248073 0.7089054  0.4219345  0.18767613 0.02124437\n",
            " 0.5750559  0.12333298 0.03170604 0.01637866]\n",
            "Min/max predictions: 2.874119127227459e-05 0.9653747081756592\n",
            "Loss for this batch: 0.3752\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.26274782 0.03333471 0.83567595 0.01859561 0.2963839  0.04424854\n",
            " 0.53359395 0.2974734  0.12346154 0.36519068]\n",
            "Min/max predictions: 0.00039563796599395573 0.9069872498512268\n",
            "Loss for this batch: 0.3721\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00456762 0.5563553  0.0754699  0.10733697 0.06277759 0.7446652\n",
            " 0.04056359 0.18201919 0.21317227 0.19813159]\n",
            "Min/max predictions: 0.0003246877167839557 0.8749858140945435\n",
            "Loss for this batch: 0.2775\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.5251916e-03 1.0256986e-01 4.2051095e-01 1.0934691e-01 1.7434324e-04\n",
            " 2.4996111e-01 5.6737423e-02 7.6744631e-02 2.3752034e-03 2.1875852e-04]\n",
            "Min/max predictions: 0.00013850664254277945 0.8330242037773132\n",
            "Loss for this batch: 0.3543\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.74118173 0.22786228 0.00685519 0.00616442 0.30343375 0.0724275\n",
            " 0.81077826 0.54647315 0.18220802 0.00271983]\n",
            "Min/max predictions: 0.00018777020159177482 0.9943830966949463\n",
            "Loss for this batch: 0.3456\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06460966 0.12434553 0.00727355 0.0476139  0.02671756 0.00039959\n",
            " 0.0036332  0.01756614 0.3714929  0.04652607]\n",
            "Min/max predictions: 0.0001263595768250525 0.869672417640686\n",
            "Loss for this batch: 0.4009\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00237935 0.17676368 0.03037579 0.02356132 0.3591854  0.00574122\n",
            " 0.48857033 0.00172042 0.00056193 0.11589743]\n",
            "Min/max predictions: 0.0004580726963467896 0.9821125268936157\n",
            "Loss for this batch: 0.3079\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.02578014 0.13453166 0.17312635 0.39404497 0.22757186 0.00238278\n",
            " 0.48188832 0.16893034 0.32823825 0.01684088]\n",
            "Min/max predictions: 0.00028147263219580054 0.8937420845031738\n",
            "Loss for this batch: 0.3057\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.01863203 0.15443255 0.01883504 0.12784514 0.00069517 0.3284885\n",
            " 0.30906516 0.08790432 0.01762796 0.68660617]\n",
            "Min/max predictions: 3.7034566048532724e-05 0.8867548704147339\n",
            "Loss for this batch: 0.3069\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.5335783  0.11767146 0.31664565 0.21301866 0.19446267 0.00078134\n",
            " 0.4994637  0.00142978 0.00106552 0.3328261 ]\n",
            "Min/max predictions: 8.494045323459432e-05 0.9499973654747009\n",
            "Loss for this batch: 0.3216\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [4.3742605e-02 7.8355454e-02 1.2880062e-01 9.3232729e-03 2.3292855e-04\n",
            " 5.3266203e-01 2.3929378e-01 1.4615425e-01 3.2527581e-01 3.7553791e-02]\n",
            "Min/max predictions: 7.480947533622384e-05 0.9327527284622192\n",
            "Loss for this batch: 0.3432\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.8911134  0.24985203 0.27761865 0.0381396  0.21978675 0.4326441\n",
            " 0.18801062 0.02245368 0.00662151 0.23501049]\n",
            "Min/max predictions: 0.00010510304127819836 0.8911134004592896\n",
            "Loss for this batch: 0.2889\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.6498045  0.7267482  0.46457183 0.57876325 0.00136081 0.13136367\n",
            " 0.02139347 0.00470421 0.1421647  0.5997875 ]\n",
            "Min/max predictions: 5.476666410686448e-05 0.8835965991020203\n",
            "Loss for this batch: 0.3736\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.11052576 0.00678133 0.33428138 0.16950107 0.17017755 0.01310176\n",
            " 0.54131925 0.01491666 0.2641244  0.05236517]\n",
            "Min/max predictions: 0.0001060420909198001 0.8735136985778809\n",
            "Loss for this batch: 0.4050\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4583556  0.5337769  0.30308613 0.23317067 0.04104399 0.33085343\n",
            " 0.00449471 0.35007432 0.46263114 0.23759264]\n",
            "Min/max predictions: 6.604174996027723e-05 0.9839529395103455\n",
            "Loss for this batch: 0.3892\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00064793 0.51690984 0.0073517  0.01549743 0.08775634 0.00791241\n",
            " 0.29079115 0.00399335 0.01449503 0.11973072]\n",
            "Min/max predictions: 0.00032002691295929253 0.9481013417243958\n",
            "Loss for this batch: 0.3537\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01636572 0.00127453 0.32423788 0.14464666 0.20073521 0.01090801\n",
            " 0.17217883 0.09915435 0.02598699 0.02669255]\n",
            "Min/max predictions: 0.0003259658988099545 0.8507673740386963\n",
            "Loss for this batch: 0.3248\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03608448 0.00175031 0.35394707 0.32271963 0.5032021  0.27693626\n",
            " 0.05872671 0.8283715  0.06072874 0.47051698]\n",
            "Min/max predictions: 1.5843153960304335e-05 0.9044808745384216\n",
            "Loss for this batch: 0.3321\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.16358976 0.17274365 0.16394873 0.03443602 0.6697186  0.02624946\n",
            " 0.00390267 0.19608782 0.47062135 0.00267451]\n",
            "Min/max predictions: 9.824085282161832e-05 0.8942110538482666\n",
            "Loss for this batch: 0.3224\n",
            "Sample labels: [0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.1626773e-02 5.3183705e-01 4.9003744e-01 7.2733152e-01 2.5914942e-03\n",
            " 5.8879417e-01 2.7244554e-03 1.9560514e-03 2.9066136e-01 6.5992551e-04]\n",
            "Min/max predictions: 0.00011456473293947056 0.9270881414413452\n",
            "Loss for this batch: 0.3588\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.105603   0.38465434 0.00101855 0.01492049 0.34202993 0.00292083\n",
            " 0.3534914  0.14893843 0.04569565 0.7293837 ]\n",
            "Min/max predictions: 0.00011604364408412948 0.8544834852218628\n",
            "Loss for this batch: 0.2998\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.17064469 0.47550133 0.03023032 0.39584905 0.57309735 0.20792328\n",
            " 0.0215278  0.252157   0.51521367 0.67478997]\n",
            "Min/max predictions: 0.00014869422011543065 0.9104185104370117\n",
            "Loss for this batch: 0.3290\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.04392043 0.45353085 0.02950392 0.36306858 0.21880312 0.18304124\n",
            " 0.66319185 0.29290232 0.22103171 0.51225156]\n",
            "Min/max predictions: 0.00015217471809592098 0.9437504410743713\n",
            "Loss for this batch: 0.3698\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.09267867 0.5858474  0.00390246 0.00205965 0.25771385 0.207429\n",
            " 0.01704915 0.07783846 0.3225124  0.00220045]\n",
            "Min/max predictions: 0.00010636819206411019 0.8993184566497803\n",
            "Loss for this batch: 0.2953\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.5371948  0.20953919 0.06143422 0.10724065 0.22491777 0.0124768\n",
            " 0.40060887 0.02523296 0.30710328 0.35453153]\n",
            "Min/max predictions: 0.00019043729116674513 0.8617626428604126\n",
            "Loss for this batch: 0.3777\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00190212 0.11473528 0.3733842  0.35931528 0.6278975  0.15912087\n",
            " 0.0009973  0.4090123  0.06007018 0.60724276]\n",
            "Min/max predictions: 2.1485173419932835e-05 0.8638306260108948\n",
            "Loss for this batch: 0.3386\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.61716634 0.05922658 0.00935585 0.22878695 0.05720296 0.35947636\n",
            " 0.00436874 0.02011391 0.1426616  0.04774434]\n",
            "Min/max predictions: 7.26389407645911e-05 0.8712788224220276\n",
            "Loss for this batch: 0.3874\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.41593257 0.29567876 0.01474238 0.00893216 0.38934058 0.31009468\n",
            " 0.00111425 0.01031399 0.09011084 0.11702719]\n",
            "Min/max predictions: 0.0001318888389505446 0.9323477745056152\n",
            "Loss for this batch: 0.3087\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.3871613  0.0264981  0.00312508 0.08391876 0.11329123 0.02290425\n",
            " 0.05573048 0.75842106 0.0107688  0.00768296]\n",
            "Min/max predictions: 0.0001766016212059185 0.9064040184020996\n",
            "Loss for this batch: 0.3078\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3588528  0.10661515 0.31181934 0.60052437 0.17547026 0.12597293\n",
            " 0.04233767 0.01670763 0.09215331 0.463335  ]\n",
            "Min/max predictions: 0.00014780984201934189 0.8828564286231995\n",
            "Loss for this batch: 0.3430\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.18704523 0.02226275 0.29671627 0.90003115 0.57046074 0.54206043\n",
            " 0.01348926 0.43509242 0.19199644 0.01820785]\n",
            "Min/max predictions: 0.0001856116869021207 0.9156417846679688\n",
            "Loss for this batch: 0.3037\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00201645 0.00410184 0.00415148 0.14443229 0.14258537 0.09198824\n",
            " 0.00966103 0.00126709 0.02802682 0.25605917]\n",
            "Min/max predictions: 7.961513620102778e-05 0.8385350108146667\n",
            "Loss for this batch: 0.3085\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00827925 0.09887993 0.5722429  0.0805818  0.18385431 0.40873352\n",
            " 0.48685685 0.03487211 0.4398704  0.23237391]\n",
            "Min/max predictions: 3.288534207968041e-05 0.9425804615020752\n",
            "Loss for this batch: 0.3103\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
            "Sample predictions: [1.4919621e-01 2.0843545e-04 2.2226325e-01 8.2337186e-02 6.3591057e-01\n",
            " 1.3964398e-01 3.8912871e-03 3.7659651e-01 9.3064584e-02 4.1591609e-01]\n",
            "Min/max predictions: 0.0001223206490976736 0.8671293258666992\n",
            "Loss for this batch: 0.3042\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00116671 0.00695726 0.04785188 0.6678569  0.03003603 0.24069023\n",
            " 0.07948117 0.01065899 0.00637817 0.2155643 ]\n",
            "Min/max predictions: 0.00020815222524106503 0.8788268566131592\n",
            "Loss for this batch: 0.3464\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01279627 0.15133305 0.00781584 0.07161061 0.22257416 0.20222515\n",
            " 0.01171406 0.02719236 0.18718532 0.77438515]\n",
            "Min/max predictions: 1.3540223335439805e-05 0.7812336683273315\n",
            "Loss for this batch: 0.3200\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03015228 0.04486583 0.57372624 0.545839   0.00454834 0.1421742\n",
            " 0.11466037 0.04238017 0.03650877 0.00616011]\n",
            "Min/max predictions: 1.81452305696439e-05 0.9439385533332825\n",
            "Loss for this batch: 0.3421\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [3.5639614e-04 4.8574182e-01 3.0430496e-01 2.2509969e-03 2.3206242e-03\n",
            " 7.5969152e-02 6.1052644e-01 1.5689041e-02 1.2703723e-02 3.8160852e-01]\n",
            "Min/max predictions: 1.2293566214793827e-05 0.875497579574585\n",
            "Loss for this batch: 0.3903\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01243358 0.22968988 0.00926723 0.02388392 0.07508676 0.9116787\n",
            " 0.0522575  0.08062341 0.10958337 0.00935886]\n",
            "Min/max predictions: 8.903685375116765e-05 0.9571839570999146\n",
            "Loss for this batch: 0.3436\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07863706 0.08896717 0.00249653 0.5204345  0.03034193 0.70871985\n",
            " 0.01182226 0.13471533 0.05782418 0.08367041]\n",
            "Min/max predictions: 0.00012291532766539603 0.8841714262962341\n",
            "Loss for this batch: 0.3253\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00635725 0.01082115 0.00065959 0.64871484 0.11837751 0.03481291\n",
            " 0.00164351 0.62813926 0.03998963 0.25251767]\n",
            "Min/max predictions: 0.00018531848036218435 0.9364328980445862\n",
            "Loss for this batch: 0.3671\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [8.6802607e-03 1.6296806e-02 8.0420781e-04 2.2993797e-04 3.3032992e-01\n",
            " 3.3793159e-02 4.9225685e-01 4.8550968e-03 4.3180890e-02 6.7589246e-02]\n",
            "Min/max predictions: 0.00022993797028902918 0.8888622522354126\n",
            "Loss for this batch: 0.3403\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7366737  0.00088475 0.1174918  0.52880734 0.5549979  0.02177978\n",
            " 0.01090926 0.02483486 0.00465947 0.00330968]\n",
            "Min/max predictions: 0.00011734847066691145 0.9847805500030518\n",
            "Loss for this batch: 0.3290\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02837401 0.7714826  0.0197705  0.00077597 0.3257151  0.4264554\n",
            " 0.01965072 0.01459872 0.19636863 0.3984681 ]\n",
            "Min/max predictions: 1.5274410543497652e-05 0.8765774965286255\n",
            "Loss for this batch: 0.2853\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.17422898 0.00070721 0.0007065  0.03169901 0.20710824 0.04911831\n",
            " 0.08525049 0.01287722 0.03933351 0.01076377]\n",
            "Min/max predictions: 0.00010615459177643061 0.875909149646759\n",
            "Loss for this batch: 0.3250\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.21828471 0.07388395 0.05112367 0.08568619 0.06018676 0.24507663\n",
            " 0.37472478 0.32836783 0.00500209 0.18156132]\n",
            "Min/max predictions: 7.613246270921081e-05 0.9390040040016174\n",
            "Loss for this batch: 0.3742\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00347249 0.09645622 0.00146045 0.02304596 0.09264753 0.09356673\n",
            " 0.00280687 0.00238745 0.19972935 0.02007729]\n",
            "Min/max predictions: 0.0002953704970423132 0.8339395523071289\n",
            "Loss for this batch: 0.3338\n",
            "Sample labels: [1. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.31610087 0.23954308 0.05507008 0.42435572 0.6137465  0.77157724\n",
            " 0.07105032 0.01405357 0.28119346 0.00784894]\n",
            "Min/max predictions: 0.0002157765266019851 0.9690725803375244\n",
            "Loss for this batch: 0.3669\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.09124833 0.01636827 0.22705704 0.06680717 0.07554612 0.01110374\n",
            " 0.0993632  0.02270859 0.56840754 0.09790029]\n",
            "Min/max predictions: 0.00010436020966153592 0.9195207953453064\n",
            "Loss for this batch: 0.2750\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.3029284  0.02002509 0.01048503 0.27684432 0.04177914 0.05568086\n",
            " 0.26118273 0.1938555  0.19286577 0.41826195]\n",
            "Min/max predictions: 2.6237314159516245e-05 0.9233794808387756\n",
            "Loss for this batch: 0.3563\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15648392 0.0026473  0.00213642 0.02140512 0.1733872  0.5421729\n",
            " 0.09194081 0.09587766 0.31643724 0.04941422]\n",
            "Min/max predictions: 9.813337783270981e-06 0.9668813943862915\n",
            "Loss for this batch: 0.3622\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.08147222 0.5346181  0.03020733 0.31914687 0.39207017 0.00309311\n",
            " 0.0554168  0.92845833 0.07062484 0.19472295]\n",
            "Min/max predictions: 6.637657497776672e-05 0.9284583330154419\n",
            "Loss for this batch: 0.3372\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.5780572  0.77193135 0.23116739 0.40740484 0.05376733 0.00992258\n",
            " 0.5950298  0.30993614 0.04591355 0.45971715]\n",
            "Min/max predictions: 6.431947986129671e-05 0.9790575504302979\n",
            "Loss for this batch: 0.3052\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.6257658  0.24242161 0.01489754 0.85230047 0.43789074 0.0228767\n",
            " 0.01002868 0.22038127 0.07050597 0.30126205]\n",
            "Min/max predictions: 0.00017995797679759562 0.9309402704238892\n",
            "Loss for this batch: 0.3465\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.23352577 0.01920933 0.01905239 0.00197693 0.05380059 0.08890203\n",
            " 0.13951705 0.61512876 0.06305024 0.13848077]\n",
            "Min/max predictions: 5.4216314310906455e-05 0.9538469314575195\n",
            "Loss for this batch: 0.3586\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [4.5346358e-04 1.0058581e-01 4.0933073e-02 2.8462279e-01 1.7240934e-01\n",
            " 4.3227872e-01 5.4696214e-04 7.2680050e-01 4.5718797e-02 3.7278280e-02]\n",
            "Min/max predictions: 8.081259875325486e-05 0.9047209620475769\n",
            "Loss for this batch: 0.2820\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.26968145 0.03578677 0.28256753 0.01816265 0.11413874 0.03386163\n",
            " 0.00368482 0.01028595 0.27235022 0.12049688]\n",
            "Min/max predictions: 0.0001656384119996801 0.7951230406761169\n",
            "Loss for this batch: 0.3020\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [9.74101797e-02 5.59415281e-01 4.37258035e-01 2.03624796e-02\n",
            " 2.44045779e-02 4.33118374e-04 3.22759360e-01 1.07723195e-02\n",
            " 2.41532102e-01 6.90205097e-02]\n",
            "Min/max predictions: 0.00023827279801480472 0.8679958581924438\n",
            "Loss for this batch: 0.3336\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07817783 0.36330703 0.66737205 0.36882666 0.03273594 0.5202993\n",
            " 0.21664888 0.00737811 0.06620138 0.11466701]\n",
            "Min/max predictions: 0.00017292865959461778 0.9002823829650879\n",
            "Loss for this batch: 0.3342\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.11209032 0.2199982  0.7587733  0.84383607 0.02538572 0.00664472\n",
            " 0.03962887 0.65318084 0.38641876 0.02498567]\n",
            "Min/max predictions: 9.815376688493416e-05 0.9254325032234192\n",
            "Loss for this batch: 0.3360\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03363315 0.00778724 0.21656224 0.10520077 0.31828836 0.0015423\n",
            " 0.19423378 0.34952533 0.49451306 0.00126925]\n",
            "Min/max predictions: 5.2975057769799605e-05 0.9550577998161316\n",
            "Loss for this batch: 0.3354\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.29535002 0.00244737 0.00064641 0.143093   0.34029302 0.00288265\n",
            " 0.01018942 0.01553902 0.44351035 0.01303733]\n",
            "Min/max predictions: 5.4252413974609226e-05 0.855491042137146\n",
            "Loss for this batch: 0.3341\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.18383798 0.11443111 0.62745285 0.26501328 0.6084789  0.04954725\n",
            " 0.7911323  0.00161788 0.0013912  0.21417978]\n",
            "Min/max predictions: 6.72396199661307e-05 0.9372623562812805\n",
            "Loss for this batch: 0.3492\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.4726739  0.02942049 0.00251423 0.01086004 0.17986128 0.01266013\n",
            " 0.7572103  0.00273785 0.4999398  0.04615978]\n",
            "Min/max predictions: 0.00014932279009371996 0.9319183230400085\n",
            "Loss for this batch: 0.3521\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.2518342  0.17102703 0.0804963  0.5351027  0.04118671 0.08308819\n",
            " 0.01545361 0.1345831  0.03174591 0.01807137]\n",
            "Min/max predictions: 0.0001242013240698725 0.8392507433891296\n",
            "Loss for this batch: 0.3329\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00098842 0.152395   0.27619022 0.00081031 0.02859067 0.38613853\n",
            " 0.5470311  0.01449724 0.01797061 0.06373479]\n",
            "Min/max predictions: 0.0002817397762555629 0.9367414712905884\n",
            "Loss for this batch: 0.3336\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [2.2324920e-01 1.5291412e-01 5.6331021e-01 1.3451697e-03 3.4199092e-01\n",
            " 5.2014291e-03 3.5127643e-01 3.7812151e-02 3.6581102e-04 3.9608803e-02]\n",
            "Min/max predictions: 0.00015618848556187004 0.9014527201652527\n",
            "Loss for this batch: 0.3049\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.09946823 0.23855689 0.00087889 0.21814474 0.00547893 0.23387837\n",
            " 0.00607427 0.0655342  0.2838882  0.2519246 ]\n",
            "Min/max predictions: 0.00017101802222896367 0.9535849690437317\n",
            "Loss for this batch: 0.3441\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.48779327 0.00840118 0.21498959 0.00493364 0.00749847 0.00326195\n",
            " 0.12474998 0.0092762  0.40470937 0.19504805]\n",
            "Min/max predictions: 0.0001452209980925545 0.8760727643966675\n",
            "Loss for this batch: 0.3411\n",
            "Sample labels: [0. 1. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.06103545 0.51100355 0.7781526  0.09393623 0.2656947  0.01642501\n",
            " 0.14569037 0.03209186 0.5221239  0.41409153]\n",
            "Min/max predictions: 0.00020173627126496285 0.8987693786621094\n",
            "Loss for this batch: 0.3248\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.22391048 0.01585524 0.04418604 0.00235321 0.59781134 0.16109554\n",
            " 0.0224011  0.00105978 0.7755595  0.14824674]\n",
            "Min/max predictions: 0.00010861669579753652 0.9760736227035522\n",
            "Loss for this batch: 0.3444\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.42218265 0.17036192 0.01054097 0.08502768 0.22710124 0.01106285\n",
            " 0.00188136 0.22129282 0.43343762 0.95375437]\n",
            "Min/max predictions: 0.00018175659351982176 0.9629538655281067\n",
            "Loss for this batch: 0.2571\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.16684735 0.0701812  0.4152953  0.13132682 0.022496   0.35930514\n",
            " 0.13495344 0.19795199 0.00426776 0.0035913 ]\n",
            "Min/max predictions: 0.00012641838111449033 0.9191356301307678\n",
            "Loss for this batch: 0.3661\n",
            "Sample labels: [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.24297701 0.00160762 0.27485743 0.38215938 0.76613724 0.3771532\n",
            " 0.585251   0.355082   0.03849085 0.01155829]\n",
            "Min/max predictions: 0.00037182969390414655 0.9797678589820862\n",
            "Loss for this batch: 0.3819\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.28953132 0.05367612 0.06498397 0.12240281 0.07735062 0.09285051\n",
            " 0.23426312 0.16849373 0.43902552 0.21479444]\n",
            "Min/max predictions: 0.0001103852700907737 0.9244714975357056\n",
            "Loss for this batch: 0.3249\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.13148248 0.49918938 0.8472915  0.75942016 0.0046824  0.08689533\n",
            " 0.21309394 0.30216637 0.16770487 0.41462883]\n",
            "Min/max predictions: 0.00017981715791393071 0.8472915291786194\n",
            "Loss for this batch: 0.3241\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00866325 0.47020006 0.00229344 0.74226654 0.1597452  0.15557759\n",
            " 0.44783625 0.00685186 0.047515   0.22834045]\n",
            "Min/max predictions: 0.0001843045320129022 0.9495152235031128\n",
            "Loss for this batch: 0.3138\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.8085782  0.3121816  0.3601933  0.01485396 0.11602162 0.3648458\n",
            " 0.01626642 0.162651   0.06219993 0.50776714]\n",
            "Min/max predictions: 0.0001269583881366998 0.943705141544342\n",
            "Loss for this batch: 0.3530\n",
            "Sample labels: [1. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.19147474 0.2525157  0.06006644 0.06977262 0.34442136 0.61482275\n",
            " 0.7862859  0.09366606 0.01154911 0.01401478]\n",
            "Min/max predictions: 4.486347461352125e-05 0.9085893034934998\n",
            "Loss for this batch: 0.3525\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.02796282 0.03548904 0.05910222 0.16534357 0.01966985 0.62133\n",
            " 0.2066867  0.74604464 0.28642613 0.2689292 ]\n",
            "Min/max predictions: 7.405928772641346e-05 0.913775622844696\n",
            "Loss for this batch: 0.3937\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.2442209e-01 1.8689115e-02 7.4095595e-01 1.5828837e-01 2.2001749e-01\n",
            " 6.6748548e-01 3.7989538e-02 1.7764226e-02 3.9154280e-04 1.3700925e-01]\n",
            "Min/max predictions: 0.00020534514624159783 0.8588773012161255\n",
            "Loss for this batch: 0.3708\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.3132492e-02 7.1991426e-01 3.9791372e-02 7.5256920e-01 2.4503484e-01\n",
            " 1.0640116e-04 5.3170826e-03 1.0968787e-02 1.3212922e-03 1.8879843e-03]\n",
            "Min/max predictions: 0.00010640115942806005 0.8261333107948303\n",
            "Loss for this batch: 0.3224\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00563048 0.04860456 0.04524177 0.02133634 0.11243495 0.1796036\n",
            " 0.00436192 0.11689454 0.07241795 0.00064516]\n",
            "Min/max predictions: 0.0002972054935526103 0.9308621883392334\n",
            "Loss for this batch: 0.2815\n",
            "Sample labels: [1. 0. 1. 0. 1. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.1436991  0.11459041 0.2317138  0.02439427 0.2658619  0.03156719\n",
            " 0.09378779 0.10623259 0.6205616  0.06605692]\n",
            "Min/max predictions: 0.0001231177884619683 0.9549881219863892\n",
            "Loss for this batch: 0.3672\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.00767915 0.06974375 0.00323393 0.10128019 0.058855   0.18553156\n",
            " 0.9006704  0.08805272 0.01530937 0.706269  ]\n",
            "Min/max predictions: 0.0001903712109196931 0.9812337756156921\n",
            "Loss for this batch: 0.2905\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.02310811 0.7960612  0.0091866  0.40335456 0.95023185 0.26589242\n",
            " 0.05037162 0.26012233 0.30513433 0.08018047]\n",
            "Min/max predictions: 0.00010003872739616781 0.9502318501472473\n",
            "Loss for this batch: 0.3311\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.04310032 0.0042006  0.03245505 0.18175796 0.6253246  0.01840141\n",
            " 0.47974554 0.1279206  0.11040435 0.5642579 ]\n",
            "Min/max predictions: 4.366873326944187e-05 0.9432558417320251\n",
            "Loss for this batch: 0.3679\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.10750354 0.38512167 0.02143131 0.07491897 0.00506197 0.03008419\n",
            " 0.12611964 0.62670225 0.6563965  0.45584938]\n",
            "Min/max predictions: 0.00020529658650048077 0.8954282999038696\n",
            "Loss for this batch: 0.3497\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [1.5888704e-02 4.7111416e-01 8.4616311e-02 4.1188851e-02 1.8097827e-02\n",
            " 1.6306400e-02 6.0618106e-02 4.2622879e-01 1.6579105e-04 5.8332241e-01]\n",
            "Min/max predictions: 2.7668329494190402e-05 0.9378086924552917\n",
            "Loss for this batch: 0.3321\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.4164195  0.14694229 0.00443651 0.25563678 0.04028223 0.37820235\n",
            " 0.01686996 0.43247095 0.01339837 0.42155638]\n",
            "Min/max predictions: 0.00016136521298903972 0.8170620203018188\n",
            "Loss for this batch: 0.3323\n",
            "Sample labels: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.73689306 0.44239122 0.21130513 0.00171255 0.00174318 0.05277693\n",
            " 0.00793718 0.00529856 0.51046014 0.6646831 ]\n",
            "Min/max predictions: 0.00011400078801671043 0.9148690104484558\n",
            "Loss for this batch: 0.3469\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.29751113 0.7845023  0.01794442 0.33214745 0.04031511 0.00962628\n",
            " 0.00834424 0.27997863 0.05467871 0.09501629]\n",
            "Min/max predictions: 0.00014479360834229738 0.9244065880775452\n",
            "Loss for this batch: 0.3134\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.11745135 0.0194212  0.41941676 0.02166308 0.0275373  0.06536139\n",
            " 0.4377885  0.5964368  0.18059562 0.00894112]\n",
            "Min/max predictions: 0.0005601888988167048 0.978325605392456\n",
            "Loss for this batch: 0.3313\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06979457 0.03362255 0.11748744 0.05011129 0.22493625 0.07642919\n",
            " 0.01767844 0.00881607 0.06831671 0.0532086 ]\n",
            "Min/max predictions: 0.00014728493988513947 0.9604235291481018\n",
            "Loss for this batch: 0.2959\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.05128143 0.07197709 0.32321504 0.09549523 0.1762934  0.26165912\n",
            " 0.352585   0.03701032 0.6810345  0.00281616]\n",
            "Min/max predictions: 1.3575158845924307e-05 0.8594838380813599\n",
            "Loss for this batch: 0.3128\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.01944206 0.11365133 0.30865303 0.03130398 0.07276671 0.03541754\n",
            " 0.0779349  0.54094017 0.12516238 0.85978115]\n",
            "Min/max predictions: 0.0002481484552845359 0.9621437191963196\n",
            "Loss for this batch: 0.3512\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.15585758 0.6218019  0.05155654 0.13653292 0.07373161 0.02095539\n",
            " 0.02175331 0.06409159 0.00184214 0.17204586]\n",
            "Min/max predictions: 0.0002931438502855599 0.8758686184883118\n",
            "Loss for this batch: 0.2981\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05240678 0.59097505 0.09920152 0.50125796 0.09911832 0.38265565\n",
            " 0.04825908 0.01781414 0.02880902 0.19440983]\n",
            "Min/max predictions: 0.0001726215850794688 0.8338504433631897\n",
            "Loss for this batch: 0.2830\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.1485075  0.01218088 0.07450116 0.1415268  0.0007448  0.06399307\n",
            " 0.07220452 0.00544413 0.04144577 0.00189289]\n",
            "Min/max predictions: 9.054913243744522e-05 0.9347705245018005\n",
            "Loss for this batch: 0.3037\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [5.1539240e-04 1.5108405e-02 8.2588345e-02 1.5055917e-01 4.6950045e-01\n",
            " 3.7430223e-02 5.5245978e-01 3.3443728e-01 3.6895189e-01 3.3052550e-03]\n",
            "Min/max predictions: 0.0001642897550482303 0.9009965062141418\n",
            "Loss for this batch: 0.3059\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00813575 0.02181154 0.28943107 0.03246785 0.00137955 0.02006813\n",
            " 0.51033634 0.25632054 0.00760123 0.4827487 ]\n",
            "Min/max predictions: 0.00013658692478202283 0.8641168475151062\n",
            "Loss for this batch: 0.3049\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.03411176 0.14437151 0.17186055 0.08694762 0.00282224 0.06222508\n",
            " 0.1764847  0.18996255 0.37247944 0.42717186]\n",
            "Min/max predictions: 0.0001050149803631939 0.889452338218689\n",
            "Loss for this batch: 0.3596\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01845217 0.00084434 0.23181993 0.0388673  0.5493204  0.13576138\n",
            " 0.00515884 0.4048718  0.01528154 0.0165928 ]\n",
            "Min/max predictions: 5.595878610620275e-05 0.8341988921165466\n",
            "Loss for this batch: 0.3523\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00576851 0.06392628 0.07488555 0.75108117 0.31509554 0.00523587\n",
            " 0.42041814 0.41399315 0.44534057 0.38589084]\n",
            "Min/max predictions: 0.00027193440473638475 0.789574146270752\n",
            "Loss for this batch: 0.3080\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.76025677 0.00446163 0.01773617 0.26386398 0.17374636 0.00924545\n",
            " 0.5420034  0.01412512 0.02867096 0.42595965]\n",
            "Min/max predictions: 3.545203435351141e-05 0.9651982188224792\n",
            "Loss for this batch: 0.3297\n",
            "Sample labels: [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.4839435e-03 2.1950552e-01 2.4784800e-01 5.2688074e-01 4.5660910e-01\n",
            " 6.3119435e-01 1.4748502e-05 2.9148671e-01 1.7991671e-03 7.0250514e-03]\n",
            "Min/max predictions: 1.4748501598660368e-05 0.8893095254898071\n",
            "Loss for this batch: 0.3991\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [2.4374539e-02 3.2310873e-01 6.9212800e-01 3.9859247e-01 4.1498382e-02\n",
            " 5.0655747e-04 3.9957467e-01 5.2106366e-02 1.0218235e-01 2.2392723e-01]\n",
            "Min/max predictions: 1.006776528811315e-05 0.905901312828064\n",
            "Loss for this batch: 0.2988\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [7.0475668e-01 1.9149718e-01 1.4459795e-03 2.5085354e-01 3.2597792e-01\n",
            " 2.1242421e-02 6.6407639e-01 9.5505910e-03 3.0120814e-02 1.6812402e-04]\n",
            "Min/max predictions: 0.00013969586871098727 0.9456881880760193\n",
            "Loss for this batch: 0.3428\n",
            "Sample labels: [0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [7.2800353e-02 4.4961223e-01 5.1733458e-01 6.4988250e-01 6.7826197e-03\n",
            " 2.8063142e-01 1.7218111e-01 2.0324451e-01 2.6819401e-04 1.8933424e-01]\n",
            "Min/max predictions: 0.0002544075541663915 0.9317613244056702\n",
            "Loss for this batch: 0.3569\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.02888665 0.63297015 0.01986367 0.10190386 0.00870046 0.03922959\n",
            " 0.3022115  0.18817699 0.11461974 0.09521094]\n",
            "Min/max predictions: 4.4917454943060875e-05 0.8387090563774109\n",
            "Loss for this batch: 0.2711\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00293842 0.00542628 0.07560299 0.05736887 0.00112697 0.04347609\n",
            " 0.00100519 0.0096426  0.7416026  0.21492481]\n",
            "Min/max predictions: 0.000579497660510242 0.9291455149650574\n",
            "Loss for this batch: 0.3519\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.10840277 0.10843958 0.00144027 0.05922448 0.00270999 0.37922505\n",
            " 0.2197913  0.07497934 0.05542649 0.5245814 ]\n",
            "Min/max predictions: 8.489082392770797e-05 0.8775162696838379\n",
            "Loss for this batch: 0.3329\n",
            "Sample labels: [0. 1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.28036237 0.558869   0.38502166 0.013059   0.6162476  0.6218363\n",
            " 0.28386775 0.09109493 0.22592652 0.3518476 ]\n",
            "Min/max predictions: 3.343101343489252e-05 0.9042074084281921\n",
            "Loss for this batch: 0.3262\n",
            "Sample labels: [0. 1. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [6.4209435e-04 1.1101742e-01 1.3731275e-01 5.7262576e-01 3.4444231e-01\n",
            " 8.0630833e-01 4.2007837e-01 1.3380006e-04 2.6675764e-02 1.8503195e-02]\n",
            "Min/max predictions: 0.00013380005839280784 0.9249862432479858\n",
            "Loss for this batch: 0.3298\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.3201142  0.00203742 0.00312136 0.38817498 0.6026116  0.00609931\n",
            " 0.49099556 0.09688451 0.00127243 0.22776824]\n",
            "Min/max predictions: 1.245375369762769e-05 0.8799195289611816\n",
            "Loss for this batch: 0.3179\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.09343746 0.80493563 0.01027151 0.6785361  0.04350701 0.6994632\n",
            " 0.01118362 0.19288678 0.01112154 0.00784909]\n",
            "Min/max predictions: 4.9343903810949996e-05 0.9332036972045898\n",
            "Loss for this batch: 0.2962\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00330583 0.01531116 0.0452048  0.78441226 0.04889013 0.11863974\n",
            " 0.6805204  0.07316401 0.2607255  0.1759417 ]\n",
            "Min/max predictions: 0.00026361303753219545 0.885951817035675\n",
            "Loss for this batch: 0.3421\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00468134 0.23899718 0.21123101 0.08186065 0.30730957 0.08485406\n",
            " 0.06498468 0.6897351  0.01109189 0.01827049]\n",
            "Min/max predictions: 0.00012866583711002022 0.9220055937767029\n",
            "Loss for this batch: 0.3329\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.52393067 0.60798573 0.00243857 0.32077754 0.01580756 0.01213332\n",
            " 0.00989347 0.18032268 0.32891005 0.03484271]\n",
            "Min/max predictions: 6.48821733193472e-05 0.947507917881012\n",
            "Loss for this batch: 0.3279\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.04215698 0.12732705 0.50469905 0.33408377 0.09223154 0.10410214\n",
            " 0.03187791 0.21013573 0.50451165 0.27439716]\n",
            "Min/max predictions: 8.124391024466604e-05 0.9073502421379089\n",
            "Loss for this batch: 0.3270\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01662382 0.00783338 0.12844144 0.7047201  0.06828367 0.00196752\n",
            " 0.02725141 0.01533292 0.80813783 0.00496509]\n",
            "Min/max predictions: 0.0001655801315791905 0.8198648691177368\n",
            "Loss for this batch: 0.3445\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.08014797 0.03158587 0.006136   0.04819513 0.2076743  0.00071345\n",
            " 0.25719118 0.41087118 0.06019019 0.5951733 ]\n",
            "Min/max predictions: 0.000160414376296103 0.9952660799026489\n",
            "Loss for this batch: 0.3603\n",
            "Sample labels: [0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14555189 0.87553465 0.4758988  0.15004696 0.724552   0.49163693\n",
            " 0.42281628 0.00255152 0.00182455 0.00681788]\n",
            "Min/max predictions: 4.549542427412234e-05 0.9562378525733948\n",
            "Loss for this batch: 0.3922\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4228879  0.15478712 0.24818018 0.17092405 0.05863681 0.5049986\n",
            " 0.33927613 0.09188407 0.25200883 0.00814719]\n",
            "Min/max predictions: 0.0001827631494961679 0.9296912550926208\n",
            "Loss for this batch: 0.3143\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04598652 0.06080988 0.12219069 0.3488386  0.0925618  0.27976483\n",
            " 0.00353939 0.22625303 0.00202424 0.02664647]\n",
            "Min/max predictions: 4.460053241928108e-05 0.9662277698516846\n",
            "Loss for this batch: 0.3379\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.07736151 0.61053085 0.10115019 0.7105795  0.01015331 0.01088546\n",
            " 0.02025184 0.04949213 0.00320792 0.19547655]\n",
            "Min/max predictions: 0.00021843377908226103 0.9372244477272034\n",
            "Loss for this batch: 0.3186\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.5629921  0.02329829 0.02350622 0.17006098 0.44402707 0.01851517\n",
            " 0.25205407 0.04889989 0.08592287 0.01615448]\n",
            "Min/max predictions: 3.2511619792785496e-05 0.8605273962020874\n",
            "Loss for this batch: 0.3103\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.0032669  0.5336557  0.00192441 0.9595748  0.5221929  0.43892512\n",
            " 0.23429264 0.03310634 0.00287846 0.04046523]\n",
            "Min/max predictions: 3.199959610356018e-05 0.959574818611145\n",
            "Loss for this batch: 0.2966\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19139329 0.26324555 0.08848239 0.32569665 0.25435168 0.26407406\n",
            " 0.38791558 0.4495995  0.574827   0.00632762]\n",
            "Min/max predictions: 3.100433968938887e-05 0.977677047252655\n",
            "Loss for this batch: 0.2602\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.22577034 0.1400108  0.37471414 0.00193695 0.21076472 0.01391901\n",
            " 0.34011889 0.21043177 0.23362748 0.3572251 ]\n",
            "Min/max predictions: 0.00012393335055094212 0.8852145075798035\n",
            "Loss for this batch: 0.3106\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00307316 0.00829446 0.45663142 0.40615204 0.0465689  0.37178195\n",
            " 0.03461394 0.5579723  0.7474251  0.22952485]\n",
            "Min/max predictions: 0.00016969707212410867 0.9391491413116455\n",
            "Loss for this batch: 0.3606\n",
            "Sample labels: [0. 0. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
            "Sample predictions: [0.02325184 0.35286003 0.25254384 0.01225094 0.41549516 0.59697914\n",
            " 0.22806083 0.8145215  0.16461878 0.18614607]\n",
            "Min/max predictions: 0.0001022324722725898 0.9576094746589661\n",
            "Loss for this batch: 0.3327\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00276058 0.40573242 0.00304528 0.03575793 0.29539856 0.00853357\n",
            " 0.13625357 0.03154477 0.01820234 0.00628182]\n",
            "Min/max predictions: 6.383200525306165e-05 0.8046199679374695\n",
            "Loss for this batch: 0.3441\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [7.6542348e-02 1.7108822e-03 4.5772610e-04 3.5416311e-01 1.6591947e-01\n",
            " 4.6756366e-01 1.5555196e-01 2.2322351e-02 4.2124499e-02 7.1849115e-03]\n",
            "Min/max predictions: 0.0001324615441262722 0.7737579941749573\n",
            "Loss for this batch: 0.3292\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0012732  0.32251215 0.00744092 0.6601501  0.25591302 0.01214842\n",
            " 0.23368856 0.00637033 0.05086239 0.16213587]\n",
            "Min/max predictions: 0.00011262187763350084 0.8993431329727173\n",
            "Loss for this batch: 0.4298\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.16076392 0.21326634 0.01234055 0.25796616 0.05361866 0.05209932\n",
            " 0.09652153 0.04557676 0.00500922 0.09345618]\n",
            "Min/max predictions: 4.6367622417164966e-05 0.9516430497169495\n",
            "Loss for this batch: 0.3295\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.53260434 0.00354692 0.35056353 0.01096817 0.00509343 0.55044645\n",
            " 0.58854336 0.00838062 0.3844488  0.07301175]\n",
            "Min/max predictions: 0.00016975744802039117 0.9564199447631836\n",
            "Loss for this batch: 0.3040\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.19915523 0.04660962 0.32429138 0.003044   0.01419085 0.1769534\n",
            " 0.5613603  0.00197345 0.42084342 0.21375918]\n",
            "Min/max predictions: 6.341754487948492e-05 0.9452722668647766\n",
            "Loss for this batch: 0.3196\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.36252618 0.01340093 0.21525049 0.0301827  0.3295927  0.46463698\n",
            " 0.09212045 0.55364233 0.11425061 0.01647238]\n",
            "Min/max predictions: 2.4243530788226053e-05 0.9718309044837952\n",
            "Loss for this batch: 0.2908\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4097141  0.02548409 0.01202208 0.11464849 0.04985761 0.00802382\n",
            " 0.40303785 0.10854519 0.02475858 0.21246895]\n",
            "Min/max predictions: 0.0002969276683870703 0.8473706841468811\n",
            "Loss for this batch: 0.3978\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [1.7023707e-03 5.7984435e-04 2.8849214e-01 2.4925930e-05 7.2436393e-03\n",
            " 1.5567917e-01 8.1001678e-03 1.2787829e-01 1.1725888e-01 6.5570873e-01]\n",
            "Min/max predictions: 2.4925930119934492e-05 0.9638450145721436\n",
            "Loss for this batch: 0.3464\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.33458397 0.12317897 0.10941301 0.03955156 0.1384419  0.00531698\n",
            " 0.6394912  0.06048762 0.07640655 0.00192308]\n",
            "Min/max predictions: 2.7954503821092658e-05 0.9798223376274109\n",
            "Loss for this batch: 0.3352\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.3165571e-03 1.6484516e-03 2.4944435e-01 9.2741102e-02 2.6061619e-04\n",
            " 4.8111086e-03 2.9364514e-01 3.0845899e-02 1.3324281e-03 6.5091115e-01]\n",
            "Min/max predictions: 0.0002047340094577521 0.9720198512077332\n",
            "Loss for this batch: 0.3131\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.3992038  0.0198564  0.03202596 0.62910724 0.2490909  0.08191881\n",
            " 0.00193843 0.36365357 0.19769016 0.0070828 ]\n",
            "Min/max predictions: 0.00042176261194981635 0.8707667589187622\n",
            "Loss for this batch: 0.3360\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.00698209 0.05258399 0.43483523 0.00070684 0.03518378 0.5670669\n",
            " 0.42976347 0.09222076 0.4910465  0.48714298]\n",
            "Min/max predictions: 0.0002610444207675755 0.9089747667312622\n",
            "Loss for this batch: 0.2947\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.33497417 0.08174604 0.03514108 0.0294176  0.3623138  0.00571889\n",
            " 0.32160875 0.13176508 0.2731991  0.04948971]\n",
            "Min/max predictions: 0.0001808062515920028 0.7868616580963135\n",
            "Loss for this batch: 0.3312\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02919023 0.15966117 0.00441297 0.39369288 0.08082899 0.09656994\n",
            " 0.00658659 0.00184113 0.487493   0.00208738]\n",
            "Min/max predictions: 0.0005461062537506223 0.9554945826530457\n",
            "Loss for this batch: 0.3796\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.45613912 0.00166685 0.20272791 0.0572609  0.17990257 0.7001832\n",
            " 0.3621238  0.0096809  0.10258587 0.00507058]\n",
            "Min/max predictions: 4.5562421291833743e-05 0.9117224812507629\n",
            "Loss for this batch: 0.3182\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.7990518e-01 3.0213317e-01 5.8634657e-01 6.2882555e-01 4.9852416e-01\n",
            " 1.5580413e-04 4.2671435e-02 2.1189267e-02 2.8358977e-02 1.6599962e-01]\n",
            "Min/max predictions: 0.00012022253213217482 0.941343367099762\n",
            "Loss for this batch: 0.2924\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.36562717 0.49073723 0.18636066 0.633159   0.05553371 0.45869055\n",
            " 0.0188211  0.1639512  0.34539795 0.83684343]\n",
            "Min/max predictions: 6.921292515471578e-05 0.9290769696235657\n",
            "Loss for this batch: 0.3428\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0076803  0.23148227 0.00363382 0.7328146  0.00740568 0.14357287\n",
            " 0.33894917 0.00688646 0.19879545 0.09316931]\n",
            "Min/max predictions: 7.86848904681392e-05 0.8653776049613953\n",
            "Loss for this batch: 0.3179\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            "Sample predictions: [7.0889699e-01 7.1993001e-02 3.4625706e-04 2.4171982e-02 4.3303441e-02\n",
            " 7.1744412e-02 6.6833958e-02 2.6476346e-03 1.7030698e-01 3.7839109e-04]\n",
            "Min/max predictions: 4.1914179746527225e-05 0.9542802572250366\n",
            "Loss for this batch: 0.3150\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4809675  0.02176091 0.2625012  0.3073764  0.05045982 0.16131589\n",
            " 0.00984637 0.04578868 0.0223993  0.02788473]\n",
            "Min/max predictions: 0.00041699185385368764 0.9058813452720642\n",
            "Loss for this batch: 0.3599\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.09809583 0.23858392 0.00653569 0.20286274 0.11214407 0.02703689\n",
            " 0.07673538 0.00146657 0.6460729  0.13205543]\n",
            "Min/max predictions: 0.00016197768854908645 0.948735773563385\n",
            "Loss for this batch: 0.3207\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.08674958 0.26070702 0.40639964 0.67778695 0.27540475 0.470987\n",
            " 0.01414238 0.390562   0.00878195 0.02125133]\n",
            "Min/max predictions: 6.617912731599063e-05 0.8407506346702576\n",
            "Loss for this batch: 0.3317\n",
            "Sample labels: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01350559 0.03260086 0.00477217 0.4810062  0.05226168 0.09346882\n",
            " 0.02401647 0.0843645  0.09035014 0.5497318 ]\n",
            "Min/max predictions: 0.00038007472176104784 0.8934263586997986\n",
            "Loss for this batch: 0.3998\n",
            "Sample labels: [0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02189091 0.21800198 0.30009666 0.4371493  0.27797413 0.21496587\n",
            " 0.30426568 0.12619555 0.24132757 0.04328568]\n",
            "Min/max predictions: 5.486153531819582e-05 0.9841752648353577\n",
            "Loss for this batch: 0.3576\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.4160753e-04 4.6048889e-01 1.6217136e-01 3.3739105e-02 2.1074131e-02\n",
            " 5.0580855e-03 2.6934480e-03 2.4934758e-01 3.4687527e-02 1.6055006e-01]\n",
            "Min/max predictions: 0.00015217471809592098 0.8719688653945923\n",
            "Loss for this batch: 0.3137\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.7721419  0.7094624  0.1612017  0.54473424 0.00346024 0.00095168\n",
            " 0.03845918 0.33620334 0.15162456 0.04744961]\n",
            "Min/max predictions: 0.0001228137407451868 0.9849541187286377\n",
            "Loss for this batch: 0.2903\n",
            "Sample labels: [1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.68953556 0.06248773 0.0299212  0.54549676 0.02388457 0.36346173\n",
            " 0.01986993 0.00581471 0.6298561  0.29187703]\n",
            "Min/max predictions: 1.7578384358785115e-05 0.8847036957740784\n",
            "Loss for this batch: 0.2716\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6448256  0.21862502 0.29914096 0.04252398 0.00114533 0.01556043\n",
            " 0.01292417 0.01182704 0.18392123 0.07006764]\n",
            "Min/max predictions: 0.0002493665961083025 0.9564340114593506\n",
            "Loss for this batch: 0.3499\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.5323286  0.00147975 0.7839557  0.18719772 0.00430534 0.39086697\n",
            " 0.13939802 0.43834057 0.1737084  0.00723696]\n",
            "Min/max predictions: 4.636660742107779e-05 0.934666633605957\n",
            "Loss for this batch: 0.3462\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.27500594 0.00777898 0.23909892 0.00666768 0.0102476  0.00049435\n",
            " 0.0448351  0.08074961 0.3727895  0.06320814]\n",
            "Min/max predictions: 7.143379480112344e-05 0.8866438269615173\n",
            "Loss for this batch: 0.3403\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.24348919 0.01194255 0.27632725 0.30489126 0.2210049  0.04905477\n",
            " 0.01067689 0.14158691 0.24155408 0.01027261]\n",
            "Min/max predictions: 3.983567876275629e-05 0.8674927353858948\n",
            "Loss for this batch: 0.3436\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.80160064 0.1529905  0.02194309 0.38164288 0.34249312 0.00111637\n",
            " 0.00103339 0.30584908 0.00901301 0.00201863]\n",
            "Min/max predictions: 0.0001509125140728429 0.8939865231513977\n",
            "Loss for this batch: 0.3647\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.5006216  0.56595993 0.36513278 0.29143125 0.03456568 0.23229095\n",
            " 0.18443969 0.04017502 0.8331476  0.8639507 ]\n",
            "Min/max predictions: 0.0001829598331823945 0.8907012939453125\n",
            "Loss for this batch: 0.3309\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.12075628 0.02466827 0.37842166 0.08920182 0.23337956 0.00588261\n",
            " 0.01112046 0.15408686 0.18264356 0.26431528]\n",
            "Min/max predictions: 2.874119127227459e-05 0.9522911310195923\n",
            "Loss for this batch: 0.2987\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.00870674 0.01531144 0.00073002 0.02758616 0.00569521 0.04533641\n",
            " 0.160997   0.68527806 0.00549555 0.03284436]\n",
            "Min/max predictions: 0.00011025756248272955 0.94310462474823\n",
            "Loss for this batch: 0.2852\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [7.0340810e-03 1.4606050e-01 7.7273875e-01 1.8546024e-01 4.2681429e-01\n",
            " 3.0108657e-02 5.9993178e-02 6.0981548e-01 2.6154724e-01 3.2387680e-04]\n",
            "Min/max predictions: 0.0001821365294745192 0.9480043649673462\n",
            "Loss for this batch: 0.3238\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00564298 0.308718   0.05707131 0.4693457  0.03394134 0.0059452\n",
            " 0.45948282 0.2798963  0.00398407 0.25964445]\n",
            "Min/max predictions: 7.483545050490648e-05 0.8998569846153259\n",
            "Loss for this batch: 0.3581\n",
            "Sample labels: [1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.7996529  0.34124002 0.5356448  0.05707308 0.35705855 0.00266421\n",
            " 0.34374744 0.00193127 0.08040304 0.00346069]\n",
            "Min/max predictions: 0.00015055089897941798 0.9149301052093506\n",
            "Loss for this batch: 0.3004\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [1.5760404e-04 7.9635936e-01 2.0276710e-01 3.3948764e-02 1.9166341e-02\n",
            " 3.1510983e-02 8.8732481e-02 1.0165382e-02 3.2020065e-01 1.3667583e-01]\n",
            "Min/max predictions: 0.00015760403766762465 0.970319390296936\n",
            "Loss for this batch: 0.3180\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00717437 0.22182427 0.00109915 0.00930665 0.38936642 0.00138164\n",
            " 0.01428228 0.12747163 0.21419321 0.17185508]\n",
            "Min/max predictions: 7.465355156455189e-05 0.9168696403503418\n",
            "Loss for this batch: 0.3411\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.14871107 0.1858511  0.748313   0.35211176 0.21418494 0.05598833\n",
            " 0.020009   0.04793154 0.7394919  0.01516924]\n",
            "Min/max predictions: 0.00018941808957606554 0.845244824886322\n",
            "Loss for this batch: 0.3069\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00198371 0.14122668 0.14176556 0.00238965 0.00475081 0.16515219\n",
            " 0.03292082 0.02062134 0.00069067 0.01576815]\n",
            "Min/max predictions: 0.0001627016463316977 0.8790290951728821\n",
            "Loss for this batch: 0.3778\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.07535485 0.00129769 0.41238698 0.07928162 0.2018113  0.0057229\n",
            " 0.002731   0.62863094 0.6332819  0.04316005]\n",
            "Min/max predictions: 3.6626683140639216e-05 0.8252837061882019\n",
            "Loss for this batch: 0.3829\n",
            "Sample labels: [1. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.75231546 0.05573135 0.2562157  0.69834113 0.0987061  0.35031998\n",
            " 0.7284291  0.18564746 0.18873723 0.38650507]\n",
            "Min/max predictions: 0.00013580189261119813 0.8963932991027832\n",
            "Loss for this batch: 0.3346\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.3993881  0.00715731 0.06428329 0.18841082 0.23958832 0.4672636\n",
            " 0.11763919 0.00205669 0.04121315 0.0211276 ]\n",
            "Min/max predictions: 0.00025066977832466364 0.8953918218612671\n",
            "Loss for this batch: 0.3185\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05009986 0.04450292 0.1829246  0.36858526 0.1656872  0.45871207\n",
            " 0.00102361 0.1557818  0.01137298 0.01213266]\n",
            "Min/max predictions: 2.8813037715735845e-05 0.9328452348709106\n",
            "Loss for this batch: 0.3165\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.0550876  0.11788426 0.18650724 0.04779837 0.04324427 0.44752207\n",
            " 0.02042671 0.6277397  0.00351934 0.85912395]\n",
            "Min/max predictions: 9.48484885157086e-05 0.9337015151977539\n",
            "Loss for this batch: 0.3160\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.27688557 0.17184593 0.34047887 0.00799426 0.00121491 0.49558812\n",
            " 0.19345935 0.8598606  0.20566635 0.4831149 ]\n",
            "Min/max predictions: 1.3793024663755205e-05 0.8872904181480408\n",
            "Loss for this batch: 0.3338\n",
            "Sample labels: [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.2735644  0.34487423 0.2503392  0.0957981  0.5772447  0.44471654\n",
            " 0.4065103  0.01131152 0.17468452 0.0734688 ]\n",
            "Min/max predictions: 0.0003535397117957473 0.9710088968276978\n",
            "Loss for this batch: 0.3786\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05527829 0.3599442  0.5535396  0.00358709 0.00515884 0.10667055\n",
            " 0.37574103 0.09822159 0.0011955  0.10592993]\n",
            "Min/max predictions: 6.485050107585266e-05 0.8914982676506042\n",
            "Loss for this batch: 0.3179\n",
            "Sample labels: [1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.55771965 0.24073999 0.35839757 0.25646362 0.6545925  0.06636582\n",
            " 0.00352117 0.01001727 0.03380667 0.08686751]\n",
            "Min/max predictions: 7.857212040107697e-05 0.8867959976196289\n",
            "Loss for this batch: 0.2658\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02948962 0.00552481 0.48010474 0.33565956 0.19899014 0.48341915\n",
            " 0.14266454 0.02421162 0.00160384 0.26144657]\n",
            "Min/max predictions: 0.00032828241819515824 0.951952338218689\n",
            "Loss for this batch: 0.3125\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.00716192 0.5297583  0.00237709 0.5534353  0.17281404 0.05998253\n",
            " 0.6820991  0.5421972  0.17933677 0.01189495]\n",
            "Min/max predictions: 0.00013581276289187372 0.8900194764137268\n",
            "Loss for this batch: 0.3971\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.2561587e-04 4.7480941e-01 1.1543418e-01 5.0000840e-01 4.5386017e-03\n",
            " 5.9149885e-01 5.3829873e-01 8.4373415e-02 3.6929708e-02 1.8173559e-02]\n",
            "Min/max predictions: 7.408959208987653e-05 0.8453891277313232\n",
            "Loss for this batch: 0.3056\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.14945358 0.08112969 0.4189095  0.00900986 0.3825289  0.42937014\n",
            " 0.00554369 0.01106279 0.04059476 0.0323304 ]\n",
            "Min/max predictions: 0.00022234214702621102 0.9815455079078674\n",
            "Loss for this batch: 0.3462\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.04216297 0.73174256 0.8673125  0.06395351 0.04443811 0.281779\n",
            " 0.01983604 0.68881875 0.00286542 0.01019013]\n",
            "Min/max predictions: 4.699458077084273e-05 0.8679967522621155\n",
            "Loss for this batch: 0.3305\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.01342212 0.50066906 0.00267793 0.04687642 0.16719118 0.006973\n",
            " 0.6169906  0.2981323  0.3177139  0.04215169]\n",
            "Min/max predictions: 0.00010260143608320504 0.9717095494270325\n",
            "Loss for this batch: 0.3293\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.1449356  0.18394868 0.02919756 0.38917533 0.6235811  0.02051258\n",
            " 0.00355707 0.07548407 0.00956435 0.0438041 ]\n",
            "Min/max predictions: 5.3174524509813637e-05 0.7794711589813232\n",
            "Loss for this batch: 0.3282\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.1029929  0.57781696 0.01953081 0.07958488 0.16397795 0.6774277\n",
            " 0.81714827 0.44601333 0.37926632 0.06212791]\n",
            "Min/max predictions: 0.00013391353422775865 0.8171482682228088\n",
            "Loss for this batch: 0.3348\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05429879 0.11929458 0.07769633 0.21240044 0.01352166 0.00760876\n",
            " 0.11837283 0.16741759 0.24515492 0.00067609]\n",
            "Min/max predictions: 6.97799478075467e-05 0.8185517191886902\n",
            "Loss for this batch: 0.3589\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0900697  0.04894659 0.00750106 0.17506622 0.0072032  0.31730285\n",
            " 0.00865611 0.39602938 0.3739209  0.03114443]\n",
            "Min/max predictions: 1.3940724784333725e-05 0.966821551322937\n",
            "Loss for this batch: 0.3180\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.11359602 0.13773802 0.7799842  0.01986188 0.04207838 0.49582773\n",
            " 0.3047023  0.00674978 0.27353585 0.11283582]\n",
            "Min/max predictions: 0.0001597523660166189 0.9135653972625732\n",
            "Loss for this batch: 0.3314\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [2.4170740e-01 3.3403078e-01 7.3532604e-02 2.8055025e-02 4.2868711e-04\n",
            " 4.6592634e-02 3.5275778e-04 2.8095701e-01 5.0557733e-01 5.1008455e-02]\n",
            "Min/max predictions: 1.8020344214164652e-05 0.9810041785240173\n",
            "Loss for this batch: 0.3818\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.01841773 0.03232267 0.00273836 0.17241716 0.05843003 0.05004313\n",
            " 0.39386237 0.22459374 0.4406416  0.00571974]\n",
            "Min/max predictions: 0.00021579071471933275 0.9667311906814575\n",
            "Loss for this batch: 0.2824\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.05712343 0.39024782 0.3680446  0.00277346 0.16196787 0.00230377\n",
            " 0.00104998 0.21676236 0.03810186 0.0642515 ]\n",
            "Min/max predictions: 8.67880808073096e-05 0.827725887298584\n",
            "Loss for this batch: 0.3111\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Sample predictions: [0.27541965 0.5906109  0.37965178 0.08615309 0.12383074 0.01064609\n",
            " 0.1400301  0.60086423 0.23609185 0.19467635]\n",
            "Min/max predictions: 2.947276516351849e-05 0.8764732480049133\n",
            "Loss for this batch: 0.3064\n",
            "Sample labels: [1. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
            "Sample predictions: [3.6159602e-01 5.4448113e-02 3.4974059e-03 4.0499762e-01 9.3215778e-03\n",
            " 1.2876500e-01 1.3110775e-01 7.8039782e-05 3.6509845e-01 3.5005671e-01]\n",
            "Min/max predictions: 7.803978223819286e-05 0.8757519125938416\n",
            "Loss for this batch: 0.3267\n",
            "Sample labels: [0. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.13003409 0.0337956  0.27585533 0.41577289 0.01444925 0.66150707\n",
            " 0.47562292 0.00339132 0.01478176 0.878382  ]\n",
            "Min/max predictions: 0.00011720030306605622 0.8783820271492004\n",
            "Loss for this batch: 0.3962\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.59458894 0.36559692 0.11379302 0.09134028 0.424285   0.06657103\n",
            " 0.02010245 0.03565311 0.0083531  0.05165516]\n",
            "Min/max predictions: 0.00014797061157878488 0.93641597032547\n",
            "Loss for this batch: 0.3609\n",
            "Sample labels: [1. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.6317978  0.35402656 0.24941991 0.9786307  0.05142143 0.07664441\n",
            " 0.4367786  0.14712162 0.00120171 0.01057498]\n",
            "Min/max predictions: 3.927701982320286e-05 0.9786307215690613\n",
            "Loss for this batch: 0.3542\n",
            "Sample labels: [1. 1. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.03116651 0.2904275  0.17118107 0.26236016 0.22532843 0.00223048\n",
            " 0.27134302 0.01993442 0.02193946 0.75717926]\n",
            "Min/max predictions: 0.0002892322954721749 0.8985157012939453\n",
            "Loss for this batch: 0.3402\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.3955534  0.02067438 0.0046618  0.24798766 0.19256568 0.02651235\n",
            " 0.8003407  0.01136914 0.3840331  0.00469354]\n",
            "Min/max predictions: 0.00010244404256809503 0.9545248746871948\n",
            "Loss for this batch: 0.3357\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01768922 0.32213333 0.0220472  0.9747892  0.4206997  0.33987376\n",
            " 0.04881434 0.00394007 0.02058437 0.43134955]\n",
            "Min/max predictions: 6.420721183530986e-05 0.980006217956543\n",
            "Loss for this batch: 0.3299\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.02372614 0.00241071 0.00135442 0.00237333 0.07361557 0.00536709\n",
            " 0.00177436 0.00230963 0.04480541 0.60197294]\n",
            "Min/max predictions: 0.00019510311540216208 0.9860771894454956\n",
            "Loss for this batch: 0.2821\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.00307458 0.01449145 0.81017697 0.4878543  0.10404345 0.3519741\n",
            " 0.52948236 0.19195776 0.2640776  0.17637652]\n",
            "Min/max predictions: 0.00021119473967701197 0.8212666511535645\n",
            "Loss for this batch: 0.3469\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00627573 0.38373184 0.27378553 0.00515323 0.02624515 0.66818035\n",
            " 0.02793608 0.10542687 0.01117205 0.5855842 ]\n",
            "Min/max predictions: 0.00030994671396911144 0.845220148563385\n",
            "Loss for this batch: 0.3405\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.03443854 0.4090406  0.05069791 0.06694461 0.00715498 0.05183725\n",
            " 0.5326326  0.65279424 0.6106398  0.09823167]\n",
            "Min/max predictions: 0.00028239728999324143 0.9324578642845154\n",
            "Loss for this batch: 0.3589\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.42890608 0.480737   0.00086209 0.00869782 0.10527861 0.14521082\n",
            " 0.67494506 0.12825483 0.00091881 0.00503833]\n",
            "Min/max predictions: 0.0005287958192639053 0.8531538844108582\n",
            "Loss for this batch: 0.3769\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.64161557 0.51378083 0.11765315 0.30339596 0.37492803 0.00242115\n",
            " 0.46742198 0.35805783 0.62032604 0.07171605]\n",
            "Min/max predictions: 6.751629553036764e-05 0.9261341094970703\n",
            "Loss for this batch: 0.4072\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.0509235e-04 2.3968790e-03 4.3114850e-01 5.0075221e-01 1.3703896e-01\n",
            " 1.4924139e-01 3.3097647e-03 1.7962706e-03 1.4993092e-01 4.7069728e-01]\n",
            "Min/max predictions: 7.953288150019944e-05 0.9804888963699341\n",
            "Loss for this batch: 0.2887\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.18249977 0.00607065 0.0126667  0.07848191 0.14222975 0.21958928\n",
            " 0.17898867 0.1285947  0.56313384 0.17908742]\n",
            "Min/max predictions: 0.00016731386131141335 0.8451048731803894\n",
            "Loss for this batch: 0.3587\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.20793077 0.8436317  0.25232193 0.02915126 0.00344342 0.25605446\n",
            " 0.6730747  0.38435873 0.13814364 0.30874902]\n",
            "Min/max predictions: 0.00022022490156814456 0.9468738436698914\n",
            "Loss for this batch: 0.2902\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.8173746e-01 9.2233391e-03 7.1141893e-01 8.2137482e-03 6.7746983e-04\n",
            " 1.6371420e-02 1.6050784e-01 5.7021201e-01 6.3523397e-02 3.1300500e-02]\n",
            "Min/max predictions: 0.00035389597178436816 0.9673348665237427\n",
            "Loss for this batch: 0.3634\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00117534 0.08592382 0.02785747 0.01873423 0.0020084  0.16232091\n",
            " 0.51690984 0.20813611 0.7435855  0.41804755]\n",
            "Min/max predictions: 0.00019555010658223182 0.9225455522537231\n",
            "Loss for this batch: 0.3247\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.60167074 0.10547204 0.00532349 0.3119787  0.01767795 0.15507805\n",
            " 0.00883835 0.34718478 0.01479398 0.39866525]\n",
            "Min/max predictions: 2.7954503821092658e-05 0.8450959324836731\n",
            "Loss for this batch: 0.3315\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.4512543  0.47721764 0.09696696 0.00953824 0.4098666  0.18543532\n",
            " 0.0031057  0.8045608  0.00444415 0.29758498]\n",
            "Min/max predictions: 0.00037415692349895835 0.8969007730484009\n",
            "Loss for this batch: 0.3330\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.43802014 0.22563453 0.6695824  0.17460926 0.04467827 0.3982446\n",
            " 0.0096242  0.22367422 0.00531491 0.19099405]\n",
            "Min/max predictions: 1.2311352293181699e-05 0.8783246278762817\n",
            "Loss for this batch: 0.3277\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.1646645  0.47855937 0.14701192 0.15375493 0.00930567 0.42392242\n",
            " 0.00122475 0.02019685 0.05624134 0.01941691]\n",
            "Min/max predictions: 0.00030271787545643747 0.9496588110923767\n",
            "Loss for this batch: 0.3399\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01021914 0.01602217 0.06809727 0.16557859 0.00446427 0.8451838\n",
            " 0.02245166 0.00142265 0.22110125 0.4441451 ]\n",
            "Min/max predictions: 0.00016240793047472835 0.845183789730072\n",
            "Loss for this batch: 0.3359\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.15226391 0.12105808 0.15090789 0.24750939 0.00089936 0.4093386\n",
            " 0.13317218 0.5119627  0.04619951 0.0154039 ]\n",
            "Min/max predictions: 9.268386929761618e-05 0.9400505423545837\n",
            "Loss for this batch: 0.3183\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.11356238 0.23625582 0.14309905 0.0046421  0.38332742 0.24178766\n",
            " 0.61368245 0.55245566 0.11556153 0.4874273 ]\n",
            "Min/max predictions: 0.00016503443475812674 0.9178318977355957\n",
            "Loss for this batch: 0.3097\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.6876055e-01 2.4338925e-02 6.1755396e-05 4.3146199e-01 8.9726057e-03\n",
            " 2.8846646e-02 1.9916548e-01 4.6558194e-02 4.5560426e-03 5.1281852e-01]\n",
            "Min/max predictions: 6.17553960182704e-05 0.9220122694969177\n",
            "Loss for this batch: 0.3671\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.03243349 0.0640645  0.5536578  0.01411336 0.0644384  0.45037967\n",
            " 0.04543125 0.26722267 0.01749418 0.00518873]\n",
            "Min/max predictions: 6.797659443691373e-05 0.897921085357666\n",
            "Loss for this batch: 0.3087\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02827127 0.02835257 0.67702234 0.8217656  0.0033697  0.00851445\n",
            " 0.02559914 0.00302861 0.01359077 0.3629122 ]\n",
            "Min/max predictions: 0.00015687147970311344 0.9129707217216492\n",
            "Loss for this batch: 0.3371\n",
            "Sample labels: [0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10282461 0.4188021  0.33926377 0.00099321 0.29278132 0.22511084\n",
            " 0.00496727 0.01061971 0.00366064 0.89435744]\n",
            "Min/max predictions: 8.774924208410084e-05 0.9493263959884644\n",
            "Loss for this batch: 0.3640\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [9.4194809e-04 1.1499126e-04 6.9912976e-01 1.0073930e-02 4.0639964e-01\n",
            " 5.3656273e-02 1.3732140e-01 4.2130552e-02 4.8287462e-02 9.8690111e-04]\n",
            "Min/max predictions: 0.0001149912568507716 0.9210420846939087\n",
            "Loss for this batch: 0.3347\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.13811626 0.1430709  0.00502866 0.15334402 0.01499526 0.12926064\n",
            " 0.30000207 0.00528386 0.15023793 0.26012638]\n",
            "Min/max predictions: 8.279988105641678e-05 0.9420120716094971\n",
            "Loss for this batch: 0.2658\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01180523 0.06945518 0.00606995 0.19152762 0.6774051  0.00652532\n",
            " 0.10966552 0.07473319 0.00579173 0.43008882]\n",
            "Min/max predictions: 3.573318099370226e-05 0.9696027040481567\n",
            "Loss for this batch: 0.3450\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.30317888 0.57067597 0.5637569  0.04204695 0.19212675 0.00777898\n",
            " 0.0060812  0.00314295 0.10137607 0.04754305]\n",
            "Min/max predictions: 0.0004389009263832122 0.9748333692550659\n",
            "Loss for this batch: 0.3315\n",
            "Sample labels: [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00539809 0.11761227 0.03683374 0.540286   0.00196988 0.12336945\n",
            " 0.01796363 0.00234809 0.25091472 0.67107695]\n",
            "Min/max predictions: 0.00015764251293148845 0.9485563039779663\n",
            "Loss for this batch: 0.3556\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.04381987 0.81151414 0.0131806  0.00524525 0.00203531 0.0032344\n",
            " 0.72177047 0.43169537 0.3149977  0.36017767]\n",
            "Min/max predictions: 9.385305020259693e-05 0.8826767206192017\n",
            "Loss for this batch: 0.3116\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17455664 0.30780235 0.28519362 0.3429082  0.41579613 0.01107514\n",
            " 0.22114082 0.43166935 0.00342007 0.0876025 ]\n",
            "Min/max predictions: 0.00018776017532218248 0.9090871214866638\n",
            "Loss for this batch: 0.3405\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.01782331 0.00683471 0.18785968 0.02630051 0.3998987  0.00871898\n",
            " 0.01018    0.0035294  0.07117465 0.27912316]\n",
            "Min/max predictions: 5.609737854683772e-05 0.8535948991775513\n",
            "Loss for this batch: 0.3078\n",
            "Sample labels: [0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.0283516  0.49726278 0.08236898 0.5255361  0.06030793 0.00779342\n",
            " 0.26807746 0.00926585 0.0035267  0.00687695]\n",
            "Min/max predictions: 0.00022993797028902918 0.8694347143173218\n",
            "Loss for this batch: 0.3197\n",
            "Sample labels: [1. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.51876783 0.02056585 0.2895064  0.64525497 0.16515438 0.09201822\n",
            " 0.01473035 0.5300071  0.07684343 0.03863013]\n",
            "Min/max predictions: 0.0002461741678416729 0.8594998121261597\n",
            "Loss for this batch: 0.3210\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.1513266  0.00078508 0.20559266 0.13649534 0.02650456 0.01802986\n",
            " 0.0771269  0.03134706 0.2798282  0.17476176]\n",
            "Min/max predictions: 1.8157797967432998e-05 0.9786781072616577\n",
            "Loss for this batch: 0.4007\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "Sample predictions: [0.12439895 0.00770819 0.26463762 0.06698564 0.03489685 0.02159445\n",
            " 0.25001812 0.59599304 0.64985734 0.47937927]\n",
            "Min/max predictions: 4.797597648575902e-05 0.8731012344360352\n",
            "Loss for this batch: 0.3626\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
            "Sample predictions: [6.1228655e-02 1.5980051e-01 1.9125137e-03 3.7896723e-01 2.9359695e-01\n",
            " 2.9263090e-04 1.0821925e-02 7.3321229e-01 2.0636113e-01 1.4639423e-02]\n",
            "Min/max predictions: 5.0812825065804645e-05 0.9522981643676758\n",
            "Loss for this batch: 0.3388\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.6665877  0.19967246 0.10153129 0.3782458  0.34248108 0.09095199\n",
            " 0.57551664 0.26487345 0.38628328 0.3171597 ]\n",
            "Min/max predictions: 1.1597495358728338e-05 0.8838720917701721\n",
            "Loss for this batch: 0.3561\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.02680865 0.4580669  0.04371116 0.0035249  0.3905753  0.00228511\n",
            " 0.05069385 0.00204108 0.36804855 0.07596765]\n",
            "Min/max predictions: 0.0001731575175654143 0.9728365540504456\n",
            "Loss for this batch: 0.3357\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.03107314 0.02166893 0.65351665 0.5414575  0.3061905  0.5869718\n",
            " 0.0180735  0.04587873 0.6004174  0.31092605]\n",
            "Min/max predictions: 9.813693395699374e-06 0.9299017190933228\n",
            "Loss for this batch: 0.3525\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.00323467 0.17962465 0.562088   0.00438652 0.00573101 0.05235144\n",
            " 0.00105795 0.06573443 0.45022345 0.77045625]\n",
            "Min/max predictions: 0.00014067407755646855 0.9135932922363281\n",
            "Loss for this batch: 0.3200\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.40420303e-03 7.49233007e-01 2.02165261e-01 1.83647629e-02\n",
            " 1.25080347e-04 1.64064928e-03 1.20416365e-03 1.20565765e-01\n",
            " 1.92520455e-01 3.72180641e-01]\n",
            "Min/max predictions: 0.00012508034706115723 0.9258650541305542\n",
            "Loss for this batch: 0.2394\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
            "Sample predictions: [0.32902503 0.00424159 0.24809213 0.5550182  0.00596684 0.3763469\n",
            " 0.47971174 0.3075895  0.4594242  0.08248984]\n",
            "Min/max predictions: 2.2711585188517347e-05 0.9240831732749939\n",
            "Loss for this batch: 0.3173\n",
            "Sample labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [3.64999287e-02 6.02675118e-02 2.77498454e-01 1.13777891e-02\n",
            " 3.73784006e-02 1.14072885e-04 7.32783740e-03 8.37446004e-02\n",
            " 5.38856804e-01 4.57664728e-02]\n",
            "Min/max predictions: 0.00011407288548070937 0.9502370357513428\n",
            "Loss for this batch: 0.2910\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.8217169  0.09596851 0.48915094 0.08277056 0.35660943 0.05009021\n",
            " 0.20528482 0.45714247 0.01994212 0.08651026]\n",
            "Min/max predictions: 2.6382715077488683e-05 0.9337839484214783\n",
            "Loss for this batch: 0.3996\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.578419   0.09166811 0.57673293 0.29455647 0.08308643 0.04460317\n",
            " 0.01074871 0.07036141 0.1178655  0.49342203]\n",
            "Min/max predictions: 5.1782611990347505e-05 0.849700927734375\n",
            "Loss for this batch: 0.3607\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [0.09152464 0.01260506 0.06003436 0.08646172 0.35951483 0.00466252\n",
            " 0.49099278 0.04658955 0.01625742 0.00282837]\n",
            "Min/max predictions: 5.335535388439894e-05 0.9207912683486938\n",
            "Loss for this batch: 0.3459\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "Sample predictions: [1.7666742e-01 3.9729479e-01 3.7736082e-04 7.6847082e-01 2.2929148e-03\n",
            " 2.3055753e-01 4.9794260e-01 2.6647763e-03 5.6500518e-01 4.3462473e-03]\n",
            "Min/max predictions: 7.720770372543484e-05 0.8973164558410645\n",
            "Loss for this batch: 0.3898\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00786512 0.0246906  0.02251838 0.219548   0.8302646  0.17134297\n",
            " 0.00385979 0.07517514 0.63539904 0.01247878]\n",
            "Min/max predictions: 8.223427721532062e-05 0.8780803680419922\n",
            "Loss for this batch: 0.2938\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [2.8983489e-02 5.5314088e-01 3.1377234e-02 2.4124581e-01 6.8581963e-01\n",
            " 2.3236971e-01 2.7232087e-01 4.9453318e-01 1.0169802e-01 3.0205451e-04]\n",
            "Min/max predictions: 0.00023107422748580575 0.9781885147094727\n",
            "Loss for this batch: 0.3363\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00036165 0.34774095 0.00762796 0.3572967  0.08834382 0.09307841\n",
            " 0.00079827 0.14101954 0.28183293 0.00439661]\n",
            "Min/max predictions: 7.073491724440828e-05 0.8874432444572449\n",
            "Loss for this batch: 0.2928\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.0023335  0.21284461 0.12143729 0.01027653 0.03000991 0.33832815\n",
            " 0.00504466 0.12549613 0.17151672 0.17057757]\n",
            "Min/max predictions: 6.276278145378456e-05 0.7947584390640259\n",
            "Loss for this batch: 0.3639\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.32362223 0.00183204 0.6205276  0.16163522 0.00356748 0.06934525\n",
            " 0.01413142 0.16095592 0.7779569  0.15441477]\n",
            "Min/max predictions: 5.4927237215451896e-05 0.9745142459869385\n",
            "Loss for this batch: 0.3376\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.00541034 0.28197667 0.8488228  0.02495256 0.8098235  0.2824739\n",
            " 0.23909487 0.08150052 0.15574004 0.00731904]\n",
            "Min/max predictions: 9.543912165099755e-05 0.9521527886390686\n",
            "Loss for this batch: 0.3356\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.31377906 0.03634782 0.01392483 0.8813294  0.3884383  0.00240958\n",
            " 0.05764225 0.06323178 0.5553898  0.12848425]\n",
            "Min/max predictions: 0.00019908742979168892 0.9119471907615662\n",
            "Loss for this batch: 0.3442\n",
            "Sample labels: [0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Sample predictions: [0.5552241  0.7769405  0.20439811 0.01207248 0.01608131 0.5088311\n",
            " 0.11163477 0.03462795 0.6714466  0.3691784 ]\n",
            "Min/max predictions: 8.641810563858598e-05 0.846492350101471\n",
            "Loss for this batch: 0.3171\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.17607144 0.3852979  0.01142146 0.324215   0.1096392  0.25878578\n",
            " 0.15567446 0.00627237 0.37665623 0.00184366]\n",
            "Min/max predictions: 0.00028919451870024204 0.8805979490280151\n",
            "Loss for this batch: 0.2864\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.7729005  0.15928935 0.0430331  0.01891609 0.05714964 0.5491842\n",
            " 0.03568314 0.29914096 0.02379519 0.35052288]\n",
            "Min/max predictions: 2.86704544123495e-05 0.9502953886985779\n",
            "Loss for this batch: 0.3535\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.29626817 0.09582501 0.00918731 0.3216088  0.01336011 0.6164576\n",
            " 0.35470164 0.02723879 0.00940074 0.14379905]\n",
            "Min/max predictions: 0.00012020947906421497 0.8567391633987427\n",
            "Loss for this batch: 0.3112\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03358416 0.03803673 0.4523739  0.0046989  0.0034012  0.26951253\n",
            " 0.09768487 0.12080891 0.548045   0.00443322]\n",
            "Min/max predictions: 0.00021390711481217295 0.918362021446228\n",
            "Loss for this batch: 0.3484\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.09164044 0.00133036 0.16777372 0.06126206 0.5514187  0.46473187\n",
            " 0.66191256 0.22456153 0.13975923 0.14368479]\n",
            "Min/max predictions: 0.00014577136607840657 0.9230788350105286\n",
            "Loss for this batch: 0.4039\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [1.8361640e-01 2.3041378e-01 4.6845382e-01 4.9504244e-01 3.0008899e-03\n",
            " 2.9019127e-04 6.1750704e-01 2.2400045e-01 2.5759584e-01 3.9075669e-02]\n",
            "Min/max predictions: 0.0001573958870721981 0.9499581456184387\n",
            "Loss for this batch: 0.3804\n",
            "Sample labels: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00765711 0.00163423 0.1030886  0.10812582 0.48953596 0.6156922\n",
            " 0.0569148  0.11054326 0.00225978 0.10365728]\n",
            "Min/max predictions: 3.925657438230701e-05 0.9827560186386108\n",
            "Loss for this batch: 0.3485\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.01871527 0.1601303  0.00228926 0.7584829  0.44327223 0.3131483\n",
            " 0.07360858 0.41169465 0.07321605 0.01927684]\n",
            "Min/max predictions: 2.9639684726134874e-05 0.9746372103691101\n",
            "Loss for this batch: 0.3592\n",
            "Sample labels: [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.58456427 0.31653064 0.14100166 0.12687474 0.05815164 0.21673684\n",
            " 0.0771069  0.02573921 0.04746735 0.5152057 ]\n",
            "Min/max predictions: 0.00011522493878146634 0.9487659335136414\n",
            "Loss for this batch: 0.3617\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [1.4201622e-01 5.9620297e-01 7.8448668e-02 4.8983069e-03 2.7115049e-04\n",
            " 8.0030449e-02 8.0347661e-04 1.3166316e-01 5.0924814e-01 2.8476557e-01]\n",
            "Min/max predictions: 0.0001506343251094222 0.9599777460098267\n",
            "Loss for this batch: 0.2873\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Sample predictions: [0.6646196  0.22520997 0.2625069  0.03227062 0.06328908 0.03606726\n",
            " 0.24807079 0.12073328 0.0872099  0.20185582]\n",
            "Min/max predictions: 0.0008258825400844216 0.9529759287834167\n",
            "Loss for this batch: 0.3501\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [4.0136114e-02 1.3436912e-01 6.5097593e-02 2.3629729e-04 7.5127786e-01\n",
            " 7.2916313e-03 1.1003634e-02 7.0723824e-02 2.8299862e-01 2.5980827e-01]\n",
            "Min/max predictions: 4.8519537813263014e-05 0.9537762403488159\n",
            "Loss for this batch: 0.3396\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.20276244 0.07507399 0.02527164 0.6616498  0.09101137 0.82255244\n",
            " 0.04021626 0.2815933  0.24660034 0.8116788 ]\n",
            "Min/max predictions: 0.0002234319254057482 0.8637090921401978\n",
            "Loss for this batch: 0.3466\n",
            "Sample labels: [1. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.62699205 0.40675613 0.00114423 0.19491039 0.07821766 0.5820385\n",
            " 0.05926248 0.64226955 0.00651799 0.3460539 ]\n",
            "Min/max predictions: 0.0001874727022368461 0.9301126599311829\n",
            "Loss for this batch: 0.3615\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Sample predictions: [6.0529768e-01 3.8388919e-02 9.9988945e-02 5.6806272e-01 2.4863805e-01\n",
            " 5.3141668e-04 3.1558648e-01 2.5894254e-01 1.5923366e-02 6.3161422e-03]\n",
            "Min/max predictions: 0.00025122318766079843 0.8917269110679626\n",
            "Loss for this batch: 0.3124\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.39499494 0.00920071 0.31371874 0.10297502 0.38799024 0.2965348\n",
            " 0.05452454 0.22417982 0.17639114 0.00455041]\n",
            "Min/max predictions: 0.0001400055189151317 0.9043336510658264\n",
            "Loss for this batch: 0.2826\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.14036794 0.00720768 0.09818251 0.2606934  0.00444567 0.24407643\n",
            " 0.02551709 0.23310567 0.27060467 0.24864998]\n",
            "Min/max predictions: 0.00010326506162527949 0.8588966131210327\n",
            "Loss for this batch: 0.3050\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.8071486  0.01256951 0.0022127  0.49961704 0.3086638  0.26976955\n",
            " 0.00927331 0.13971654 0.00201582 0.02903815]\n",
            "Min/max predictions: 3.647864286904223e-05 0.9039266109466553\n",
            "Loss for this batch: 0.3537\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.00581436 0.00234508 0.12088896 0.20813376 0.14608938 0.01151005\n",
            " 0.8765719  0.1107448  0.32546818 0.0548912 ]\n",
            "Min/max predictions: 0.00013614002091344446 0.9377818703651428\n",
            "Loss for this batch: 0.3030\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.19981122 0.04611007 0.27092502 0.00535366 0.31267878 0.5614336\n",
            " 0.00728159 0.07288231 0.42445326 0.6117352 ]\n",
            "Min/max predictions: 0.00017130770720541477 0.9739721417427063\n",
            "Loss for this batch: 0.3544\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.35554066 0.23489064 0.42911193 0.02886213 0.00170781 0.03901258\n",
            " 0.0615228  0.25112188 0.11408128 0.00796707]\n",
            "Min/max predictions: 0.00012435833923518658 0.891845703125\n",
            "Loss for this batch: 0.3162\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.18507405 0.16820672 0.5100422  0.52029634 0.18030182 0.01084676\n",
            " 0.0086387  0.04153832 0.00342077 0.38618937]\n",
            "Min/max predictions: 0.0002595096011646092 0.8247259855270386\n",
            "Loss for this batch: 0.3792\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4069867  0.00634    0.25639087 0.4318276  0.02956513 0.19629095\n",
            " 0.14375623 0.05457254 0.00517841 0.0756772 ]\n",
            "Min/max predictions: 2.3410444555338472e-05 0.9077292084693909\n",
            "Loss for this batch: 0.2867\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00942836 0.01118696 0.12931548 0.17435111 0.13497849 0.03003652\n",
            " 0.00503443 0.1377976  0.00661011 0.35688305]\n",
            "Min/max predictions: 9.323559061158448e-05 0.8985149264335632\n",
            "Loss for this batch: 0.2641\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.01687262 0.01711881 0.06637965 0.11705888 0.16179402 0.3305875\n",
            " 0.26063767 0.02845497 0.23540542 0.4949779 ]\n",
            "Min/max predictions: 2.5514147637295537e-05 0.9022019505500793\n",
            "Loss for this batch: 0.3265\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.03628385 0.42598742 0.71655685 0.26467478 0.09612417 0.49753916\n",
            " 0.21386072 0.21142732 0.00357221 0.1100529 ]\n",
            "Min/max predictions: 0.00010260143608320504 0.9456548094749451\n",
            "Loss for this batch: 0.3521\n",
            "Sample labels: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.06114639 0.0279216  0.30238768 0.10872096 0.5691614  0.04862246\n",
            " 0.3366403  0.19609997 0.6158926  0.14046097]\n",
            "Min/max predictions: 4.771448948304169e-05 0.839323878288269\n",
            "Loss for this batch: 0.3389\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.29246134 0.32163    0.0008419  0.20595387 0.4285002  0.35446396\n",
            " 0.3113381  0.40348634 0.35826382 0.05987808]\n",
            "Min/max predictions: 0.00020680051238741726 0.9417287707328796\n",
            "Loss for this batch: 0.2980\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [5.7411975e-01 7.8902423e-02 3.5278048e-04 2.6289595e-03 5.2312799e-03\n",
            " 1.8600841e-01 2.5365910e-01 5.3016770e-01 1.6760577e-01 7.4470866e-01]\n",
            "Min/max predictions: 0.00013293401570990682 0.9684991836547852\n",
            "Loss for this batch: 0.3166\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.6570931  0.11598229 0.01381498 0.10635541 0.19173549 0.4368708\n",
            " 0.07127199 0.47389454 0.01305312 0.443156  ]\n",
            "Min/max predictions: 4.338584767538123e-05 0.9677652716636658\n",
            "Loss for this batch: 0.3542\n",
            "Sample labels: [0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            "Sample predictions: [0.31410736 0.87494355 0.08457249 0.08708584 0.00996673 0.3773571\n",
            " 0.1830409  0.36965743 0.06331839 0.93631154]\n",
            "Min/max predictions: 4.9356753152096644e-05 0.9581629037857056\n",
            "Loss for this batch: 0.3302\n",
            "Sample labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.00078827 0.16822758 0.6192435  0.7502977  0.15374187 0.00151161\n",
            " 0.21720287 0.00352545 0.19710226 0.12554525]\n",
            "Min/max predictions: 0.0001833543210523203 0.8333554863929749\n",
            "Loss for this batch: 0.3456\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [3.6429293e-02 3.5723180e-01 2.1481603e-04 2.6219353e-01 2.4295504e-01\n",
            " 4.9018338e-02 7.9340971e-01 1.3552205e-01 5.5584037e-01 4.6883881e-02]\n",
            "Min/max predictions: 8.27921467134729e-05 0.8875008821487427\n",
            "Loss for this batch: 0.3540\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.33683    0.00398872 0.21650039 0.01392325 0.4583974  0.00922504\n",
            " 0.10625102 0.00608535 0.05609536 0.01827134]\n",
            "Min/max predictions: 0.00013111541920807213 0.899802029132843\n",
            "Loss for this batch: 0.3355\n",
            "Sample labels: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.10090205 0.20372318 0.3595918  0.00472687 0.5214546  0.7529589\n",
            " 0.0271961  0.00111854 0.23891307 0.03278486]\n",
            "Min/max predictions: 0.0003723528643604368 0.9072737097740173\n",
            "Loss for this batch: 0.3470\n",
            "Sample labels: [1. 1. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.21338056 0.6495852  0.66603327 0.2898653  0.28607473 0.34523106\n",
            " 0.02912779 0.00437267 0.02882641 0.46122453]\n",
            "Min/max predictions: 0.00017024211410898715 0.8666074275970459\n",
            "Loss for this batch: 0.4008\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.08384877 0.07425924 0.0527635  0.12543136 0.09916496 0.01831545\n",
            " 0.5513943  0.07691876 0.00703367 0.05481256]\n",
            "Min/max predictions: 8.056944352574646e-05 0.9539358019828796\n",
            "Loss for this batch: 0.3211\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.1331766  0.04094202 0.00793826 0.10002236 0.18138638 0.35088372\n",
            " 0.00681189 0.764653   0.27759448 0.39382803]\n",
            "Min/max predictions: 5.7737423048820347e-05 0.9254162907600403\n",
            "Loss for this batch: 0.3100\n",
            "Sample labels: [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.47266155 0.00153165 0.07263629 0.2114547  0.3386086  0.05396179\n",
            " 0.02165868 0.20530134 0.28516543 0.01101467]\n",
            "Min/max predictions: 0.0001157762817456387 0.9817165732383728\n",
            "Loss for this batch: 0.3881\n",
            "Sample labels: [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.04309474 0.00539033 0.09525583 0.2372571  0.51445425 0.28332356\n",
            " 0.0381197  0.17277567 0.00151612 0.00644778]\n",
            "Min/max predictions: 6.109553942224011e-05 0.9144839644432068\n",
            "Loss for this batch: 0.3496\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.09821418 0.10919596 0.00084419 0.38397586 0.29264253 0.00896293\n",
            " 0.18016718 0.54015315 0.00567942 0.59017444]\n",
            "Min/max predictions: 1.4800336430198513e-05 0.8962995409965515\n",
            "Loss for this batch: 0.3508\n",
            "Sample labels: [1. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Sample predictions: [0.43142045 0.04270067 0.79984635 0.32549974 0.0741579  0.1091498\n",
            " 0.68732804 0.10664179 0.46558148 0.07606555]\n",
            "Min/max predictions: 9.131931437877938e-05 0.8279654383659363\n",
            "Loss for this batch: 0.3200\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [5.5801128e-03 3.2393655e-01 8.4058167e-03 9.8797776e-02 2.3400857e-01\n",
            " 1.0095051e-03 3.3228394e-02 2.7392217e-04 1.5702745e-01 2.9916912e-01]\n",
            "Min/max predictions: 0.00023647579655516893 0.8777180910110474\n",
            "Loss for this batch: 0.4003\n",
            "Sample labels: [0. 1. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
            "Sample predictions: [0.08028722 0.2833455  0.01719772 0.0178336  0.27437395 0.3530477\n",
            " 0.67231524 0.5812016  0.09744531 0.12553708]\n",
            "Min/max predictions: 0.0005865053972229362 0.8274958729743958\n",
            "Loss for this batch: 0.3428\n",
            "Sample labels: [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.05455732 0.00098089 0.4760702  0.2084622  0.00501037 0.580891\n",
            " 0.10611303 0.28360307 0.15849073 0.40545902]\n",
            "Min/max predictions: 4.4892960431752726e-05 0.921344518661499\n",
            "Loss for this batch: 0.3436\n",
            "Sample labels: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
            "Sample predictions: [3.4785461e-02 2.5203314e-02 1.4044265e-01 4.2463830e-01 1.5193447e-03\n",
            " 8.2528818e-01 8.6512856e-02 9.3936920e-02 7.9013361e-04 2.0349899e-02]\n",
            "Min/max predictions: 0.00025722337886691093 0.9376230835914612\n",
            "Loss for this batch: 0.2734\n",
            "Sample labels: [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [6.4473569e-02 1.6173151e-03 1.5004136e-01 7.0265186e-01 8.0600582e-02\n",
            " 2.3687957e-02 8.2495093e-02 8.8515952e-02 5.9301028e-04 1.4011694e-01]\n",
            "Min/max predictions: 3.111194018856622e-05 0.8835094571113586\n",
            "Loss for this batch: 0.3168\n",
            "Sample labels: [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "Sample predictions: [0.03023452 0.34975496 0.33953315 0.0160049  0.02137444 0.22540985\n",
            " 0.4105112  0.15145166 0.4042164  0.49657243]\n",
            "Min/max predictions: 0.00017329923866782337 0.9113080501556396\n",
            "Loss for this batch: 0.3457\n",
            "Sample labels: [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.4642609  0.01592447 0.80068433 0.00215242 0.19029291 0.62539005\n",
            " 0.06845296 0.07985255 0.58654076 0.00200181]\n",
            "Min/max predictions: 0.00012480535951908678 0.9667286276817322\n",
            "Loss for this batch: 0.3985\n",
            "Sample labels: [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            "Sample predictions: [0.17005791 0.24465613 0.07136975 0.8439903  0.01286655 0.5856693\n",
            " 0.4178541  0.25737426 0.06218123 0.30562806]\n",
            "Min/max predictions: 0.00014459148223977536 0.9339945912361145\n",
            "Loss for this batch: 0.3481\n",
            "Sample labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Sample predictions: [0.58128035 0.07966371 0.00142369 0.5272699  0.20525903 0.04434108\n",
            " 0.0289603  0.03430348 0.26642445 0.35886824]\n",
            "Min/max predictions: 8.854356565279886e-05 0.9555914402008057\n",
            "Loss for this batch: 0.3260\n",
            "Sample labels: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            "Sample predictions: [0.00101864 0.12893106 0.24026345 0.49545085 0.70098513 0.03443416\n",
            " 0.04829868 0.38342083 0.00149469 0.02812412]\n",
            "Min/max predictions: 0.00017022911924868822 0.9742398858070374\n",
            "Loss for this batch: 0.3633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gmf import GMF\n",
        "from engine import Engine\n",
        "\n",
        "config = {\n",
        "    'alias': 'gmf-movielens',\n",
        "    'num_users': num_users,\n",
        "    'num_items': num_items,\n",
        "    'embedding_dim': 32,\n",
        "    'optimizer': 'adam',\n",
        "    'adam_lr': 0.001,\n",
        "    'l2_regularization': 0.0,\n",
        "    'batch_size': 256,\n",
        "    'device_id': 0,\n",
        "    'use_cuda': torch.cuda.is_available(),\n",
        "    'model_dir': 'models/gmf_{}_epoch{}_hr{:.4f}_ndcg{:.4f}.pth'\n",
        "}\n",
        "\n",
        "model = GMF(config['num_users'], config['num_items'], config['embedding_dim'])\n",
        "engine = Engine(config)\n",
        "engine.model = model.to(engine.device)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    engine.train_an_epoch(train_loader, epoch)\n",
        "    hr, ndcg = engine.evaluate(evaluate_data, epoch)\n",
        "    #engine.save(config['alias'], epoch, hr, ndcg)\n",
        "    #print(f\"Epoch {epoch} - HR: {hr:.4f}, NDCG: {ndcg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uezdlm0AoZ5",
        "outputId": "31bbf306-a495-499d-f8bb-b4f585b202f0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Training Loss: 0.4177\n",
            "Epoch 1 HR@10: 0.4337, NDCG@10: 0.2372\n",
            "Epoch 2 Average Training Loss: 0.3294\n",
            "Epoch 2 HR@10: 0.5027, NDCG@10: 0.2796\n",
            "Epoch 3 Average Training Loss: 0.3038\n",
            "Epoch 3 HR@10: 0.5355, NDCG@10: 0.2993\n",
            "Epoch 4 Average Training Loss: 0.2891\n",
            "Epoch 4 HR@10: 0.5546, NDCG@10: 0.3126\n",
            "Epoch 5 Average Training Loss: 0.2782\n",
            "Epoch 5 HR@10: 0.5673, NDCG@10: 0.3221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch_train_losses=[0.4177,0.3294,0.3038,0.2891,0.2782]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(epoch_train_losses) + 1), epoch_train_losses, marker='o', color='b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Training Loss')\n",
        "plt.title('Epoch vs Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "DkY-2IbcR8hu",
        "outputId": "e7a5708e-e2c7-4d94-ba6b-d0cb125b44b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}